<!doctype html><html><head><script>theBaseUrl=location.origin+"/";</script><meta charset=utf-8><meta name=viewport content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1"><title>Frequently Asked Questions</title><meta name=generator content="Hugo 0.58.3"><meta name=description content="Documentation for the TensorFlow for R interface"><link rel=canonical href=/guide/keras/faq/><meta name=author content="J.J. Allaire"><meta property=og:url content=/guide/keras/faq/><meta property=og:title content="TensorFlow for R"><meta name=apple-mobile-web-app-title content="TensorFlow for R"><meta name=apple-mobile-web-app-capable content=yes><meta name=apple-mobile-web-app-status-bar-style content=black-translucent><style>@font-face{font-family:icon;src:url(fonts/icon.eot?52m981);src:url(fonts/icon.eot?#iefix52m981) format('embedded-opentype'),url(fonts/icon.woff?52m981) format('woff'),url(fonts/icon.ttf?52m981) format('truetype'),url(fonts/icon.svg?52m981#icon) format('svg');font-weight:400;font-style:normal}</style><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/site-styles.css><script src=/js/vendor.js></script><script src=/js/app.js></script><link rel="shortcut icon" href=/images/favicon.png><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=single-page><nav data-gumshoe-header aria-label=Header id=header class=top-menu><div class=left><a href=/ data-parenturl class="top-menu-item site-title">TensorFlow for R</a>
<span class=logo-from>from</span>
<a href=/><div id=logo class=logo></div></a></div><div class=right><div class=top-menu-items id=top-menu-items><a href=/ data-parenturl=/ class=top-menu-item>Home</a>
<a href=/installation/ data-parenturl=/installation/ class=top-menu-item>Installation</a>
<a href=/tutorials/ data-parenturl=/tutorials/ class=top-menu-item>Tutorials</a>
<a href=/guide/ data-parenturl=/guide/ class=top-menu-item>Guide</a>
<a href=/deploy/ data-parenturl=/deploy/ class=top-menu-item>Deploy</a>
<a href=/tools/ data-parenturl=/tools/ class=top-menu-item>Tools</a>
<a href=/reference/keras data-parenturl=/reference/keras class=top-menu-item>API</a>
<a href=/learn/resources/ data-parenturl=/learn/resources/ class=top-menu-item>Learn</a>
<a href=/blog.html data-parenturl=/blog.html class=top-menu-item>Blog</a></div><a href=https://github.com/rstudio/keras class=github-logo><i class="fa fa-lg fa-github" aria-hidden=true></i></a></div></nav><nav aria-label=Header id=mobile-header><a href=/>TensorFlow for R</a><div class=right><a href=https://github.com/rstudio/keras class=github-logo><i class="fa fa-lg fa-github" aria-hidden=true></i></a><div class=hamburger><i class="fa fa-lg fa-bars" aria-hidden=true></i></div></div></nav><div id=mobile-menu-container><ul id=mobile-menu><li><a href=/>Home</a></li><ul></ul><li><a href=/installation/>Installation</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/installation/>Installing TensorFlow</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/installation/>Quick start</a></li><li><a href=/installation/custom/>Custom Installation</a></li></ul><li><a href=/installation/gpu>Using a GPU</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/installation/gpu/>Overview</a></li><li><a href=/installation/gpu/local_gpu/>Local GPU</a></li><li><a href=/installation/gpu/cloud_server_gpu/>Cloud Server</a></li><li><a href=/installation/gpu/cloud_desktop_gpu/>Cloud Desktop</a></li></ul></ul><li><a href=/tutorials/>Tutorials</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/>Overview</a></li><li><a href>Beginners</a></li><li><a href=/tutorials/beginners/>Quickstart</a></li><li><a href=/tutorials/beginners/basic-ml/>Basic ML with Keras</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/beginners/basic-ml/>Overview</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_basic_classification/>Image Classification</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_basic_regression/>Regression</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_basic_text_classification/>Text Classification</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub/>Transfer learning with tfhub</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_overfit_underfit/>Overfitting and Underfitting</a></li><li><a href=/tutorials/beginners/basic-ml/tutorial_save_and_restore/>Save and Restore Models</a></li></ul><li><a href=/tutorials/beginners/load/load_csv/>Load and preprocess data</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/beginners/load/load_csv/>Load CSV data</a></li><li><a href=/tutorials/beginners/load/load_image/>Load image data</a></li></ul><li><a href>Advanced</a></li><li><a href=/tutorials/advanced/>Quickstart</a></li><li><a href=/tutorials/advanced/customization/tensors-operations/>Customization</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/advanced/customization/tensors-operations/>Tensors and operations</a></li><li><a href=/tutorials/advanced/customization/custom-layers/>Custom layers</a></li><li><a href=/tutorials/advanced/customization/autodiff/>Automatic differentiation</a></li><li><a href=/tutorials/advanced/customization/custom-training/>Custom training</a></li></ul><li><a href=/tutorials/advanced/images/cnn/>Images</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/advanced/images/cnn/>Convolutional Neural Network</a></li><li><a href=/tutorials/advanced/images/transfer-learning-hub/>Transfer Learning with tfhub</a></li></ul><li><a href=/tutorials/advanced/structured/classify/>Structured data</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/advanced/structured/classify/>Classify structured data</a></li></ul><li><a href=/tutorials/advanced/distributed/distributed_training_with_keras/>Distributed training</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tutorials/advanced/distributed/distributed_training_with_keras/>Distributed training with Keras</a></li></ul></ul><li><a href=/guide/>Guide</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/>Overview</a></li><li><a href>Keras</a></li><li><a href=/guide/keras/>Getting started</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/keras/>Overview</a></li><li><a href=/guide/keras/guide_keras/>Keras basics</a></li><li><a href=/guide/keras/sequential_model/>Sequential API</a></li><li><a href=/guide/keras/functional_api/>Functional API</a></li><li><a href=/guide/keras/saving_serializing/>Saving and serializing models</a></li><li><a href=/guide/keras/faq/>Frequently Asked Questions</a></li></ul><li><a href=/guide/keras/custom_layers/>Advanced</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/keras/custom_layers/>Custom Layers</a></li><li><a href=/guide/keras/custom_models/>Custom Models</a></li><li><a href=/guide/keras/training_callbacks/>Training Callbacks</a></li><li><a href=/guide/keras/applications/>Pre-Trained Models</a></li><li><a href=/guide/keras/training_visualization/>Training Visualization</a></li></ul><li><a href=/guide/keras/examples/>Examples</a></li><li><a href>TensorFlow Mechanics</a></li><li><a href=/guide/tensorflow/eager_execution>Basics</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tensorflow/eager_execution/>Eager execution</a></li><li><a href=/guide/tensorflow/variable/>Variables</a></li><li><a href=/guide/tensorflow/tensors/>Tensors</a></li></ul><li><a href=/guide/tensorflow/ragged_tensors>Advanced</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tensorflow/ragged_tensors/>Ragged tensors</a></li></ul><li><a href>Data input pipeline</a></li><li><a href=/guide/tfdatasets/introduction>Overview</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tfdatasets/introduction/>Using Datasets</a></li></ul><li><a href=/guide/tfdatasets/feature_spec>Feature spec API</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tfdatasets/feature_spec/>Feature Spec interface</a></li><li><a href=/guide/tfdatasets/feature_columns/>Feature columns</a></li></ul><li><a href>TensorFlow Hub</a></li><li><a href=/guide/tfhub/intro>Basics</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tfhub/intro/>Overview</a></li><li><a href=/guide/tfhub/hub-with-keras/>Using with Keras</a></li></ul><li><a href=/guide/tfhub/key-concepts>Key concepts</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/tfhub/key-concepts/>Key Concepts</a></li></ul><li><a href>Model saving</a></li><li><a href=/guide/saving/checkpoints>Checkpoints</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/saving/checkpoints/>Checkpoints</a></li></ul><li><a href=/guide/saving/saved_model>Saved Model</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/guide/saving/saved_model/>Saved Model</a></li></ul></ul><li><a href=/deploy/>Deploy</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/deploy/>Overview</a></li><li><a href=/deploy/plumber/>Plumber</a></li><li><a href=/deploy/shiny/>Shiny</a></li><li><a href=/deploy/docker/>TensorFlow Serving</a></li><li><a href=/deploy/rsconnect/>RStudio Connect</a></li></ul><li><a href=/tools/>Tools</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tools/>Overview</a></li><li><a href=/tools/tfruns/>Training Runs</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/tools/tfruns/overview/>Introduction to tfruns</a></li><li><a href=/tools/tfruns/tuning/>Hyperparameter Tuning</a></li><li><a href=/tools/tfruns/managing/>Managing Training Runs</a></li></ul><li><a href=/tools/tensorboard/tensorboard/>TensorBoard</a></li></ul><li><a href=/reference/keras>API</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/reference/keras/>keras</a></li><li><a href=/reference/tensorflow/>tensorflow</a></li><li><a href=/reference/tfdatasets/>tfdatasets</a></li><li><a href=/reference/tfruns/>tfruns</a></li></ul><li><a href=/learn/resources/>Learn</a>
<i class="fa fa-chevron-right" aria-hidden=true></i></li><ul><li><a href=/learn/resources/>Resources</a></li></ul><li><a href=/blog.html>Blog</a></li><ul></ul></ul></div><div id=search-bar class=search-bar><p class=search-bar__icon><i class="fa fa-lg fa-search"></i></p><div class=search-bar__input><input type=text name=search class=st-default-search-input></div><div class=search-bar__exit><i class="fa fa-lg fa-times"></i></div><div class=inline-search-results><ul></ul></div></div></div><div class="page documentation"><div class=side-menu id=side-menu><ul class=side-menu-list data-parenturl=/ style=display:none></ul><ul class=side-menu-list data-parenturl=/installation/ style=display:none><li class=side-menu__item><a href=/installation/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Installing TensorFlow</p></a></li><li class=side-menu__item><a href=/installation/gpu class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Using a GPU</p></a></li></ul><ul class=side-menu-list data-parenturl=/tutorials/ style=display:none><li class=side-menu__item><a href=/tutorials/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Overview</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">Beginners</p></span></li><li class=side-menu__item><a href=/tutorials/beginners/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Quickstart</p></a></li><li class=side-menu__item><a href=/tutorials/beginners/basic-ml/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Basic ML with Keras</p></a></li><li class=side-menu__item><a href=/tutorials/beginners/load/load_csv/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Load and preprocess data</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">Advanced</p></span></li><li class=side-menu__item><a href=/tutorials/advanced/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Quickstart</p></a></li><li class=side-menu__item><a href=/tutorials/advanced/customization/tensors-operations/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Customization</p></a></li><li class=side-menu__item><a href=/tutorials/advanced/images/cnn/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Images</p></a></li><li class=side-menu__item><a href=/tutorials/advanced/structured/classify/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Structured data</p></a></li><li class=side-menu__item><a href=/tutorials/advanced/distributed/distributed_training_with_keras/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Distributed training</p></a></li></ul><ul class=side-menu-list data-parenturl=/guide/ style=display:none><li class=side-menu__item><a href=/guide/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Overview</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">Keras</p></span></li><li class="side-menu__item active"><a href=/guide/keras/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Getting started</p></a><ul class=side-menu__sub><span style=display:none>guide</span><li class=side-menu__sub__item><a id=menu-link-/guide/keras/ class=side-menu__sub__item__text href=/guide/keras/ data-url=/guide/keras/>Overview</a></li><span style=display:none>guide</span><li class=side-menu__sub__item><a id=menu-link-/guide/keras/guide_keras/ class=side-menu__sub__item__text href=/guide/keras/guide_keras/ data-url=/guide/keras/guide_keras/>Keras basics</a></li><span style=display:none>guide</span><li class=side-menu__sub__item><a id=menu-link-/guide/keras/sequential_model/ class=side-menu__sub__item__text href=/guide/keras/sequential_model/ data-url=/guide/keras/sequential_model/>Sequential API</a></li><span style=display:none>guide</span><li class=side-menu__sub__item><a id=menu-link-/guide/keras/functional_api/ class=side-menu__sub__item__text href=/guide/keras/functional_api/ data-url=/guide/keras/functional_api/>Functional API</a></li><span style=display:none>guide</span><li class=side-menu__sub__item><a id=menu-link-/guide/keras/saving_serializing/ class=side-menu__sub__item__text href=/guide/keras/saving_serializing/ data-url=/guide/keras/saving_serializing/>Saving and serializing models</a></li><span style=display:none>guide</span><li class="side-menu__sub__item active"><a id=menu-link-/guide/keras/faq/ class=side-menu__sub__item__text href=/guide/keras/faq/ data-url=/guide/keras/faq/>Frequently Asked Questions</a></li></ul></li><li class=side-menu__item><a href=/guide/keras/custom_layers/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Advanced</p></a></li><li class=side-menu__item><a href=/guide/keras/examples/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Examples</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">TensorFlow Mechanics</p></span></li><li class=side-menu__item><a href=/guide/tensorflow/eager_execution class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Basics</p></a></li><li class=side-menu__item><a href=/guide/tensorflow/ragged_tensors class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Advanced</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">Data input pipeline</p></span></li><li class=side-menu__item><a href=/guide/tfdatasets/introduction class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Overview</p></a></li><li class=side-menu__item><a href=/guide/tfdatasets/feature_spec class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Feature spec API</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">TensorFlow Hub</p></span></li><li class=side-menu__item><a href=/guide/tfhub/intro class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Basics</p></a></li><li class=side-menu__item><a href=/guide/tfhub/key-concepts class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Key concepts</p></a></li><li class=side-menu__item><span class=side-menu__item__link><p class="side-menu__item__link__text side-menu__item__link__text__block">Model saving</p></span></li><li class=side-menu__item><a href=/guide/saving/checkpoints class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Checkpoints</p></a></li><li class=side-menu__item><a href=/guide/saving/saved_model class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Saved Model</p></a></li></ul><ul class=side-menu-list data-parenturl=/deploy/ style=display:none><li class=side-menu__item><a href=/deploy/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Overview</p></a></li><li class=side-menu__item><a href=/deploy/plumber/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Plumber</p></a></li><li class=side-menu__item><a href=/deploy/shiny/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Shiny</p></a></li><li class=side-menu__item><a href=/deploy/docker/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>TensorFlow Serving</p></a></li><li class=side-menu__item><a href=/deploy/rsconnect/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>RStudio Connect</p></a></li></ul><ul class=side-menu-list data-parenturl=/tools/ style=display:none><li class=side-menu__item><a href=/tools/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Overview</p></a></li><li class=side-menu__item><a href=/tools/tfruns/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Training Runs</p></a></li><li class=side-menu__item><a href=/tools/tensorboard/tensorboard/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>TensorBoard</p></a></li></ul><ul class=side-menu-list data-parenturl=/reference/keras style=display:none><li class=side-menu__item><a href=/reference/keras/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>keras</p></a></li><li class=side-menu__item><a href=/reference/tensorflow/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>tensorflow</p></a></li><li class=side-menu__item><a href=/reference/tfdatasets/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>tfdatasets</p></a></li><li class=side-menu__item><a href=/reference/tfruns/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>tfruns</p></a></li></ul><ul class=side-menu-list data-parenturl=/learn/resources/ style=display:none><li class=side-menu__item><a href=/learn/resources/ class=side-menu__item__link><img class=side-menu__item__link__icon src=/images/icons/block-retina.png alt><p class=side-menu__item__link__text>Resources</p></a></li></ul><ul class=side-menu-list data-parenturl=/blog.html style=display:none></ul></div><div class=content><h1 class=content-header>Frequently Asked Questions</h1><div class=markdowned><div class=doc-page><div class=doc-page-index><ul id=gumshoe-container data-gumshoe></ul></div><div class=doc-page-body><div id=how-should-i-cite-keras class="section level2"><h2>How should I cite Keras?</h2><p>Please cite Keras in your publications if it helps your research. Here is an example BibTeX entry:</p><pre><code>@misc{chollet2017kerasR,
  title={R Interface to Keras},
  author={Chollet, Fran\c{c}ois and Allaire, JJ and others},
  year={2017},
  publisher={GitHub},
  howpublished={\url{https://github.com/rstudio/keras}},
}</code></pre></div><div id=how-can-i-run-keras-on-a-gpu class="section level2"><h2>How can I run Keras on a GPU?</h2><p>Note that installation and configuration of the GPU-based backends can take considerably more time and effort. So if you are just getting started with Keras you may want to stick with the CPU version initially, then install the appropriate GPU version once your training becomes more computationally demanding.</p><p>Below are instructions for installing and enabling GPU support for the various supported backends.</p><div id=tensorflow class="section level3"><h3>TensorFlow</h3><p>If your system has an NVIDIA® GPU and you have the GPU version of TensorFlow installed then your Keras code will automatically run on the GPU.</p><p>Additional details on GPU installation can be found here: <a href=https://tensorflow.rstudio.com/installation_gpu.html class=uri>https://tensorflow.rstudio.com/installation_gpu.html</a>.</p></div><div id=theano class="section level3"><h3>Theano</h3><p>If you are running on the Theano backend, you can set the <code>THEANO_FLAGS</code> environment variable to indicate you’d like to execute tensor operations on the GPU. For example:</p><div class=sourceCode id=cb2><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb2-1 data-line-number=1><span class=kw><a href=https://rdrr.io/r/base/Sys.setenv.html>Sys.setenv</a></span>(<span class=dt>KERAS_BACKEND =</span> <span class=st>"keras"</span>)</a>
<a class=sourceLine id=cb2-2 data-line-number=2><span class=kw><a href=https://rdrr.io/r/base/Sys.setenv.html>Sys.setenv</a></span>(<span class=dt>THEANO_FLAGS =</span> <span class=st>"device=gpu,floatX=float32"</span>)</a>
<a class=sourceLine id=cb2-3 data-line-number=3><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(keras)</a></code></pre></div><p>The name ‘gpu’ might have to be changed depending on your device’s identifier (e.g. <code>gpu0</code>, <code>gpu1</code>, etc).</p></div><div id=cntk class="section level3"><h3>CNTK</h3><p>If you have the GPU version of CNTK installed then your Keras code will automatically run on the GPU.</p><p>Additional information on installing the GPU version of CNTK can be found here: <a href=https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python class=uri>https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python</a></p></div></div><div id=how-can-i-run-a-keras-model-on-multiple-gpus class="section level2"><h2>How can I run a Keras model on multiple GPUs?</h2><p>We recommend doing so using the <strong>TensorFlow</strong> backend. There are two ways to run a single model on multiple GPUs: <strong>data parallelism</strong> and <strong>device parallelism</strong>.</p><p>In most cases, what you need is most likely data parallelism.</p><div id=data-parallelism class="section level3"><h3>Data parallelism</h3><p>Data parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data.
Keras has a built-in utility, <code><a href=../../keras/reference/multi_gpu_model.html>multi_gpu_model()</a></code>, which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.</p><p>For more information, see the documentation for <a href=https://keras.rstudio.com/reference/multi_gpu_model.html>multi_gpu_model</a>. Here is a quick example:</p><div class=sourceCode id=cb3><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb3-1 data-line-number=1><span class=co># Replicates `model` on 8 GPUs.</span></a>
<a class=sourceLine id=cb3-2 data-line-number=2><span class=co># This assumes that your machine has 8 available GPUs.</span></a>
<a class=sourceLine id=cb3-3 data-line-number=3>parallel_model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/multi_gpu_model.html>multi_gpu_model</a></span>(model, <span class=dt>gpus=</span><span class=dv>8</span>)</a>
<a class=sourceLine id=cb3-4 data-line-number=4>parallel_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>compile</a></span>(</a>
<a class=sourceLine id=cb3-5 data-line-number=5>  <span class=dt>loss =</span> <span class=st>"categorical_crossentropy"</span>,</a>
<a class=sourceLine id=cb3-6 data-line-number=6>  <span class=dt>optimizer =</span> <span class=st>"rmsprop"</span></a>
<a class=sourceLine id=cb3-7 data-line-number=7>)</a>
<a class=sourceLine id=cb3-8 data-line-number=8></a>
<a class=sourceLine id=cb3-9 data-line-number=9><span class=co># This `fit` call will be distributed on 8 GPUs.</span></a>
<a class=sourceLine id=cb3-10 data-line-number=10><span class=co># Since the batch size is 256, each GPU will process 32 samples.</span></a>
<a class=sourceLine id=cb3-11 data-line-number=11>parallel_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>fit</a></span>(x, y, <span class=dt>epochs =</span> <span class=dv>20</span>, <span class=dt>batch_size =</span> <span class=dv>256</span>)</a></code></pre></div></div><div id=device-parallelism class="section level3"><h3>Device parallelism</h3><p>Device parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.</p><p>This can be achieved by using TensorFlow device scopes. Here is a quick example:</p><div class=sourceCode id=cb4><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb4-1 data-line-number=1><span class=co># Model where a shared LSTM is used to encode two different sequences in parallel</span></a>
<a class=sourceLine id=cb4-2 data-line-number=2>input_a &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_input.html>layer_input</a></span>(<span class=dt>shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>140</span>, <span class=dv>256</span>))</a>
<a class=sourceLine id=cb4-3 data-line-number=3>input_b &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_input.html>layer_input</a></span>(<span class=dt>shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>140</span>, <span class=dv>256</span>))</a>
<a class=sourceLine id=cb4-4 data-line-number=4></a>
<a class=sourceLine id=cb4-5 data-line-number=5>shared_lstm &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_lstm.html>layer_lstm</a></span>(<span class=dt>units =</span> <span class=dv>64</span>)</a>
<a class=sourceLine id=cb4-6 data-line-number=6></a>
<a class=sourceLine id=cb4-7 data-line-number=7><span class=co># Process the first sequence on one GPU</span></a>
<a class=sourceLine id=cb4-8 data-line-number=8><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(tensorflow)</a>
<a class=sourceLine id=cb4-9 data-line-number=9><span class=kw><a href=https://rdrr.io/r/base/with.html>with</a></span>(tf<span class=op>$</span><span class=kw>device_scope</span>(<span class=st>"/gpu:0"</span>, {</a>
<a class=sourceLine id=cb4-10 data-line-number=10>  encoded_a &lt;-<span class=st> </span><span class=kw>shared_lstm</span>(tweet_a)</a>
<a class=sourceLine id=cb4-11 data-line-number=11>})<span class=op>:</span></a>
<a class=sourceLine id=cb4-12 data-line-number=12><span class=st>    </span></a>
<a class=sourceLine id=cb4-13 data-line-number=13><span class=co># Process the next sequence on another GPU</span></a>
<a class=sourceLine id=cb4-14 data-line-number=14><span class=kw><a href=https://rdrr.io/r/base/with.html>with</a></span>(tf<span class=op>$</span><span class=kw>device_scope</span>(<span class=st>"/gpu:1"</span>, {</a>
<a class=sourceLine id=cb4-15 data-line-number=15>  encoded_b &lt;-<span class=st> </span><span class=kw>shared_lstm</span>(tweet_b)</a>
<a class=sourceLine id=cb4-16 data-line-number=16>})<span class=op>:</span></a>
<a class=sourceLine id=cb4-17 data-line-number=17></a>
<a class=sourceLine id=cb4-18 data-line-number=18><span class=co># Concatenate results on CPU</span></a>
<a class=sourceLine id=cb4-19 data-line-number=19><span class=kw><a href=https://rdrr.io/r/base/with.html>with</a></span>(tf<span class=op>$</span><span class=kw>device_scope</span>(<span class=st>"/cpu:0"</span>, {</a>
<a class=sourceLine id=cb4-20 data-line-number=20>  merged_vector &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_concatenate.html>layer_concatenate</a></span>(<span class=kw><a href=https://rdrr.io/r/base/list.html>list</a></span>(encoded_a, encoded_b))</a>
<a class=sourceLine id=cb4-21 data-line-number=21>})<span class=op>:</span></a></code></pre></div></div></div><div id=what-does-sample-batch-epoch-mean class="section level2"><h2>What does “sample”, “batch”, “epoch” mean?</h2><p>Below are some common definitions that are necessary to know and understand to correctly utilize Keras:</p><ul><li><strong>Sample</strong>: one element of a dataset.<ul><li><em>Example:</em> one image is a <strong>sample</strong> in a convolutional network</li><li><em>Example:</em> one audio file is a <strong>sample</strong> for a speech recognition model</li></ul></li><li><strong>Batch</strong>: a set of <em>N</em> samples. The samples in a <strong>batch</strong> are processed independently, in parallel. If training, a batch results in only one update to the model.<ul><li>A <strong>batch</strong> generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).</li></ul></li><li><strong>Epoch</strong>: an arbitrary cutoff, generally defined as “one pass over the entire dataset”, used to separate training into distinct phases, which is useful for logging and periodic evaluation.<ul><li>When using <code>evaluation_data</code> or <code>evaluation_split</code> with the <code>fit</code> method of Keras models, evaluation will be run at the end of every <strong>epoch</strong>.</li><li>Within Keras, there is the ability to add <a href=training_callbacks.html>callbacks</a> specifically designed to be run at the end of an <strong>epoch</strong>. Examples of these are learning rate changes and model checkpointing (saving).</li></ul></li></ul></div><div id=why-are-keras-objects-modified-in-place class="section level2"><h2>Why are Keras objects modified in place?</h2><p>Unlike most R objects, Keras objects are “mutable”. That means that when you modify an object you’re modifying it “in place”, and you don’t need to assign the updated object back to the original name. For example, to add layers to a Keras model you might use this code:</p><div class=sourceCode id=cb5><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb5-1 data-line-number=1>model <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb5-2 data-line-number=2><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>activation =</span> <span class=st>'relu'</span>, <span class=dt>input_shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>784</span>)) <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb5-3 data-line-number=3><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>10</span>, <span class=dt>activation =</span> <span class=st>'softmax'</span>)</a></code></pre></div><p>Rather than this code:</p><div class=sourceCode id=cb6><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb6-1 data-line-number=1>model &lt;-<span class=st> </span>model <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb6-2 data-line-number=2><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>activation =</span> <span class=st>'relu'</span>, <span class=dt>input_shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>784</span>)) <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb6-3 data-line-number=3><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>10</span>, <span class=dt>activation =</span> <span class=st>'softmax'</span>)</a></code></pre></div><p>You need to be aware of this because it makes the Keras API a little different than most other pipelines you may have used, but it’s necessary to match the data structures and behavior of the underlying Keras library.</p></div><div id=how-can-i-save-a-keras-model class="section level2"><h2>How can I save a Keras model?</h2><div id=savingloading-whole-models-architecture-weights-optimizer-state class="section level3"><h3>Saving/loading whole models (architecture + weights + optimizer state)</h3><p>You can use <code><a href=../../keras/reference/save_model_hdf5.html>save_model_hdf5()</a></code> to save a Keras model into a single HDF5 file which will contain:</p><ul><li>the architecture of the model, allowing to re-create the model</li><li>the weights of the model</li><li>the training configuration (loss, optimizer)</li><li>the state of the optimizer, allowing to resume training exactly where you left off.</li></ul><p>You can then use <code><a href=../../keras/reference/save_model_hdf5.html>load_model_hdf5()</a></code> to reinstantiate your model.
<code><a href=../../keras/reference/save_model_hdf5.html>load_model_hdf5()</a></code> will also take care of compiling the model using the saved training configuration
(unless the model was never compiled in the first place).</p><p>Example:</p><div class=sourceCode id=cb7><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb7-1 data-line-number=1><span class=kw><a href=../../keras/reference/save_model_hdf5.html>save_model_hdf5</a></span>(model, <span class=st>'my_model.h5'</span>)</a>
<a class=sourceLine id=cb7-2 data-line-number=2>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/save_model_hdf5.html>load_model_hdf5</a></span>(<span class=st>'my_model.h5'</span>)</a></code></pre></div></div><div id=savingloading-only-a-models-architecture class="section level3"><h3>Saving/loading only a model’s architecture</h3><p>If you only need to save the <strong>architecture of a model</strong>, and not its weights or its training configuration, you can do:</p><div class=sourceCode id=cb8><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb8-1 data-line-number=1>json_string &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/model_to_json.html>model_to_json</a></span>(model)</a>
<a class=sourceLine id=cb8-2 data-line-number=2>yaml_string &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/model_to_yaml.html>model_to_yaml</a></span>(model)</a></code></pre></div><p>The generated JSON / YAML files are human-readable and can be manually edited if needed.</p><p>You can then build a fresh model from this data:</p><div class=sourceCode id=cb9><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb9-1 data-line-number=1>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/model_to_json.html>model_from_json</a></span>(json_string)</a>
<a class=sourceLine id=cb9-2 data-line-number=2>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/model_to_yaml.html>model_from_yaml</a></span>(yaml_string)</a></code></pre></div></div><div id=savingloading-only-a-models-weights class="section level3"><h3>Saving/loading only a model’s weights</h3><p>If you need to save the <strong>weights of a model</strong>, you can do so in HDF5 with the code below.</p><div class=sourceCode id=cb10><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb10-1 data-line-number=1><span class=kw><a href=../../keras/reference/save_model_weights_hdf5.html>save_model_weights_hdf5</a></span>(<span class=st>'my_model_weights.h5'</span>)</a></code></pre></div><p>Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the <em>same</em> architecture:</p><div class=sourceCode id=cb11><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb11-1 data-line-number=1>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/save_model_weights_hdf5.html>load_model_weights_hdf5</a></span>(<span class=st>'my_model_weights.h5'</span>)</a></code></pre></div><p>If you need to load weights into a <em>different</em> architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by <em>layer name</em>:</p><div class=sourceCode id=cb12><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb12-1 data-line-number=1>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/save_model_weights_hdf5.html>load_model_weights_hdf5</a></span>(<span class=st>'my_model_weights.h5'</span>, <span class=dt>by_name =</span> <span class=ot>TRUE</span>)</a></code></pre></div><p>For example:</p><div class=sourceCode id=cb13><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb13-1 data-line-number=1><span class=co># assuming the original model looks like this:</span></a>
<a class=sourceLine id=cb13-2 data-line-number=2><span class=co>#   model &lt;- keras_model_sequential()</span></a>
<a class=sourceLine id=cb13-3 data-line-number=3><span class=co>#   model %&gt;% </span></a>
<a class=sourceLine id=cb13-4 data-line-number=4><span class=co>#     layer_dense(units = 2, input_dim = 3, name = "dense 1") %&gt;% </span></a>
<a class=sourceLine id=cb13-5 data-line-number=5><span class=co>#     layer_dense(units = 3, name = "dense_3") %&gt;% </span></a>
<a class=sourceLine id=cb13-6 data-line-number=6><span class=co>#     ...</span></a>
<a class=sourceLine id=cb13-7 data-line-number=7><span class=co>#   save_model_weights(model, fname)</span></a>
<a class=sourceLine id=cb13-8 data-line-number=8></a>
<a class=sourceLine id=cb13-9 data-line-number=9><span class=co># new model</span></a>
<a class=sourceLine id=cb13-10 data-line-number=10>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model_sequential.html>keras_model_sequential</a></span>()</a>
<a class=sourceLine id=cb13-11 data-line-number=11>model <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb13-12 data-line-number=12><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>2</span>, <span class=dt>input_dim =</span> <span class=dv>3</span>, <span class=dt>name =</span> <span class=st>"dense 1"</span>) <span class=op>%&gt;%</span><span class=st>  </span><span class=co># will be loaded</span></a>
<a class=sourceLine id=cb13-13 data-line-number=13><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>3</span>, <span class=dt>name =</span> <span class=st>"dense_3"</span>)                     <span class=co># will not be loaded</span></a>
<a class=sourceLine id=cb13-14 data-line-number=14></a>
<a class=sourceLine id=cb13-15 data-line-number=15><span class=co># load weights from first model; will only affect the first layer, dense_1.</span></a>
<a class=sourceLine id=cb13-16 data-line-number=16><span class=kw>load_model_weights</span>(fname, <span class=dt>by_name =</span> <span class=ot>TRUE</span>)</a></code></pre></div></div></div><div id=why-is-the-training-loss-much-higher-than-the-testing-loss class="section level2"><h2>Why is the training loss much higher than the testing loss?</h2><p>A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.</p><p>Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.</p></div><div id=how-can-i-obtain-the-output-of-an-intermediate-layer class="section level2"><h2>How can I obtain the output of an intermediate layer?</h2><p>One simple way is to create a new <code>Model</code> that will output the layers that you are interested in:</p><div class=sourceCode id=cb14><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb14-1 data-line-number=1>model &lt;-<span class=st> </span>...  <span class=co># create the original model</span></a>
<a class=sourceLine id=cb14-2 data-line-number=2></a>
<a class=sourceLine id=cb14-3 data-line-number=3>layer_name &lt;-<span class=st> 'my_layer'</span></a>
<a class=sourceLine id=cb14-4 data-line-number=4>intermediate_layer_model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model.html>keras_model</a></span>(<span class=dt>inputs =</span> model<span class=op>$</span>input,</a>
<a class=sourceLine id=cb14-5 data-line-number=5>                                        <span class=dt>outputs =</span> <span class=kw><a href=../../keras/reference/get_layer.html>get_layer</a></span>(model, layer_name)<span class=op>$</span>output)</a>
<a class=sourceLine id=cb14-6 data-line-number=6>intermediate_output &lt;-<span class=st> </span><span class=kw><a href=https://rdrr.io/r/stats/predict.html>predict</a></span>(intermediate_layer_model, data)</a></code></pre></div></div><div id=how-can-i-use-keras-with-datasets-that-dont-fit-in-memory class="section level2"><h2>How can I use Keras with datasets that don’t fit in memory?</h2><div id=generator-functions class="section level3"><h3>Generator Functions</h3><p>To provide training or evaluation data incrementally you can write an R generator function that yields batches of training data then pass the function to the <code><a href=../../keras/reference/fit_generator.html>fit_generator()</a></code> function (or related functions <code><a href=../../keras/reference/evaluate_generator.html>evaluate_generator()</a></code> and <code><a href=../../keras/reference/predict_generator.html>predict_generator()</a></code>.</p><p>The output of generator functions must be a list of one of these forms:</p><ul><li>(inputs, targets)</li><li>(inputs, targets, sample_weights)</li></ul><p>All arrays should contain the same number of samples. The generator is expected to loop over its data indefinitely. For example, here’s simple generator function that yields randomly sampled batches of data:</p><div class=sourceCode id=cb15><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb15-1 data-line-number=1>sampling_generator &lt;-<span class=st> </span><span class=cf>function</span>(X_data, Y_data, batch_size) {</a>
<a class=sourceLine id=cb15-2 data-line-number=2>  <span class=cf>function</span>() {</a>
<a class=sourceLine id=cb15-3 data-line-number=3>    rows &lt;-<span class=st> </span><span class=kw><a href=https://rdrr.io/r/base/sample.html>sample</a></span>(<span class=dv>1</span><span class=op>:</span><span class=kw><a href=https://rdrr.io/r/base/nrow.html>nrow</a></span>(X_data), batch_size, <span class=dt>replace =</span> <span class=ot>TRUE</span>)</a>
<a class=sourceLine id=cb15-4 data-line-number=4>    <span class=kw><a href=https://rdrr.io/r/base/list.html>list</a></span>(X_data[rows,], Y_data[rows,])</a>
<a class=sourceLine id=cb15-5 data-line-number=5>  }</a>
<a class=sourceLine id=cb15-6 data-line-number=6>}</a>
<a class=sourceLine id=cb15-7 data-line-number=7></a>
<a class=sourceLine id=cb15-8 data-line-number=8>model <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb15-9 data-line-number=9><span class=st>  </span><span class=kw><a href=../../keras/reference/fit_generator.html>fit_generator</a></span>(<span class=kw>sampling_generator</span>(X_train, Y_train, <span class=dt>batch_size =</span> <span class=dv>128</span>), </a>
<a class=sourceLine id=cb15-10 data-line-number=10>                <span class=dt>steps_per_epoch =</span> <span class=kw><a href=https://rdrr.io/r/base/nrow.html>nrow</a></span>(X_train) <span class=op>/</span><span class=st> </span><span class=dv>128</span>, <span class=dt>epochs =</span> <span class=dv>10</span>)</a></code></pre></div><p>The <code>steps_per_epoch</code> parameter indicates the number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples if your dataset divided by the batch size.</p></div><div id=external-data-generators class="section level3"><h3>External Data Generators</h3><p>The above example doesn’t however address the use case of datasets that don’t fit in memory. Typically to do that you’ll write a generator that reads from another source (e.g. a sparse matrix or file(s) on disk) and maintains an offset into that data as it’s called repeatedly. For example, imagine you have a set of text files in a directory you want to read from:</p><div class=sourceCode id=cb16><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb16-1 data-line-number=1>data_files_generator &lt;-<span class=st> </span><span class=cf>function</span>(dir) {</a>
<a class=sourceLine id=cb16-2 data-line-number=2>  </a>
<a class=sourceLine id=cb16-3 data-line-number=3>  files <span class=op>&lt;</span><span class=st> </span><span class=kw><a href=https://rdrr.io/r/base/list.files.html>list.files</a></span>(dir)</a>
<a class=sourceLine id=cb16-4 data-line-number=4>  next_file &lt;-<span class=st> </span><span class=dv>0</span></a>
<a class=sourceLine id=cb16-5 data-line-number=5>  </a>
<a class=sourceLine id=cb16-6 data-line-number=6>  <span class=cf>function</span>() {</a>
<a class=sourceLine id=cb16-7 data-line-number=7>    </a>
<a class=sourceLine id=cb16-8 data-line-number=8>    <span class=co># move to the next file (note the &lt;&lt;- assignment operator)</span></a>
<a class=sourceLine id=cb16-9 data-line-number=9>    next_file &lt;&lt;-<span class=st> </span>next_file <span class=op>+</span><span class=st> </span><span class=dv>1</span></a>
<a class=sourceLine id=cb16-10 data-line-number=10>    </a>
<a class=sourceLine id=cb16-11 data-line-number=11>    <span class=co># if we've exhausted all of the files then start again at the</span></a>
<a class=sourceLine id=cb16-12 data-line-number=12>    <span class=co># beginning of the list (keras generators need to yield</span></a>
<a class=sourceLine id=cb16-13 data-line-number=13>    <span class=co># data infinitely -- termination is controlled by the epochs</span></a>
<a class=sourceLine id=cb16-14 data-line-number=14>    <span class=co># and steps_per_epoch arguments to fit_generator())</span></a>
<a class=sourceLine id=cb16-15 data-line-number=15>    <span class=cf>if</span> (next_file <span class=op>&gt;</span><span class=st> </span><span class=kw><a href=https://rdrr.io/r/base/length.html>length</a></span>(files))</a>
<a class=sourceLine id=cb16-16 data-line-number=16>      next_file &lt;&lt;-<span class=st> </span><span class=dv>1</span></a>
<a class=sourceLine id=cb16-17 data-line-number=17>    </a>
<a class=sourceLine id=cb16-18 data-line-number=18>    <span class=co># determine the file name</span></a>
<a class=sourceLine id=cb16-19 data-line-number=19>    file &lt;-<span class=st> </span>files[[next_file]]</a>
<a class=sourceLine id=cb16-20 data-line-number=20>    </a>
<a class=sourceLine id=cb16-21 data-line-number=21>    <span class=co># process and return the data in the file. note that in a </span></a>
<a class=sourceLine id=cb16-22 data-line-number=22>    <span class=co># real example you'd further subdivide the data within the</span></a>
<a class=sourceLine id=cb16-23 data-line-number=23>    <span class=co># file into appropriately sized training batches. this </span></a>
<a class=sourceLine id=cb16-24 data-line-number=24>    <span class=co># would make this function much more complicated so we</span></a>
<a class=sourceLine id=cb16-25 data-line-number=25>    <span class=co># don't demonstrated it here</span></a>
<a class=sourceLine id=cb16-26 data-line-number=26>    <span class=kw>file_to_training_data</span>(file)</a>
<a class=sourceLine id=cb16-27 data-line-number=27>  }</a>
<a class=sourceLine id=cb16-28 data-line-number=28>}</a></code></pre></div><p>The above function is an example of a stateful generator—the function maintains information across calls to keep track of which data to provide next. This is accomplished by defining shared state outside the generator function body and using the <code>&lt;&lt;-</code> operator to assign to it from within the generator.</p></div><div id=image-generators class="section level3"><h3>Image Generators</h3><p>You can also use the <code><a href=../../keras/reference/flow_images_from_directory.html>flow_images_from_directory()</a></code> and <code><a href=../../keras/reference/flow_images_from_data.html>flow_images_from_data()</a></code> functions along with <code><a href=../../keras/reference/fit_generator.html>fit_generator()</a></code> for training on sets of images stored on disk (with optional image augmentation/normalization via <code><a href=../../keras/reference/image_data_generator.html>image_data_generator()</a></code>).</p><p>You can see batch image training in action in our <a href=https://keras.rstudio.com/articles/examples/cifar10_cnn.html>CIFAR10 example</a>.</p></div><div id=batch-functions class="section level3"><h3>Batch Functions</h3><p>You can also do batch training using the <code><a href=../../keras/reference/train_on_batch.html>train_on_batch()</a></code> and <code><a href=../../keras/reference/train_on_batch.html>test_on_batch()</a></code> functions. These functions enable you to write a training loop that reads into memory only the data required for each batch.</p></div></div><div id=how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore class="section level2"><h2>How can I interrupt training when the validation loss isn’t decreasing anymore?</h2><p>You can use an early stopping callback:</p><div class=sourceCode id=cb17><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb17-1 data-line-number=1>early_stopping &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/callback_early_stopping.html>callback_early_stopping</a></span>(<span class=dt>monitor =</span> <span class=st>'val_loss'</span>, <span class=dt>patience =</span> <span class=dv>2</span>)</a>
<a class=sourceLine id=cb17-2 data-line-number=2>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>fit</a></span>(X, y, <span class=dt>validation_split =</span> <span class=fl>0.2</span>, <span class=dt>callbacks =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(early_stopping))</a></code></pre></div><p>Find out more in the <a href=training_callbacks.html>callbacks documentation</a>.</p></div><div id=how-is-the-validation-split-computed class="section level2"><h2>How is the validation split computed?</h2><p>If you set the <code>validation_split</code> argument in <code>fit</code> to e.g. 0.1, then the validation data used will be the <em>last 10%</em> of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn’t shuffled before extracting the validation split, so the validation is literally just the <em>last</em> x% of samples in the input you passed.</p><p>The same validation set is used for all epochs (within a same call to <code>fit</code>).</p></div><div id=is-the-data-shuffled-during-training class="section level2"><h2>Is the data shuffled during training?</h2><p>Yes, if the <code>shuffle</code> argument in <code>fit</code> is set to <code>TRUE</code> (which is the default), the training data will be randomly shuffled at each epoch.</p><p>Validation data is never shuffled.</p></div><div id=how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch class="section level2"><h2>How can I record the training / validation loss / accuracy at each epoch?</h2><p>The <code>model.fit</code> method returns an <code>History</code> callback, which has a <code>history</code> attribute containing the lists of successive losses and other metrics.</p><div class=sourceCode id=cb18><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb18-1 data-line-number=1>hist &lt;-<span class=st> </span>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>fit</a></span>(X, y, <span class=dt>validation_split=</span><span class=fl>0.2</span>)</a>
<a class=sourceLine id=cb18-2 data-line-number=2>hist<span class=op>$</span>history</a></code></pre></div></div><div id=how-can-i-freeze-keras-layers class="section level2"><h2>How can I “freeze” Keras layers?</h2><p>To “freeze” a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.</p><p>You can pass a <code>trainable</code> argument (boolean) to a layer constructor to set a layer to be non-trainable:</p><div class=sourceCode id=cb19><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb19-1 data-line-number=1>frozen_layer &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>trainable =</span> <span class=ot>FALSE</span>)</a></code></pre></div><p>Additionally, you can set the <code>trainable</code> property of a layer to <code>TRUE</code> or <code>FALSE</code> after instantiation. For this to take effect, you will need to call <code><a href=../../keras/reference/reexports.html>compile()</a></code> on your model after modifying the <code>trainable</code> property. Here’s an example:</p><div class=sourceCode id=cb20><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb20-1 data-line-number=1>x &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_input.html>layer_input</a></span>(<span class=dt>shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>32</span>))</a>
<a class=sourceLine id=cb20-2 data-line-number=2>layer &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>)</a>
<a class=sourceLine id=cb20-3 data-line-number=3>layer<span class=op>$</span>trainable &lt;-<span class=st> </span><span class=ot>FALSE</span></a>
<a class=sourceLine id=cb20-4 data-line-number=4>y &lt;-<span class=st> </span>x <span class=op>%&gt;%</span><span class=st> </span>layer</a>
<a class=sourceLine id=cb20-5 data-line-number=5></a>
<a class=sourceLine id=cb20-6 data-line-number=6>frozen_model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model.html>keras_model</a></span>(x, y)</a>
<a class=sourceLine id=cb20-7 data-line-number=7><span class=co># in the model below, the weights of `layer` will not be updated during training</span></a>
<a class=sourceLine id=cb20-8 data-line-number=8>frozen_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>compile</a></span>(<span class=dt>optimizer =</span> <span class=st>'rmsprop'</span>, <span class=dt>loss =</span> <span class=st>'mse'</span>)</a>
<a class=sourceLine id=cb20-9 data-line-number=9></a>
<a class=sourceLine id=cb20-10 data-line-number=10>layer<span class=op>$</span>trainable &lt;-<span class=st> </span><span class=ot>TRUE</span></a>
<a class=sourceLine id=cb20-11 data-line-number=11>trainable_model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model.html>keras_model</a></span>(x, y)</a>
<a class=sourceLine id=cb20-12 data-line-number=12><span class=co># with this model the weights of the layer will be updated during training</span></a>
<a class=sourceLine id=cb20-13 data-line-number=13><span class=co># (which will also affect the above model since it uses the same layer instance)</span></a>
<a class=sourceLine id=cb20-14 data-line-number=14>trainable_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>compile</a></span>(<span class=dt>optimizer =</span> <span class=st>'rmsprop'</span>, <span class=dt>loss =</span> <span class=st>'mse'</span>)</a>
<a class=sourceLine id=cb20-15 data-line-number=15></a>
<a class=sourceLine id=cb20-16 data-line-number=16>frozen_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>fit</a></span>(data, labels)  <span class=co># this does NOT update the weights of `layer`</span></a>
<a class=sourceLine id=cb20-17 data-line-number=17>trainable_model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>fit</a></span>(data, labels)  <span class=co># this updates the weights of `layer`</span></a></code></pre></div><p>Finally, you can freeze or unfreeze the weights for an entire model (or a range of layers within the model) using the <code><a href=../../keras/reference/freeze_weights.html>freeze_weights()</a></code> and <code><a href=../../keras/reference/freeze_weights.html>unfreeze_weights()</a></code> functions. For example:</p><div class=sourceCode id=cb21><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb21-1 data-line-number=1><span class=co># instantiate a VGG16 model</span></a>
<a class=sourceLine id=cb21-2 data-line-number=2>conv_base &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/application_vgg.html>application_vgg16</a></span>(</a>
<a class=sourceLine id=cb21-3 data-line-number=3>  <span class=dt>weights =</span> <span class=st>"imagenet"</span>,</a>
<a class=sourceLine id=cb21-4 data-line-number=4>  <span class=dt>include_top =</span> <span class=ot>FALSE</span>,</a>
<a class=sourceLine id=cb21-5 data-line-number=5>  <span class=dt>input_shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>150</span>, <span class=dv>150</span>, <span class=dv>3</span>)</a>
<a class=sourceLine id=cb21-6 data-line-number=6>)</a>
<a class=sourceLine id=cb21-7 data-line-number=7></a>
<a class=sourceLine id=cb21-8 data-line-number=8><span class=co># freeze it's weights</span></a>
<a class=sourceLine id=cb21-9 data-line-number=9><span class=kw><a href=../../keras/reference/freeze_weights.html>freeze_weights</a></span>(conv_base)</a>
<a class=sourceLine id=cb21-10 data-line-number=10></a>
<a class=sourceLine id=cb21-11 data-line-number=11><span class=co># create a composite model that includes the base + more layers</span></a>
<a class=sourceLine id=cb21-12 data-line-number=12>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model_sequential.html>keras_model_sequential</a></span>() <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb21-13 data-line-number=13><span class=st>  </span>conv_base <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb21-14 data-line-number=14><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_flatten.html>layer_flatten</a></span>() <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb21-15 data-line-number=15><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>256</span>, <span class=dt>activation =</span> <span class=st>"relu"</span>) <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb21-16 data-line-number=16><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>1</span>, <span class=dt>activation =</span> <span class=st>"sigmoid"</span>)</a>
<a class=sourceLine id=cb21-17 data-line-number=17></a>
<a class=sourceLine id=cb21-18 data-line-number=18><span class=co># compile</span></a>
<a class=sourceLine id=cb21-19 data-line-number=19>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>compile</a></span>(</a>
<a class=sourceLine id=cb21-20 data-line-number=20>  <span class=dt>loss =</span> <span class=st>"binary_crossentropy"</span>,</a>
<a class=sourceLine id=cb21-21 data-line-number=21>  <span class=dt>optimizer =</span> <span class=kw><a href=../../keras/reference/optimizer_rmsprop.html>optimizer_rmsprop</a></span>(<span class=dt>lr =</span> <span class=fl>2e-5</span>),</a>
<a class=sourceLine id=cb21-22 data-line-number=22>  <span class=dt>metrics =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=st>"accuracy"</span>)</a>
<a class=sourceLine id=cb21-23 data-line-number=23>)</a>
<a class=sourceLine id=cb21-24 data-line-number=24></a>
<a class=sourceLine id=cb21-25 data-line-number=25><span class=co># unfreeze weights from "block5_conv1" on</span></a>
<a class=sourceLine id=cb21-26 data-line-number=26><span class=kw><a href=../../keras/reference/freeze_weights.html>unfreeze_weights</a></span>(conv_base, <span class=dt>from =</span> <span class=st>"block5_conv1"</span>)</a>
<a class=sourceLine id=cb21-27 data-line-number=27></a>
<a class=sourceLine id=cb21-28 data-line-number=28><span class=co># compile again since we froze or unfroze layers</span></a>
<a class=sourceLine id=cb21-29 data-line-number=29>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/reexports.html>compile</a></span>(</a>
<a class=sourceLine id=cb21-30 data-line-number=30>  <span class=dt>loss =</span> <span class=st>"binary_crossentropy"</span>,</a>
<a class=sourceLine id=cb21-31 data-line-number=31>  <span class=dt>optimizer =</span> <span class=kw><a href=../../keras/reference/optimizer_rmsprop.html>optimizer_rmsprop</a></span>(<span class=dt>lr =</span> <span class=fl>2e-5</span>),</a>
<a class=sourceLine id=cb21-32 data-line-number=32>  <span class=dt>metrics =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=st>"accuracy"</span>)</a>
<a class=sourceLine id=cb21-33 data-line-number=33>)</a></code></pre></div></div><div id=how-can-i-use-stateful-rnns class="section level2"><h2>How can I use stateful RNNs?</h2><p>Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.</p><p>When using stateful RNNs, it is therefore assumed that:</p><ul><li>all batches have the same number of samples</li><li>If <code>X1</code> and <code>X2</code> are successive batches of samples, then <code>X2[[i]]</code> is the follow-up sequence to <code>X1[[i]</code>, for every <code>i</code>.</li></ul><p>To use statefulness in RNNs, you need to:</p><ul><li>explicitly specify the batch size you are using, by passing a <code>batch_size</code> argument to the first layer in your model. E.g. <code>batch_size=32</code> for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.</li><li>set <code>stateful=TRUE</code> in your RNN layer(s).</li><li>specify <code>shuffle=FALSE</code> when calling fit().</li></ul><p>To reset the states accumulated in either a single layer or an entire model use the <code><a href=../../keras/reference/reset_states.html>reset_states()</a></code> function.</p><p>Notes that the methods <code><a href=https://rdrr.io/r/stats/predict.html>predict()</a></code>, <code><a href=../../keras/reference/reexports.html>fit()</a></code>, <code><a href=../../keras/reference/train_on_batch.html>train_on_batch()</a></code>, <code><a href=../../keras/reference/predict_proba.html>predict_classes()</a></code>, etc. will <em>all</em> update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.</p></div><div id=how-can-i-remove-a-layer-from-a-sequential-model class="section level2"><h2>How can I remove a layer from a Sequential model?</h2><p>You can remove the last added layer in a Sequential model by calling <code><a href=../../keras/reference/pop_layer.html>pop_layer()</a></code>:</p><div class=sourceCode id=cb22><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb22-1 data-line-number=1>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/keras_model_sequential.html>keras_model_sequential</a></span>()</a>
<a class=sourceLine id=cb22-2 data-line-number=2>model <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb22-3 data-line-number=3><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>activation =</span> <span class=st>'relu'</span>, <span class=dt>input_shape =</span> <span class=kw><a href=https://rdrr.io/r/base/c.html>c</a></span>(<span class=dv>784</span>)) <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb22-4 data-line-number=4><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>activation =</span> <span class=st>'relu'</span>) <span class=op>%&gt;%</span><span class=st> </span></a>
<a class=sourceLine id=cb22-5 data-line-number=5><span class=st>  </span><span class=kw><a href=../../keras/reference/layer_dense.html>layer_dense</a></span>(<span class=dt>units =</span> <span class=dv>32</span>, <span class=dt>activation =</span> <span class=st>'relu'</span>)</a>
<a class=sourceLine id=cb22-6 data-line-number=6></a>
<a class=sourceLine id=cb22-7 data-line-number=7><span class=kw><a href=https://rdrr.io/r/base/length.html>length</a></span>(model<span class=op>$</span>layers)     <span class=co># "3"</span></a>
<a class=sourceLine id=cb22-8 data-line-number=8>model <span class=op>%&gt;%</span><span class=st> </span><span class=kw><a href=../../keras/reference/pop_layer.html>pop_layer</a></span>()</a>
<a class=sourceLine id=cb22-9 data-line-number=9><span class=kw><a href=https://rdrr.io/r/base/length.html>length</a></span>(model<span class=op>$</span>layers)     <span class=co># "2"</span></a></code></pre></div></div><div id=how-can-i-use-pre-trained-models-in-keras class="section level2"><h2>How can I use pre-trained models in Keras?</h2><p>Code and pre-trained weights are available for the following image classification models:</p><ul><li><a href=https://keras.rstudio.com/reference/application_xception.html>Xception</a></li><li><a href=https://keras.rstudio.com/reference/application_vgg.html>VGG16</a></li><li><a href=https://keras.rstudio.com/reference/application_vgg.html>VGG19</a></li><li><a href=https://keras.rstudio.com/reference/application_resnet50.html>ResNet50</a></li><li><a href=https://keras.rstudio.com/reference/application_inception_v3.html>InceptionV3</a></li><li><a href=https://keras.rstudio.com/reference/application_inception_resnet_v2.html>InceptionResNetV2</a></li><li><a href=https://keras.rstudio.com/reference/application_mobilenet.html>MobileNet</a></li><li><a href=https://keras.rstudio.com/reference/application_mobilenet_v2.html>MobileNetV2</a></li><li><a href=https://keras.rstudio.com/reference/application_densenet.html>DenseNet</a></li><li><a href=https://keras.rstudio.com/reference/application_nasnet.html>NASNet</a></li></ul><p>For example:</p><div class=sourceCode id=cb23><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb23-1 data-line-number=1>model &lt;-<span class=st> </span><span class=kw><a href=../../keras/reference/application_vgg.html>application_vgg16</a></span>(<span class=dt>weights =</span> <span class=st>'imagenet'</span>, <span class=dt>include_top =</span> <span class=ot>TRUE</span>)</a></code></pre></div><p>For a few simple usage examples, see <a href=applications.html>the documentation for the Applications module</a>.</p><p>The VGG16 model is also the basis for the <a href=https://keras.rstudio.com/articles/examples/deep_dream.html>Deep dream</a> Keras example script.</p></div><div id=how-can-i-use-other-keras-backends class="section level2"><h2>How can I use other Keras backends?</h2><p>By default the Keras Python and R packages use the TensorFlow backend. Other available backends include Theano or CNTK. To learn more about using alternatate backends (e.g. Theano or CNTK) see the <a href=backend.html>article on Keras backends</a>.</p></div><div id=how-can-i-use-the-plaidml-backend class="section level2"><h2>How can I use the PlaidML backend?</h2><p><a href=http://vertex.ai/blog/announcing-plaidml>PlaidML</a> is an open source portable deep learning engine that runs on most existing PC hardware with OpenCL-capable GPUs from NVIDIA, AMD, or Intel. PlaidML includes a Keras backend which you can use as described below.</p><p>First, build and install PlaidML as described on the <a href=https://github.com/plaidml/plaidml>project website</a>. You must be sure that PlaidML is correctly installed, setup, and working before proceeding further!</p><p>Then, to use Keras with the PlaidML backend you do the following:</p><div class=sourceCode id=cb24><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb24-1 data-line-number=1><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(keras)</a>
<a class=sourceLine id=cb24-2 data-line-number=2><span class=kw><a href=../../keras/reference/use_implementation.html>use_backend</a></span>(<span class=st>"plaidml"</span>)</a></code></pre></div><p>This should automatically discover and use the Python environment where <code>plaidml</code> and <code>plaidml-keras</code> were installed. If this doesn’t work as expected you can also force the selection of a particular Python environment. For example, if you installed PlaidML in conda environment named “plaidml” you would do this:</p><div class=sourceCode id=cb25><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb25-1 data-line-number=1><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(keras)</a>
<a class=sourceLine id=cb25-2 data-line-number=2><span class=kw><a href=../../keras/reference/reexports.html>use_condaenv</a></span>(<span class=st>"plaidml"</span>) </a>
<a class=sourceLine id=cb25-3 data-line-number=3><span class=kw><a href=../../keras/reference/use_implementation.html>use_backend</a></span>(<span class=st>"plaidml"</span>)</a></code></pre></div></div><div id=how-can-i-use-keras-in-another-r-package class="section level2"><h2>How can I use Keras in another R package?</h2><div id=testing-on-cran class="section level3"><h3>Testing on CRAN</h3><p>The main consideration in using Keras within another R package is to ensure that your package can be tested in an environment where Keras is not available (e.g. the CRAN test servers). To do this, arrange for your tests to be skipped when Keras isn’t available using the <code><a href=../../keras/reference/is_keras_available.html>is_keras_available()</a></code> function.</p><p>For example, here’s a <a href=http://r-pkgs.had.co.nz/tests.html>testthat</a> utility function that can be used to skip a test when Keras isn’t available:</p><div class=sourceCode id=cb26><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb26-1 data-line-number=1><span class=co># testthat utilty for skipping tests when Keras isn't available</span></a>
<a class=sourceLine id=cb26-2 data-line-number=2>skip_if_no_keras &lt;-<span class=st> </span><span class=cf>function</span>(<span class=dt>version =</span> <span class=ot>NULL</span>) {</a>
<a class=sourceLine id=cb26-3 data-line-number=3>  <span class=cf>if</span> (<span class=op>!</span><span class=kw><a href=../../keras/reference/is_keras_available.html>is_keras_available</a></span>(version))</a>
<a class=sourceLine id=cb26-4 data-line-number=4>    <span class=kw>skip</span>(<span class=st>"Required keras version not available for testing"</span>)</a>
<a class=sourceLine id=cb26-5 data-line-number=5>}</a>
<a class=sourceLine id=cb26-6 data-line-number=6></a>
<a class=sourceLine id=cb26-7 data-line-number=7><span class=co># use the function within a test</span></a>
<a class=sourceLine id=cb26-8 data-line-number=8><span class=kw>test_that</span>(<span class=st>"keras function works correctly"</span>, {</a>
<a class=sourceLine id=cb26-9 data-line-number=9>  <span class=kw>skip_if_no_keras</span>()</a>
<a class=sourceLine id=cb26-10 data-line-number=10>  <span class=co># test code here</span></a>
<a class=sourceLine id=cb26-11 data-line-number=11>})</a></code></pre></div><p>You can pass the <code>version</code> argument to check for a specific version of Keras.</p></div><div id=keras-module class="section level3"><h3>Keras Module</h3><p>Another consideration is gaining access to the underlying Keras python module. You might need to do this if you require lower level access to Keras than is provided for by the Keras R package.</p><p>Since the Keras R package can bind to multiple different implementations of Keras (either the original Keras or the TensorFlow implementation of Keras), you should use the <code><a href=../../keras/reference/implementation.html>keras::implementation()</a></code> function to obtain access to the correct python module. You can use this function within the <code>.onLoad</code> function of a package to provide global access to the module within your package. For example:</p><div class=sourceCode id=cb27><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb27-1 data-line-number=1><span class=co># Keras python module</span></a>
<a class=sourceLine id=cb27-2 data-line-number=2>keras &lt;-<span class=st> </span><span class=ot>NULL</span></a>
<a class=sourceLine id=cb27-3 data-line-number=3></a>
<a class=sourceLine id=cb27-4 data-line-number=4><span class=co># Obtain a reference to the module from the keras R package</span></a>
<a class=sourceLine id=cb27-5 data-line-number=5>.onLoad &lt;-<span class=st> </span><span class=cf>function</span>(libname, pkgname) {</a>
<a class=sourceLine id=cb27-6 data-line-number=6>  keras &lt;&lt;-<span class=st> </span>keras<span class=op>::</span><span class=kw><a href=../../keras/reference/implementation.html>implementation</a></span>() </a>
<a class=sourceLine id=cb27-7 data-line-number=7>}</a></code></pre></div></div><div id=custom-layers class="section level3"><h3>Custom Layers</h3><p>If you create <a href=custom_layers.html>custom layers in R</a> or import other Python packages which include custom Keras layers, be sure to wrap them using the <code><a href=../../keras/reference/create_layer.html>create_layer()</a></code> function so that they are composable using the magrittr pipe operator. See the documentation on <a href=custom_layers.html#layer-wrapper-function>layer wrapper functions</a> for additional details.</p></div></div><div id=how-can-i-obtain-reproducible-results-using-keras-during-development class="section level2"><h2>How can I obtain reproducible results using Keras during development?</h2><p>During development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample.</p><p>The <code><a href=../../keras/reference/reexports.html>use_session_with_seed()</a></code> function establishes a common random seed for R, Python, NumPy, and TensorFlow. It furthermore disables hash randomization, GPU computations, and CPU parallelization, which can be additional sources of non-reproducibility.</p><p>To use the function, call it immediately after you load the keras package:</p><div class=sourceCode id=cb28><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb28-1 data-line-number=1><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(keras)</a>
<a class=sourceLine id=cb28-2 data-line-number=2><span class=kw><a href=../../keras/reference/reexports.html>use_session_with_seed</a></span>(<span class=dv>42</span>)</a>
<a class=sourceLine id=cb28-3 data-line-number=3></a>
<a class=sourceLine id=cb28-4 data-line-number=4><span class=co># ...rest of code follows...</span></a></code></pre></div><p>This function takes all measures known to promote reproducible results from Keras sessions, however it’s possible that various individual features or libraries used by the backend escape its effects. If you encounter non-reproducible results please investigate the possible sources of the problem. The source code for <code><a href=../../keras/reference/reexports.html>use_session_with_seed()</a></code> is here: <a href=https://github.com/rstudio/tensorflow/blob/master/R/seed.R class=uri>https://github.com/rstudio/tensorflow/blob/master/R/seed.R</a>. Contributions via pull request are very welcome!</p><p>Please note again that <code><a href=../../keras/reference/reexports.html>use_session_with_seed()</a></code> disables GPU computations and CPU parallelization by default (as both can lead to non-deterministic computations) so should generally not be used when model training time is paramount. You can re-enable GPU computations and/or CPU parallelism using the <code>disable_gpu</code> and <code>disable_parallel_cpu</code> arguments. For example:</p><div class=sourceCode id=cb29><pre class="sourceCode r"><code class="sourceCode r"><a class=sourceLine id=cb29-1 data-line-number=1><span class=kw><a href=https://rdrr.io/r/base/library.html>library</a></span>(keras)</a>
<a class=sourceLine id=cb29-2 data-line-number=2><span class=kw><a href=../../keras/reference/reexports.html>use_session_with_seed</a></span>(<span class=dv>42</span>, <span class=dt>disable_gpu =</span> <span class=ot>FALSE</span>, <span class=dt>disable_parallel_cpu =</span> <span class=ot>FALSE</span>)</a></code></pre></div></div><div id=where-is-the-keras-configuration-filed-stored class="section level2"><h2>Where is the Keras configuration filed stored?</h2><p>The default directory where all Keras data is stored is:</p><div class=sourceCode id=cb30><pre class="sourceCode bash"><code class="sourceCode bash"><a class=sourceLine id=cb30-1 data-line-number=1><span class=ex>~/.keras/</span></a></code></pre></div><p>In case Keras cannot create the above directory (e.g. due to permission issues), <code>/tmp/.keras/</code> is used as a backup.</p><p>The Keras configuration file is a JSON file stored at <code>$HOME/.keras/keras.json</code>. The default configuration file looks like this:</p><pre><code>{
    "image_data_format": "channels_last",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
}</code></pre><p>It contains the following fields:</p><ul><li>The image data format to be used as default by image processing layers and utilities (either <code>channels_last</code> or <code>channels_first</code>).</li><li>The <code>epsilon</code> numerical fuzz factor to be used to prevent division by zero in some operations.</li><li>The default float data type.</li><li>The default backend (this will always be “tensorflow” in the R interface to Keras)</li></ul><p>Likewise, cached dataset files, such as those downloaded with <code><a href=../../keras/reference/get_file.html>get_file()</a></code>, are stored by default in <code>$HOME/.keras/datasets/</code>.</p></div></div></div></div><footer>Copyright © 2015-2019 The TensorFlow Authors and RStudio, Inc.
<script type=text/javascript>(function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){(w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);})(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');_st('install','Ne8tbLfE121Pkj3xMr_G','2.0.0');</script></footer></div></div></body></html>