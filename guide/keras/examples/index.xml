<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Keras Examples on TensorFlow for R</title><link>/guide/keras/examples/</link><description>Recent content in Keras Examples on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/guide/keras/examples/index.xml" rel="self" type="application/rss+xml"/><item><title>addition_rnn</title><link>/guide/keras/examples/addition_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/addition_rnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/addition_rnn.R
An implementation of sequence to sequence learning for performing addition
Input: “535+61”
Output: “596”
Padding is handled by using a repeated sentinel character (space)
Input may optionally be reversed, shown to increase performance in many tasks in: “Learning to Execute” http://arxiv.org/abs/1410.4615 and “Sequence to Sequence Learning with Neural Networks” http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf Theoretically it introduces shorter term dependencies between source and target.
Two digits reversed: One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs</description></item><item><title>babi_memnn</title><link>/guide/keras/examples/babi_memnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/babi_memnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/babi_memnn.R
Trains a memory network on the bAbI dataset.
References:
Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks”, http://arxiv.org/abs/1502.05698
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, “End-To-End Memory Networks”, http://arxiv.org/abs/1503.08895
Reaches 98.6% accuracy on task ‘single_supporting_fact_10k’ after 120 epochs. Time per epoch: 3s on CPU (core i7).
library(keras) library(readr) library(stringr) library(purrr) library(tibble) library(dplyr) # Function definition ----------------------------------------------------- tokenize_words &amp;lt;-function(x){ x &amp;lt;-x %&amp;gt;% str_replace_all(&#39;([[:punct:]]+)&#39;, &#39; \\1&#39;) %&amp;gt;% str_split(&#39; &#39;) %&amp;gt;% unlist() x[x !</description></item><item><title>babi_rnn</title><link>/guide/keras/examples/babi_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/babi_rnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/babi_rnn.R
Trains two recurrent neural networks based upon a story and a question. The resulting merged vector is then queried to answer a range of bAbI tasks.
The results are comparable to those for an LSTM model provided in Weston et al.: “Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks” http://arxiv.org/abs/1502.05698
Task Number FB LSTM Baseline Keras QA QA1 - Single Supporting Fact 50 100.</description></item><item><title>cifar10_cnn</title><link>/guide/keras/examples/cifar10_cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/cifar10_cnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_cnn.R
Train a simple deep CNN on the CIFAR10 small images dataset.
It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs, though it is still underfitting at that point.
library(keras) # Parameters -------------------------------------------------------------- batch_size &amp;lt;-32 epochs &amp;lt;-200 data_augmentation &amp;lt;-TRUE # Data Preparation -------------------------------------------------------- # See ?dataset_cifar10 for more info cifar10 &amp;lt;-dataset_cifar10() # Feature scale RGB values in test and train inputs x_train &amp;lt;-cifar10$train$x/255 x_test &amp;lt;-cifar10$test$x/255 y_train &amp;lt;-to_categorical(cifar10$train$y, num_classes = 10) y_test &amp;lt;-to_categorical(cifar10$test$y, num_classes = 10) # Defining Model ---------------------------------------------------------- # Initialize sequential model model &amp;lt;-keras_model_sequential() model %&amp;gt;% # Start with hidden 2D convolutional layer being fed 32x32 pixel images layer_conv_2d( filter = 32, kernel_size = c(3,3), padding = &#34;</description></item><item><title>cifar10_densenet</title><link>/guide/keras/examples/cifar10_densenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/cifar10_densenet/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_densenet.R
In this example we will train a DenseNet-40-12 to classify images from the CIFAR10 small images dataset. This takes ~125s per epoch on a NVIDIA GEFORCE 1080 Ti, so using a GPU is highly recommended.
DenseNet is a network architecture where each layer is directly connected to every other layer in a feed-forward fashion (within each dense block). For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers.</description></item><item><title>cifar10_resnet</title><link>/guide/keras/examples/cifar10_resnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/cifar10_resnet/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/cifar10_resnet.R
# Example</description></item><item><title>conv_filter_visualization</title><link>/guide/keras/examples/conv_filter_visualization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/conv_filter_visualization/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/conv_filter_visualization.R
library(keras)</description></item><item><title>conv_lstm</title><link>/guide/keras/examples/conv_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/conv_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/conv_lstm.R
# This script demonstrates the use of a convolutional LSTM network. # This network is used to predict the next frame of an artificially # generated movie which contains moving squares. library(keras) library(abind) library(raster) # Function Definition ----------------------------------------------------- generate_movies &amp;lt;-function(n_samples = 1200, n_frames = 15){ rows &amp;lt;-80 cols &amp;lt;-80 noisy_movies &amp;lt;-array(0, dim = c(n_samples, n_frames, rows, cols)) shifted_movies &amp;lt;-array(0, dim = c(n_samples, n_frames, rows, cols)) n &amp;lt;-sample(3:8, 1) for(s in 1:n_samples){ for(i in 1:n){ # Initial position xstart &amp;lt;-sample(20:60, 1) ystart &amp;lt;-sample(20:60, 1) # Direction of motion directionx &amp;lt;-sample(-1:1, 1) directiony &amp;lt;-sample(-1:1, 1) # Size of the square w &amp;lt;-sample(2:3, 1) x_shift &amp;lt;-xstart +directionx*(0:(n_frames)) y_shift &amp;lt;-ystart +directiony*(0:(n_frames)) for(t in 1:n_frames){ square_x &amp;lt;-(x_shift[t] -w):(x_shift[t] +w) square_y &amp;lt;-(y_shift[t] -w):(y_shift[t] +w) noisy_movies[s, t, square_x, square_y] &amp;lt;- noisy_movies[s, t, square_x, square_y] +1 # Make it more robust by adding noise.</description></item><item><title>deep_dream</title><link>/guide/keras/examples/deep_dream/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/deep_dream/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/deep_dream.R
library(keras) # Utility functions ------------------------------------------------------- # Util function to open, resize, and format pictures into tensors that Inception V3 can process preprocess_image &amp;lt;-function(image_path) { image_load(image_path) %&amp;gt;% image_to_array() %&amp;gt;% array_reshape(dim = c(1, dim(.))) %&amp;gt;% inception_v3_preprocess_input() } # Util function to convert a tensor into a valid image deprocess_image &amp;lt;-function(img) { img &amp;lt;-array_reshape(img, dim = c(dim(img)[[2]], dim(img)[[3]], 3)) # Undoes preprocessing that was performed by `imagenet_preprocess_input` img &amp;lt;-img /2 img &amp;lt;-img +0.</description></item><item><title>eager_cvae</title><link>/guide/keras/examples/eager_cvae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/eager_cvae/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_cvae.R
This is part of the companion code to the post “Representation learning with MMD-VAE” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) library(tfdatasets) library(dplyr) library(ggplot2) library(glue) # Setup and preprocessing ------------------------------------------------- fashion &amp;lt;-dataset_fashion_mnist() c(train_images, train_labels) %&amp;lt;-%fashion$train c(test_images, test_labels) %&amp;lt;-%fashion$test train_x &amp;lt;- train_images %&amp;gt;%`/`(255) %&amp;gt;%k_reshape(c(60000, 28, 28, 1)) test_x &amp;lt;- test_images %&amp;gt;%`/`(255) %&amp;gt;%k_reshape(c(10000, 28, 28, 1)) class_names =c(&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;) buffer_size &amp;lt;-60000 batch_size &amp;lt;-100 batches_per_epoch &amp;lt;-buffer_size /batch_size train_dataset &amp;lt;-tensor_slices_dataset(train_x) %&amp;gt;% dataset_shuffle(buffer_size) %&amp;gt;% dataset_batch(batch_size) test_dataset &amp;lt;-tensor_slices_dataset(test_x) %&amp;gt;% dataset_batch(10000) # Model ------------------------------------------------------------------- latent_dim &amp;lt;-2 encoder_model &amp;lt;-function(name = NULL) { keras_model_custom(name = name, function(self) { self$conv1 &amp;lt;- layer_conv_2d( filters = 32, kernel_size = 3, strides = 2, activation = &#34;</description></item><item><title>eager_dcgan</title><link>/guide/keras/examples/eager_dcgan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/eager_dcgan/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_dcgan.R
This is the companion code to the post “Generating digits with Keras and TensorFlow eager execution” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-08-26-eager-dcgan/
library(keras) use_implementation(&#34;tensorflow&#34;) use_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) library(tfdatasets) mnist &amp;lt;-dataset_mnist() c(train_images, train_labels) %&amp;lt;-%mnist$train train_images &amp;lt;-train_images %&amp;gt;% k_expand_dims() %&amp;gt;% k_cast(dtype = &#34;float32&#34;) train_images &amp;lt;-(train_images -127.5) /127.5 buffer_size &amp;lt;-60000 batch_size &amp;lt;-256 batches_per_epoch &amp;lt;-(buffer_size /batch_size) %&amp;gt;%round() train_dataset &amp;lt;-tensor_slices_dataset(train_images) %&amp;gt;% dataset_shuffle(buffer_size) %&amp;gt;% dataset_batch(batch_size) generator &amp;lt;- function(name = NULL) { keras_model_custom(name = name, function(self) { self$fc1 &amp;lt;-layer_dense(units = 7 *7 *64, use_bias = FALSE) self$batchnorm1 &amp;lt;-layer_batch_normalization() self$leaky_relu1 &amp;lt;-layer_activation_leaky_relu() self$conv1 &amp;lt;- layer_conv_2d_transpose( filters = 64, kernel_size = c(5, 5), strides = c(1, 1), padding = &#34;</description></item><item><title>eager_image_captioning</title><link>/guide/keras/examples/eager_image_captioning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/eager_image_captioning/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_image_captioning.R
This is the companion code to the post “Attention-based Image Captioning with Keras” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-09-17-eager-captioning
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) np &amp;lt;-import(&#34;numpy&#34;) library(tfdatasets) library(purrr) library(stringr) library(glue) library(rjson) library(rlang) library(dplyr) library(magick) maybecat &amp;lt;-function(context, x) { if (debugshapes) { name &amp;lt;-enexpr(x) dims &amp;lt;-paste0(dim(x), collapse = &#34; &#34;) cat(context, &#34;: shape of &#34;, name, &#34;: &#34;, dims, &#34;\n&#34;, sep = &#34;</description></item><item><title>eager_pix2pix</title><link>/guide/keras/examples/eager_pix2pix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/eager_pix2pix/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_pix2pix.R
This is the companion code to the post “Image-to-image translation with Pix2Pix: An implementation using Keras and eager execution” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-09-20-eager-pix2pix
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) library(tfdatasets) library(purrr) restore &amp;lt;-TRUE data_dir &amp;lt;-&#34;facades&#34; buffer_size &amp;lt;-400 batch_size &amp;lt;-1 batches_per_epoch &amp;lt;-buffer_size /batch_size img_width &amp;lt;-256L img_height &amp;lt;-256L load_image &amp;lt;-function(image_file, is_train) { image &amp;lt;-tf$read_file(image_file) image &amp;lt;-tf$image$decode_jpeg(image) w &amp;lt;-as.</description></item><item><title>eager_styletransfer</title><link>/guide/keras/examples/eager_styletransfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/eager_styletransfer/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/eager_styletransfer.R
This is the companion code to the post “Neural style transfer with eager execution and Keras” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-09-09-eager-style-transfer
library(keras) use_implementation(&#34;tensorflow&#34;) use_session_with_seed(7777, disable_gpu = FALSE, disable_parallel_cpu = FALSE) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) library(purrr) library(glue) img_shape &amp;lt;-c(128, 128, 3) content_path &amp;lt;-&#34;isar.jpg&#34; style_path &amp;lt;-&#34;The_Great_Wave_off_Kanagawa.jpg&#34; num_iterations &amp;lt;-2000 content_weight &amp;lt;-100 style_weight &amp;lt;-0.8 total_variation_weight &amp;lt;-0.01 content_image &amp;lt;- image_load(content_path, target_size = img_shape[1:2]) content_image %&amp;gt;%image_to_array() %&amp;gt;% `/`(.</description></item><item><title>fine_tuning</title><link>/guide/keras/examples/fine_tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/fine_tuning/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/fine_tuning.R
In this example we fine tune Mobile Net to better predict cats and dogs in photos. It also demonstrates the usage of image data generators for efficient preprocessing and training.
It’s preferable to run this example in a GPU.
# Download data ----------------------------------------------------------- download.file( &#34;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&#34;, destfile = &#34;cats-dogs.zip&#34; ) # Pre-processing ---------------------------------------------------------- zip::unzip(&#34;cats-dogs.zip&#34;, exdir = &#34;data-raw&#34;) # We will organize images in the following structure: # data/ # train/ # Cat/ # Dog/ # validation # Cat/ # Dog/ # test/ # images/ # all_imgs &amp;lt;-fs::dir_ls( &#34;</description></item><item><title>image_ocr</title><link>/guide/keras/examples/image_ocr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/image_ocr/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/image_ocr.R
library(keras)</description></item><item><title>imdb_bidirectional_lstm</title><link>/guide/keras/examples/imdb_bidirectional_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/imdb_bidirectional_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_bidirectional_lstm.R
Train a Bidirectional LSTM on the IMDB sentiment classification task.
Output after 4 epochs on CPU: ~0.8146 Time per epoch on CPU (Core i7): ~150s.
library(keras) # Define maximum number of input features max_features &amp;lt;-20000 # Cut texts after this number of words # (among top max_features most common words) maxlen &amp;lt;-100 batch_size &amp;lt;-32 # Load imdb dataset cat(&#39;Loading data...\n&#39;) imdb &amp;lt;-dataset_imdb(num_words = max_features) # Define training and test sets x_train &amp;lt;-imdb$train$x y_train &amp;lt;-imdb$train$y x_test &amp;lt;-imdb$test$x y_test &amp;lt;-imdb$test$y # Output lengths of testing and training sets cat(length(x_train), &#39;train sequences\n&#39;) cat(length(x_test), &#39;test sequences\n&#39;) cat(&#39;Pad sequences (samples x time)\n&#39;) # Pad training and test inputs x_train &amp;lt;-pad_sequences(x_train, maxlen = maxlen) x_test &amp;lt;-pad_sequences(x_test, maxlen = maxlen) # Output dimensions of training and test inputs cat(&#39;x_train shape:&#39;, dim(x_train), &#39;\n&#39;) cat(&#39;x_test shape:&#39;, dim(x_test), &#39;\n&#39;) # Initialize model model &amp;lt;-keras_model_sequential() model %&amp;gt;% # Creates dense embedding layer; outputs 3D tensor # with shape (batch_size, sequence_length, output_dim) layer_embedding(input_dim = max_features, output_dim = 128, input_length = maxlen) %&amp;gt;% bidirectional(layer_lstm(units = 64)) %&amp;gt;% layer_dropout(rate = 0.</description></item><item><title>imdb_cnn</title><link>/guide/keras/examples/imdb_cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/imdb_cnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R
Use Convolution1D for text classification.
Output after 2 epochs: ~0.89 Time per epoch on CPU (Intel i5 2.4Ghz): 90s Time per epoch on GPU (Tesla K40): 10s
library(keras) # Set parameters: max_features &amp;lt;-5000 maxlen &amp;lt;-400 batch_size &amp;lt;-32 embedding_dims &amp;lt;-50 filters &amp;lt;-250 kernel_size &amp;lt;-3 hidden_dims &amp;lt;-250 epochs &amp;lt;-2 # Data Preparation -------------------------------------------------------- # Keras load all data into a list with the following structure: # List of 2 # $ train:List of 2 # .</description></item><item><title>imdb_cnn_lstm</title><link>/guide/keras/examples/imdb_cnn_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/imdb_cnn_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn_lstm.R
Train a recurrent convolutional network on the IMDB sentiment classification task.
Achieves 0.8498 test accuracy after 2 epochs. 41s/epoch on K520 GPU.
library(keras) # Parameters -------------------------------------------------------------- # Embedding max_features =20000 maxlen =100 embedding_size =128 # Convolution kernel_size =5 filters =64 pool_size =4 # LSTM lstm_output_size =70 # Training batch_size =30 epochs =2 # Data Preparation -------------------------------------------------------- # The x data includes integer sequences, each integer is a word # The y data includes a set of integer labels (0 or 1) # The num_words argument indicates that only the max_fetures most frequent # words will be integerized.</description></item><item><title>imdb_fasttext</title><link>/guide/keras/examples/imdb_fasttext/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/imdb_fasttext/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_fasttext.R
This example demonstrates the use of fasttext for text classification
Based on Joulin et al’s paper: “Bags of Tricks for Efficient Text Classification” https://arxiv.org/abs/1607.01759
Results on IMDB datasets with uni and bi-gram embeddings: Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 CPU Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M GPU
library(keras) library(purrr) # Function Definitions ---------------------------------------------------- create_ngram_set &amp;lt;-function(input_list, ngram_value = 2){ indices &amp;lt;-map(0:(length(input_list) -ngram_value), ~1:ngram_value +.</description></item><item><title>imdb_lstm</title><link>/guide/keras/examples/imdb_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/imdb_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_lstm.R
Trains a LSTM on the IMDB sentiment classification task.
The dataset is actually too small for LSTM to be of any advantage compared to simpler, much faster methods such as TF-IDF + LogReg.
Notes: - RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won’t converge. - LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc.</description></item><item><title>lstm_benchmark</title><link>/guide/keras/examples/lstm_benchmark/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/lstm_benchmark/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_benchmark.R
library(keras)</description></item><item><title>lstm_seq2seq</title><link>/guide/keras/examples/lstm_seq2seq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/lstm_seq2seq/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.R
Sequence to sequence example in Keras (character-level).
This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain.
Algorithm
We start with input sequences from a domain (e.g. English sentences) and correspding target sequences from another domain (e.</description></item><item><title>lstm_stateful</title><link>/guide/keras/examples/lstm_stateful/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/lstm_stateful/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_stateful.R
# Example</description></item><item><title>lstm_text_generation</title><link>/guide/keras/examples/lstm_text_generation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/lstm_text_generation/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_text_generation.R
Example script to generate text from Nietzsche’s writings.
At least 20 epochs are required before the generated text starts sounding coherent.
It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive.
If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better.
library(keras) library(readr) library(stringr) library(purrr) library(tokenizers) # Parameters -------------------------------------------------------------- maxlen &amp;lt;-40 # Data Preparation -------------------------------------------------------- # Retrieve text path &amp;lt;-get_file( &#39;nietzsche.</description></item><item><title>mmd_cvae</title><link>/guide/keras/examples/mmd_cvae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mmd_cvae/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mmd_cvae.R
This is part of the companion code to the post “Representation learning with MMD-VAE” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-10-22-mmd-vae/
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) library(tfdatasets) library(dplyr) library(ggplot2) library(glue) # Setup and preprocessing ------------------------------------------------- fashion &amp;lt;-dataset_fashion_mnist() c(train_images, train_labels) %&amp;lt;-%fashion$train c(test_images, test_labels) %&amp;lt;-%fashion$test train_x &amp;lt;- train_images %&amp;gt;%`/`(255) %&amp;gt;%k_reshape(c(60000, 28, 28, 1)) test_x &amp;lt;- test_images %&amp;gt;%`/`(255) %&amp;gt;%k_reshape(c(10000, 28, 28, 1)) class_names =c(&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;) buffer_size &amp;lt;-60000 batch_size &amp;lt;-100 batches_per_epoch &amp;lt;-buffer_size /batch_size train_dataset &amp;lt;-tensor_slices_dataset(train_x) %&amp;gt;% dataset_shuffle(buffer_size) %&amp;gt;% dataset_batch(batch_size) test_dataset &amp;lt;-tensor_slices_dataset(test_x) %&amp;gt;% dataset_batch(10000) # Model ------------------------------------------------------------------- latent_dim &amp;lt;-2 encoder_model &amp;lt;-function(name = NULL) { keras_model_custom(name = name, function(self) { self$conv1 &amp;lt;- layer_conv_2d( filters = 32, kernel_size = 3, strides = 2, activation = &#34;</description></item><item><title>mnist_acgan</title><link>/guide/keras/examples/mnist_acgan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_acgan/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_acgan.R
Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the MNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.
You should start to see reasonable images after ~5 epochs, and good images by ~15 epochs. You should use a GPU, as the convolution-heavy operations are very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating, as the compilation time can be a blocker using Theano.</description></item><item><title>mnist_antirectifier</title><link>/guide/keras/examples/mnist_antirectifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_antirectifier/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_antirectifier.R
Demonstrates how to write custom layers for Keras.
We build a custom activation layer called ‘Antirectifier’, which modifies the shape of the tensor that passes through it. We need to specify two methods: compute_output_shape and call.
Note that the same result can also be achieved via a Lambda layer.
library(keras) # Data Preparation -------------------------------------------------------- batch_size &amp;lt;-128 num_classes &amp;lt;-10 epochs &amp;lt;-40 # The data, shuffled and split between train and test sets mnist &amp;lt;-dataset_mnist() x_train &amp;lt;-mnist$train$x y_train &amp;lt;-mnist$train$y x_test &amp;lt;-mnist$test$x y_test &amp;lt;-mnist$test$y # Redimension x_train &amp;lt;-array_reshape(x_train, c(nrow(x_train), 784)) x_test &amp;lt;-array_reshape(x_test, c(nrow(x_test), 784)) # Transform RGB values into [0,1] range x_train &amp;lt;-x_train /255 x_test &amp;lt;-x_test /255 cat(nrow(x_train), &#39;train samples\n&#39;) cat(nrow(x_test), &#39;test samples\n&#39;) # Convert class vectors to binary class matrices y_train &amp;lt;-to_categorical(y_train, num_classes) y_test &amp;lt;-to_categorical(y_test, num_classes) # Antirectifier Layer ----------------------------------------------------- This is the combination of a sample-wise L2 normalization with the concatenation of the positive part of the input with the negative part of the input.</description></item><item><title>mnist_cnn</title><link>/guide/keras/examples/mnist_cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_cnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_cnn.R
Trains a simple convnet on the MNIST dataset.
Gets to 99.25% test accuracy after 12 epochs Note: There is still a large margin for parameter tuning
16 seconds per epoch on a GRID K520 GPU.
library(keras) # Data Preparation ----------------------------------------------------- batch_size &amp;lt;-128 num_classes &amp;lt;-10 epochs &amp;lt;-12 # Input image dimensions img_rows &amp;lt;-28 img_cols &amp;lt;-28 # The data, shuffled and split between train and test sets mnist &amp;lt;-dataset_mnist() x_train &amp;lt;-mnist$train$x y_train &amp;lt;-mnist$train$y x_test &amp;lt;-mnist$test$x y_test &amp;lt;-mnist$test$y # Redefine dimension of train/test inputs x_train &amp;lt;-array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1)) x_test &amp;lt;-array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1)) input_shape &amp;lt;-c(img_rows, img_cols, 1) # Transform RGB values into [0,1] range x_train &amp;lt;-x_train /255 x_test &amp;lt;-x_test /255 cat(&#39;x_train_shape:&#39;, dim(x_train), &#39;\n&#39;) cat(nrow(x_train), &#39;train samples\n&#39;) cat(nrow(x_test), &#39;test samples\n&#39;) # Convert class vectors to binary class matrices y_train &amp;lt;-to_categorical(y_train, num_classes) y_test &amp;lt;-to_categorical(y_test, num_classes) # Define Model ----------------------------------------------------------- # Define model model &amp;lt;-keras_model_sequential() %&amp;gt;% layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = &#39;relu&#39;, input_shape = input_shape) %&amp;gt;% layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = &#39;relu&#39;) %&amp;gt;% layer_max_pooling_2d(pool_size = c(2, 2)) %&amp;gt;% layer_dropout(rate = 0.</description></item><item><title>mnist_cnn_embeddings</title><link>/guide/keras/examples/mnist_cnn_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_cnn_embeddings/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_cnn_embeddings.R
This example shows how to visualize embeddings in TensorBoard.
Embeddings in the sense used here don’t necessarily refer to embedding layers. In fact, features (= activations) from other hidden layers can be visualized, as shown in this example for a dense layer.
library(keras) # Data Preparation ----------------------------------------------------- batch_size &amp;lt;-128 num_classes &amp;lt;-10 epochs &amp;lt;-12 # Input image dimensions img_rows &amp;lt;-28 img_cols &amp;lt;-28 # The data, shuffled and split between train and test sets mnist &amp;lt;-dataset_mnist() x_train &amp;lt;-mnist$train$x y_train &amp;lt;-mnist$train$y x_test &amp;lt;-mnist$test$x y_test &amp;lt;-mnist$test$y # Redefine dimension of train/test inputs x_train &amp;lt;- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1)) x_test &amp;lt;- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1)) input_shape &amp;lt;-c(img_rows, img_cols, 1) # Transform RGB values into [0,1] range x_train &amp;lt;-x_train /255 x_test &amp;lt;-x_test /255 cat(&#39;x_train_shape:&#39;, dim(x_train), &#39;\n&#39;) cat(nrow(x_train), &#39;train samples\n&#39;) cat(nrow(x_test), &#39;test samples\n&#39;) # Prepare for logging embeddings -------------------------------------------------- embeddings_dir &amp;lt;-file.</description></item><item><title>mnist_dataset_api</title><link>/guide/keras/examples/mnist_dataset_api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_dataset_api/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_dataset_api.R
MNIST classification with TensorFlow’s Dataset API.</description></item><item><title>mnist_hierarchical_rnn</title><link>/guide/keras/examples/mnist_hierarchical_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_hierarchical_rnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_hierarchical_rnn.R
This is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.
HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence. Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors) into a sentence vector. The second recurrent layer then encodes a sequence of such vectors (encoded by the first layer) into a document vector. This document vector is considered to preserve both the word-level and sentence-level structure of the context.</description></item><item><title>mnist_irnn</title><link>/guide/keras/examples/mnist_irnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_irnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_irnn.R
This is a reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton
arxiv:1504.00941v2 [cs.NE] 7 Apr 2015 http://arxiv.org/pdf/1504.00941v2.pdf
Optimizer is replaced with RMSprop which yields more stable and steady improvement.
Reaches 0.93 train/test accuracy after 900 epochs This corresponds to roughly 1687500 steps in the original paper.</description></item><item><title>mnist_mlp</title><link>/guide/keras/examples/mnist_mlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_mlp/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_mlp.R
Trains a simple deep NN on the MNIST dataset.
Gets to 98.40% test accuracy after 20 epochs (there is a lot of margin for parameter tuning). 2 seconds per epoch on a K520 GPU.
library(keras) # Data Preparation --------------------------------------------------- batch_size &amp;lt;-128 num_classes &amp;lt;-10 epochs &amp;lt;-30 # The data, shuffled and split between train and test sets c(c(x_train, y_train), c(x_test, y_test)) %&amp;lt;-%dataset_mnist() x_train &amp;lt;-array_reshape(x_train, c(nrow(x_train), 784)) x_test &amp;lt;-array_reshape(x_test, c(nrow(x_test), 784)) # Transform RGB values into [0,1] range x_train &amp;lt;-x_train /255 x_test &amp;lt;-x_test /255 cat(nrow(x_train), &#39;train samples\n&#39;) cat(nrow(x_test), &#39;test samples\n&#39;) # Convert class vectors to binary class matrices y_train &amp;lt;-to_categorical(y_train, num_classes) y_test &amp;lt;-to_categorical(y_test, num_classes) # Define Model -------------------------------------------------------------- model &amp;lt;-keras_model_sequential() model %&amp;gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(784)) %&amp;gt;% layer_dropout(rate = 0.</description></item><item><title>mnist_net2net</title><link>/guide/keras/examples/mnist_net2net/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_net2net/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_net2net.R
library(keras)</description></item><item><title>mnist_siamese_graph</title><link>/guide/keras/examples/mnist_siamese_graph/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_siamese_graph/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_siamese_graph.R
library(keras)</description></item><item><title>mnist_swwae</title><link>/guide/keras/examples/mnist_swwae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_swwae/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_swwae.R
library(keras)</description></item><item><title>mnist_tfrecord</title><link>/guide/keras/examples/mnist_tfrecord/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_tfrecord/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_tfrecord.R
MNIST dataset with TFRecords, the standard TensorFlow data format.
TFRecord is a data format supported throughout TensorFlow. This example demonstrates how to load TFRecord data using Input Tensors. Input Tensors differ from the normal Keras workflow because instead of fitting to data loaded into a a numpy array, data is supplied via a special tensor that reads data from nodes that are wired directly into model graph with the layer_input(tensor=input_tensor) parameter.</description></item><item><title>mnist_transfer_cnn</title><link>/guide/keras/examples/mnist_transfer_cnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/mnist_transfer_cnn/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_transfer_cnn.R
Transfer learning toy example:
Train a simple convnet on the MNIST dataset the first 5 digits [0..4]. Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]. library(keras) now &amp;lt;-Sys.time() batch_size &amp;lt;-128 num_classes &amp;lt;-5 epochs &amp;lt;-5 # input image dimensions img_rows &amp;lt;-28 img_cols &amp;lt;-28 # number of convolutional filters to use filters &amp;lt;-32 # size of pooling area for max pooling pool_size &amp;lt;-2 # convolution kernel size kernel_size &amp;lt;-c(3, 3) # input shape input_shape &amp;lt;-c(img_rows, img_cols, 1) # the data, shuffled and split between train and test sets data &amp;lt;-dataset_mnist() x_train &amp;lt;-data$train$x y_train &amp;lt;-data$train$y x_test &amp;lt;-data$test$x y_test &amp;lt;-data$test$y # create two datasets one with digits below 5 and one with 5 and above x_train_lt5 &amp;lt;-x_train[y_train &amp;lt;5] y_train_lt5 &amp;lt;-y_train[y_train &amp;lt;5] x_test_lt5 &amp;lt;-x_test[y_test &amp;lt;5] y_test_lt5 &amp;lt;-y_test[y_test &amp;lt;5] x_train_gte5 &amp;lt;-x_train[y_train &amp;gt;=5] y_train_gte5 &amp;lt;-y_train[y_train &amp;gt;=5] -5 x_test_gte5 &amp;lt;-x_test[y_test &amp;gt;=5] y_test_gte5 &amp;lt;-y_test[y_test &amp;gt;=5] -5 # define two groups of layers: feature (convolutions) and classification (dense) feature_layers &amp;lt;- layer_conv_2d(filters = filters, kernel_size = kernel_size, input_shape = input_shape) %&amp;gt;% layer_activation(activation = &#39;relu&#39;) %&amp;gt;% layer_conv_2d(filters = filters, kernel_size = kernel_size) %&amp;gt;% layer_activation(activation = &#39;relu&#39;) %&amp;gt;% layer_max_pooling_2d(pool_size = pool_size) %&amp;gt;% layer_dropout(rate = 0.</description></item><item><title>neural_style_transfer</title><link>/guide/keras/examples/neural_style_transfer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/neural_style_transfer/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/neural_style_transfer.R
Neural style transfer with Keras.
It is preferable to run this script on a GPU, for speed.
Example result: https://twitter.com/fchollet/status/686631033085677568
Style transfer consists in generating an image with the same “content” as a base image, but with the “style” of a different picture (typically artistic).
This is achieved through the optimization of a loss function that has 3 components: “style loss”, “content loss”, and “total variation loss”:</description></item><item><title>nmt_attention</title><link>/guide/keras/examples/nmt_attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/nmt_attention/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/nmt_attention.R
This is the companion code to the post “Attention-based Neural Machine Translation with Keras” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2018-07-30-attention-layer/
library(tensorflow) library(keras) library(tfdatasets) library(purrr) library(stringr) library(reshape2) library(viridis) library(ggplot2) library(tibble) # Preprocessing ----------------------------------------------------------- # Assumes you&#39;ve downloaded and unzipped one of the bilingual datasets offered at # http://www.manythings.org/anki/ and put it into a directory &#34;data&#34; # This example translates English to Dutch. filepath &amp;lt;-file.</description></item><item><title>nueral_doodle</title><link>/guide/keras/examples/nueral_doodle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/nueral_doodle/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/nueral_doodle.R
library(keras)</description></item><item><title>pretrained_word_embeddings</title><link>/guide/keras/examples/pretrained_word_embeddings/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/pretrained_word_embeddings/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/pretrained_word_embeddings.R
This script loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset (classication of newsgroup messages into 20 different categories).
GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip (source page: http://nlp.stanford.edu/projects/glove/)
20 Newsgroup data can be found at: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html
IMPORTANT NOTE: This example does yet work correctly. The code executes fine and appears to mimic the Python code upon which it is based however it achieves only half the training accuracy that the Python code does so there is clearly a subtle difference.</description></item><item><title>quora_siamese_lstm</title><link>/guide/keras/examples/quora_siamese_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/quora_siamese_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/quora_siamese_lstm.R
In this tutorial we will use Keras to classify duplicated questions from Quora. The dataset first appeared in the Kaggle competition Quora Question Pairs. The dataset consists of ~400k pairs of questions and a column indicating if the question pair is duplicated.
Our implementation is inspired by the Siamese Recurrent Architecture, Mueller et al. Siamese recurrent architectures for learning sentence similarity, with small modifications like the similarity measure and the embedding layers (The original paper uses pre-trained word vectors).</description></item><item><title>reuters_mlp</title><link>/guide/keras/examples/reuters_mlp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/reuters_mlp/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/reuters_mlp.R
Train and evaluate a simple MLP on the Reuters newswire topic classification task.
library(keras) max_words &amp;lt;-1000 batch_size &amp;lt;-32 epochs &amp;lt;-5 cat(&#39;Loading data...\n&#39;) reuters &amp;lt;-dataset_reuters(num_words = max_words, test_split = 0.2) x_train &amp;lt;-reuters$train$x y_train &amp;lt;-reuters$train$y x_test &amp;lt;-reuters$test$x y_test &amp;lt;-reuters$test$y cat(length(x_train), &#39;train sequences\n&#39;) cat(length(x_test), &#39;test sequences\n&#39;) num_classes &amp;lt;-max(y_train) +1 cat(num_classes, &#39;\n&#39;) cat(&#39;Vectorizing sequence data...\n&#39;) tokenizer &amp;lt;-text_tokenizer(num_words = max_words) x_train &amp;lt;-sequences_to_matrix(tokenizer, x_train, mode = &#39;binary&#39;) x_test &amp;lt;-sequences_to_matrix(tokenizer, x_test, mode = &#39;binary&#39;) cat(&#39;x_train shape:&#39;, dim(x_train), &#39;\n&#39;) cat(&#39;x_test shape:&#39;, dim(x_test), &#39;\n&#39;) cat(&#39;Convert class vector to binary class matrix&#39;, &#39;(for use with categorical_crossentropy)\n&#39;) y_train &amp;lt;-to_categorical(y_train, num_classes) y_test &amp;lt;-to_categorical(y_test, num_classes) cat(&#39;y_train shape:&#39;, dim(y_train), &#39;\n&#39;) cat(&#39;y_test shape:&#39;, dim(y_test), &#39;\n&#39;) cat(&#39;Building model.</description></item><item><title>reuters_mlp_relu_vs_selu</title><link>/guide/keras/examples/reuters_mlp_relu_vs_selu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/reuters_mlp_relu_vs_selu/</guid><description> Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/reuters_mlp_relu_vs_selu.R
library(keras)</description></item><item><title>stateful_lstm</title><link>/guide/keras/examples/stateful_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/stateful_lstm/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/stateful_lstm.R
Example script showing how to use stateful RNNs to model long sequences efficiently.
library(keras) # since we are using stateful rnn tsteps can be set to 1 tsteps &amp;lt;-1 batch_size &amp;lt;-25 epochs &amp;lt;-25 # number of elements ahead that are used to make the prediction lahead &amp;lt;-1 # Generates an absolute cosine time series with the amplitude exponentially decreasing # Arguments: # amp: amplitude of the cosine function # period: period of the cosine function # x0: initial x of the time series # xn: final x of the time series # step: step of the time series discretization # k: exponential rate gen_cosine_amp &amp;lt;-function(amp = 100, period = 1000, x0 = 0, xn = 50000, step = 1, k = 0.</description></item><item><title>text_explanation_lime</title><link>/guide/keras/examples/text_explanation_lime/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/text_explanation_lime/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/text_explanation_lime.R
This example shows how to use lime to explain text data.
library(readr) library(dplyr) library(keras) library(tidyverse) # Download and unzip data activity_url &amp;lt;-&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip&#34; temp &amp;lt;-tempfile() download.file(activity_url, temp) unzip(temp, &#34;drugLibTest_raw.tsv&#34;) # Read dataset df &amp;lt;-read_delim(&#39;drugLibTest_raw.tsv&#39;,delim = &#39;\t&#39;) unlink(temp) # Select only rating and text from the whole dataset df =df %&amp;gt;%select(rating,commentsReview) %&amp;gt;%mutate(rating = if_else(rating &amp;gt;=8, 0, 1)) # This is our text text &amp;lt;-df$commentsReview # And these are ratings given by customers y_train &amp;lt;-df$rating # text_tokenizer helps us to turn each word into integers.</description></item><item><title>tfprob_vae</title><link>/guide/keras/examples/tfprob_vae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/tfprob_vae/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/tfprob_vae.R
This is the companion code to the post “Getting started with TensorFlow Probability from R” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2019-01-08-getting-started-with-tf-probability/
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) tfp &amp;lt;-import(&#34;tensorflow_probability&#34;) tfd &amp;lt;-tfp$distributions library(tfdatasets) library(dplyr) library(glue) # Utilities -------------------------------------------------------- num_examples_to_generate &amp;lt;-64L generate_random &amp;lt;-function(epoch) { decoder_likelihood &amp;lt;- decoder(latent_prior$sample(num_examples_to_generate)) predictions &amp;lt;-decoder_likelihood$mean() # change path according to your preferences png(file.path(&#34;/tmp&#34;, paste0(&#34;random_epoch_&#34;, epoch, &#34;.png&#34;))) par(mfcol = c(8, 8)) par(mar = c(0.</description></item><item><title>unet</title><link>/guide/keras/examples/unet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/unet/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/unet.R
To run this example:
Download the train.zip and train_masks.zip files from: https://www.kaggle.com/c/carvana-image-masking-challenge/data
Create an “input” directory and extract the zip files into it (after this there should be “train” and “train_masks” subdirectories within the “input” directory).
#` This code runs only on Windows because of specific parallel backend. #` You can find Linux version here: https://keras.rstudio.com/articles/examples/unet_linux.html #` unet architecture is based on original Python code #` from https://github.</description></item><item><title>unet_linux</title><link>/guide/keras/examples/unet_linux/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/unet_linux/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/unet_linux.R
To run this example:
Download the train.zip and train_masks.zip files from: https://www.kaggle.com/c/carvana-image-masking-challenge/data
Create an “input” directory and extract the zip files into it (after this there should be “train” and “train_masks” subdirectories within the “input” directory).
#` This code runs only on Linux because of specific parallel backend. #` You can find Windows version here: https://keras.rstudio.com/articles/examples/unet.html #` unet architecture is based on original Python code #` from https://github.</description></item><item><title>variational_autoencoder</title><link>/guide/keras/examples/variational_autoencoder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/variational_autoencoder/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder.R
This script demonstrates how to build a variational autoencoder with Keras. Reference: “Auto-Encoding Variational Bayes” https://arxiv.org/abs/1312.6114
library(keras) K &amp;lt;-keras::backend() # Parameters -------------------------------------------------------------- batch_size &amp;lt;-100L original_dim &amp;lt;-784L latent_dim &amp;lt;-2L intermediate_dim &amp;lt;-256L epochs &amp;lt;-50L epsilon_std &amp;lt;-1.0 # Model definition -------------------------------------------------------- x &amp;lt;-layer_input(shape = c(original_dim)) h &amp;lt;-layer_dense(x, intermediate_dim, activation = &#34;relu&#34;) z_mean &amp;lt;-layer_dense(h, latent_dim) z_log_var &amp;lt;-layer_dense(h, latent_dim) sampling &amp;lt;-function(arg){ z_mean &amp;lt;-arg[, 1:(latent_dim)] z_log_var &amp;lt;-arg[, (latent_dim +1):(2 *latent_dim)] epsilon &amp;lt;-k_random_normal( shape = c(k_shape(z_mean)[[1]]), mean=0.</description></item><item><title>variational_autoencoder_deconv</title><link>/guide/keras/examples/variational_autoencoder_deconv/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/variational_autoencoder_deconv/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/variational_autoencoder_deconv.R
This script demonstrates how to build a variational autoencoder with Keras and deconvolution layers. Reference: “Auto-Encoding Variational Bayes” https://arxiv.org/abs/1312.6114
library(keras) K &amp;lt;-keras::backend() #### Parameterization #### # input image dimensions img_rows &amp;lt;-28L img_cols &amp;lt;-28L # color channels (1 = grayscale, 3 = RGB) img_chns &amp;lt;-1L # number of convolutional filters to use filters &amp;lt;-64L # convolution kernel size num_conv &amp;lt;-3L latent_dim &amp;lt;-2L intermediate_dim &amp;lt;-128L epsilon_std &amp;lt;-1.</description></item><item><title>vq_vae</title><link>/guide/keras/examples/vq_vae/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/keras/examples/vq_vae/</guid><description>Source: https://github.com/rstudio/keras/blob/master/vignettes/examples/vq_vae.R
This is the companion code to the post “Discrete Representation Learning with VQ-VAE and TensorFlow Probability” on the TensorFlow for R blog.
https://blogs.rstudio.com/tensorflow/posts/2019-01-24-vq-vae/
library(keras) use_implementation(&#34;tensorflow&#34;) library(tensorflow) tfe_enable_eager_execution(device_policy = &#34;silent&#34;) use_session_with_seed(7778, disable_gpu = FALSE, disable_parallel_cpu = FALSE) tfp &amp;lt;-import(&#34;tensorflow_probability&#34;) tfd &amp;lt;-tfp$distributions library(tfdatasets) library(dplyr) library(glue) library(curry) moving_averages &amp;lt;-tf$python$training$moving_averages # Utilities -------------------------------------------------------- visualize_images &amp;lt;- function(dataset, epoch, reconstructed_images, random_images) { write_png(dataset, epoch, &#34;reconstruction&#34;, reconstructed_images) write_png(dataset, epoch, &#34;</description></item></channel></rss>