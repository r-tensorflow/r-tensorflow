<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow Hub Examples on TensorFlow for R</title><link>/guide/tfhub/examples/</link><description>Recent content in TensorFlow Hub Examples on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/guide/tfhub/examples/index.xml" rel="self" type="application/rss+xml"/><item><title>biggan_image_generation</title><link>/guide/tfhub/examples/biggan_image_generation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfhub/examples/biggan_image_generation/</guid><description>Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/biggan_image_generation.R
This example is a demo of BigGAN image generators available on TF Hub.
See this jupyter notebook for more info.
This example currently requires TensorFlow 2.0 Nightly preview. It can be installed with reticulate::py_install(“tf-nightly-2.0-preview”, pip = TRUE)
# Setup ------------------------------------------------------------------- library(tensorflow) library(tfhub) module &amp;lt;-hub_load(handle = &#34;https://tfhub.dev/deepmind/biggan-deep-256/1&#34;) # ImageNet label ---------------------------------------------------------- # Select the ImageNet label you want to generate images for. imagenet_labels &amp;lt;-jsonlite::fromJSON(&#34;</description></item><item><title>feature_column</title><link>/guide/tfhub/examples/feature_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfhub/examples/feature_column/</guid><description>Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/feature_column.R
In this example we will use the PetFinder dataset to demonstrate the feature_spec functionality with TensorFlow Hub.
Currently, we need TensorFlow 2.0 nightly and disable eager execution in order for this example to work.
Waiting for https://github.com/tensorflow/hub/issues/333
library(keras) library(tfhub) library(tfdatasets) library(readr) library(dplyr) tf$compat$v1$disable_eager_execution() # Read data --------------------------------------------------------------- dataset &amp;lt;-read_csv(&#34;train.csv&#34;) %&amp;gt;% filter(PhotoAmt &amp;gt;0) %&amp;gt;% mutate(img_path = path.expand(paste0(&#34;train_images/&#34;, PetID, &#34;-1.jpg&#34;))) %&amp;gt;% mutate_at(vars(Breed1:Health, State), as.character) %&amp;gt;% sample_n(size = nrow(.</description></item><item><title>image_classification</title><link>/guide/tfhub/examples/image_classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfhub/examples/image_classification/</guid><description>Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/image_classification.R
In this example we will use pre-trained features from the Mobile Net model to create an image classifier to the CIFAR-100 dataset.
library(keras) library(tfhub) # Get data ---------------------------------------------------------------- cifar &amp;lt;-dataset_cifar100() # Build the model --------------------------------------------------------- feature_model &amp;lt;-&#34;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&#34; input &amp;lt;-layer_input(shape = c(32, 32, 3)) resize_and_scale &amp;lt;-function(x) { tf$image$resize(x/255, size = shape(224, 224)) } output &amp;lt;-input %&amp;gt;% layer_lambda(f = resize_and_scale) %&amp;gt;% layer_hub(handle = feature_model) %&amp;gt;% layer_dense(units = 10, activation = &#34;</description></item><item><title>recipes</title><link>/guide/tfhub/examples/recipes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfhub/examples/recipes/</guid><description>Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/recipes.R
In this example we use tfhub and recipes to obtain pre-trained sentence embeddings. We then firt a logistic regression model.
The dataset comes from the Toxic Comment Classification Challenge in Kaggle and can be downlaoded here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data
library(tfhub) library(readr) library(tidymodels) # Read data --------------------------------------------------------------- comments &amp;lt;-read_csv(&#34;train.csv.zip&#34;) ind_train &amp;lt;-sample.int(nrow(comments), 0.8*nrow(comments)) train &amp;lt;-comments[ind_train,] test &amp;lt;-comments[-ind_train,] # Create our recipe specification ----------------------------------------- rec &amp;lt;-recipe( obscene ~comment_text, data = train ) %&amp;gt;%step_pretrained_text_embedding( comment_text, handle = &#34;</description></item><item><title>text_classification</title><link>/guide/tfhub/examples/text_classification/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfhub/examples/text_classification/</guid><description>Source: https://github.com/rstudio/tfhub/blob/master/vignettes/examples/text_classification.R
In this example we use tfhub to obtain pre-trained word-mbeddings and we use the word vectors to identify and classify toxic comments.
The dataset comes from the Toxic Comment Classification Challenge in Kaggle and can be downlaoded here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data
library(keras) library(tfhub) library(readr) # Read data --------------------------------------------------------------- comments &amp;lt;-read_csv(&#34;train.csv.zip&#34;) ind_train &amp;lt;-sample.int(nrow(comments), 0.8*nrow(comments)) train &amp;lt;-comments[ind_train,] test &amp;lt;-comments[-ind_train,] # Build the model --------------------------------------------------------- # We the token based text embedding trained on English Google News 130GB corpus.</description></item></channel></rss>