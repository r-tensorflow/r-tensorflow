<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow Estimator API Examples on TensorFlow for R</title><link>/guide/tfestimators/examples/</link><description>Recent content in TensorFlow Estimator API Examples on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/guide/tfestimators/examples/index.xml" rel="self" type="application/rss+xml"/><item><title>custom_estimator</title><link>/guide/tfestimators/examples/custom_estimator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/custom_estimator/</guid><description>Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/custom_estimator.R
In this article, we’ll develop a custom estimator to be used with the Abalone dataset. This dataset provides information on the physical characteristics of a number of abalones (a type of sea snail), and use these characteristics to predict the number of rings in the shell. As described at https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names:
Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope – a boring and time-consuming task.</description></item><item><title>iris_custom_decay_dnn</title><link>/guide/tfestimators/examples/iris_custom_decay_dnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/iris_custom_decay_dnn/</guid><description>Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/iris_custom_decay_dnn.R
library(tfestimators) # Construct the input inputs &amp;lt;-input_fn( iris, response = &#34;Species&#34;, features = c( &#34;Sepal.Length&#34;, &#34;Sepal.Width&#34;, &#34;Petal.Length&#34;, &#34;Petal.Width&#34;), batch_size = 10 ) custom_model_fn &amp;lt;-function(features, labels, mode, params, config) { # Create three fully connected layers respectively of size 10, 20, and 10 with # each layer having a dropout probability of 0.1. logits &amp;lt;-features %&amp;gt;% tf$contrib$layers$stack( tf$contrib$layers$fully_connected, c(10L, 20L, 10L), normalizer_fn = tf$contrib$layers$dropout, normalizer_params = list(keep_prob = 0.</description></item><item><title>iris_dnn_classifier</title><link>/guide/tfestimators/examples/iris_dnn_classifier/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/iris_dnn_classifier/</guid><description> Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/iris_dnn_classifier.R
library(tfestimators) response &amp;lt;-function() &#34;Species&#34; features &amp;lt;-function() setdiff(names(iris), response()) # split into train, test datasets set.seed(123) partitions &amp;lt;-modelr::resample_partition(iris, c(test = 0.2, train = 0.8)) iris_train &amp;lt;-as.data.frame(partitions$train) iris_test &amp;lt;-as.data.frame(partitions$test) # construct feature columns feature_columns &amp;lt;-feature_columns( column_numeric(features()) ) # construct classifier classifier &amp;lt;-dnn_classifier( feature_columns = feature_columns, hidden_units = c(10, 20, 10), n_classes = 3 ) # construct input function iris_input_fn &amp;lt;-function(data) { input_fn(data, features = features(), response = response()) } # train classifier with training dataset train(classifier, input_fn = iris_input_fn(iris_train)) # valuate with test dataset predictions &amp;lt;-predict(classifier, input_fn = iris_input_fn(iris_test)) evaluation &amp;lt;-evaluate(classifier, input_fn = iris_input_fn(iris_test))</description></item><item><title>mnist</title><link>/guide/tfestimators/examples/mnist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/mnist/</guid><description>Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/mnist.R
library(ggplot2) library(reshape2) library(tensorflow) library(tfestimators) # initialize data directory data_dir &amp;lt;-&#34;mnist-data&#34; dir.create(data_dir, recursive = TRUE, showWarnings = FALSE) # download the MNIST data sets, and read them into R sources &amp;lt;-list( train = list( x = &#34;https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz&#34;, y = &#34;https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz&#34; ), test = list( x = &#34;https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz&#34;, y = &#34;https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz&#34; ) ) # read an MNIST file (encoded in IDX format) read_idx &amp;lt;-function(file) { # create binary connection to file conn &amp;lt;-gzfile(file, open = &#34;</description></item><item><title>tensorflow_layers</title><link>/guide/tfestimators/examples/tensorflow_layers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/tensorflow_layers/</guid><description>Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/tensorflow_layers.R
library(tensorflow) library(tfestimators) tf$logging$set_verbosity(tf$logging$INFO) cnn_model_fn &amp;lt;-function(features, labels, mode, params, config) { # Input Layer # Reshape X to 4-D tensor: [batch_size, width, height, channels] # MNIST images are 28x28 pixels, and have one color channel input_layer &amp;lt;-tf$reshape(features$x, c(-1L, 28L, 28L, 1L)) # Convolutional Layer #1 # Computes 32 features using a 5x5 filter with ReLU activation. # Padding is added to preserve width and height.</description></item><item><title>wide_and_deep</title><link>/guide/tfestimators/examples/wide_and_deep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/guide/tfestimators/examples/wide_and_deep/</guid><description>Source: https://github.com/rstudio/tfestimators/blob/master/vignettes/examples/wide_and_deep.R
In this example, we’ll introduce how to use the TensorFlow Estimators API to jointly train a wide linear model and a deep feed-forward neural network. This approach combines the strengths of memorization and generalization. It’s useful for generic large-scale regression and classification problems with sparse input features (e.g., categorical features with a large number of possible feature values). If you’re interested in learning more about how Wide &amp;amp; Deep Learning works, please check out the white paper.</description></item></channel></rss>