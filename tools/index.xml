<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Overview on TensorFlow for R</title><link>/tools/</link><description>Recent content in Overview on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/tools/index.xml" rel="self" type="application/rss+xml"/><item><title>Hyperparameter Tuning</title><link>/tools/tfruns/tuning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/tools/tfruns/tuning/</guid><description>Overview Tuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not by changing the source code of the training script as we did above, but instead by defining flags for key parameters then training over the combinations of those flags to determine which combination of flags yields the best model.
Training Flags Here’s a declaration of 2 flags that control dropout rate within a model:</description></item><item><title>Managing Runs</title><link>/tools/tfruns/managing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/tools/tfruns/managing/</guid><description>Run Output Any graphical or console output as well as file artifacts created by a training run (e.g. saved models or saved model weights) can be viewed from the Output tab of the run view:
You can use the copy_run_files() function to export file artifacts from runs into another directory. For example:
copy_run_files(&#34;runs/2017-09-24T10-54-00Z&#34;, to = &#34;saved-model&#34;) You can also use the copy_run() function to export a run directory in it’s entirety.</description></item><item><title>TensorBoard</title><link>/tools/tensorboard/tensorboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/tools/tensorboard/tensorboard/</guid><description>Overview The computations you’ll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, a suite of visualization tools called TensorBoard is available. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it.</description></item><item><title>tfruns: Track and Visualize Training Runs</title><link>/tools/tfruns/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/tools/tfruns/overview/</guid><description>The tfruns package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R:
Track the hyperparameters, metrics, output, and source code of every training run.
Compare hyperparmaeters and metrics across runs to find the best performing model.
Automatically generate reports to visualize individual training runs or comparisons between runs.
No changes to source code required (run data is automatically captured for all Keras and TF Estimator models).</description></item></channel></rss>