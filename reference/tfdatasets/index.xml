<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow for R</title><link>/reference/tfdatasets/</link><description>Recent content on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/reference/tfdatasets/index.xml" rel="self" type="application/rss+xml"/><item><title>A dataset comprising lines from one or more text files.</title><link>/reference/tfdatasets/text_line_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/text_line_dataset/</guid><description>A dataset comprising lines from one or more text files.
text_line_dataset( filenames, compression_type = NULL, record_spec = NULL, parallel_records = NULL ) Arguments filenames String(s) specifying one or more filenames
compression_type A string, one of: NULL (no compression), &#34;ZLIB&#34;, or &#34;GZIP&#34;.
record_spec (Optional) Specification used to decode delimimted text lines into records (see delim_record_spec()).
parallel_records (Optional) An integer, representing the number of records to decode in parallel.</description></item><item><title>A dataset comprising records from one or more TFRecord files.</title><link>/reference/tfdatasets/tfrecord_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/tfrecord_dataset/</guid><description>A dataset comprising records from one or more TFRecord files.
tfrecord_dataset( filenames, compression_type = NULL, buffer_size = NULL, num_parallel_reads = NULL ) Arguments filenames String(s) specifying one or more filenames
compression_type A string, one of: NULL (no compression), &#34;ZLIB&#34;, or &#34;GZIP&#34;.
buffer_size An integer representing the number of bytes in the read buffer. (0 means no buffering).
num_parallel_reads An integer representing the number of files to read in parallel.</description></item><item><title>A dataset consisting of the results from a SQL query</title><link>/reference/tfdatasets/sql_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/sql_dataset/</guid><description>A dataset consisting of the results from a SQL query
sql_record_spec(names, types) sql_dataset(driver_name, data_source_name, query, record_spec) sqlite_dataset(filename, query, record_spec) Arguments names Names of columns returned from the query
types List of tf$DType objects (e.g. tf$int32, tf$double, tf$string) representing the types of the columns returned by the query.
driver_name String containing the database type. Currently, the only supported value is &#39;sqlite&#39;.</description></item><item><title>A dataset of all files matching a pattern</title><link>/reference/tfdatasets/file_list_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/file_list_dataset/</guid><description>A dataset of all files matching a pattern
file_list_dataset(file_pattern, shuffle = NULL, seed = NULL) Arguments file_pattern A string, representing the filename pattern that will be matched.
shuffle (Optional) If TRUE``, the file names will be shuffled randomly. Defaults to TRUE`
seed (Optional) An integer, representing the random seed that will be used to create the distribution.
Value A dataset of string correponding to file names</description></item><item><title>A dataset of fixed-length records from one or more binary files.</title><link>/reference/tfdatasets/fixed_length_record_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/fixed_length_record_dataset/</guid><description>A dataset of fixed-length records from one or more binary files.
fixed_length_record_dataset( filenames, record_bytes, header_bytes = NULL, footer_bytes = NULL, buffer_size = NULL ) Arguments filenames A string tensor containing one or more filenames.
record_bytes An integer representing the number of bytes in each record.
header_bytes (Optional) An integer scalar representing the number of bytes to skip at the start of a file.</description></item><item><title>A transformation that prefetches dataset values to the given &lt;code&gt;device&lt;/code&gt;</title><link>/reference/tfdatasets/dataset_prefetch_to_device/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_prefetch_to_device/</guid><description>A transformation that prefetches dataset values to the given device
dataset_prefetch_to_device(dataset, device, buffer_size = NULL) Arguments dataset A dataset
device A string. The name of a device to which elements will be prefetched (e.g. &#34;/gpu:0&#34;).
buffer_size (Optional.) The number of elements to buffer on device. Defaults to an automatically chosen value.
Value A dataset
Note Although the transformation creates a dataset, the transformation must be the final dataset in the input pipeline.</description></item><item><title>Add the tf_dataset class to a dataset</title><link>/reference/tfdatasets/as_tf_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/as_tf_dataset/</guid><description>Calling this function on a dataset adds the &#34;tf_dataset&#34; class to the dataset object. All datasets returned by functions in the tfdatasets package call this function on the dataset before returning it.
as_tf_dataset(dataset) Arguments dataset A dataset
Value A dataset with class &#34;tf_dataset&#34;</description></item><item><title>An operation that should be run to initialize this iterator.</title><link>/reference/tfdatasets/iterator_initializer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/iterator_initializer/</guid><description> An operation that should be run to initialize this iterator.
iterator_initializer(iterator) Arguments iterator An iterator
See also Other iterator functions: iterator_get_next(), iterator_make_initializer(), iterator_string_handle(), make-iterator</description></item><item><title>Caches the elements in this dataset.</title><link>/reference/tfdatasets/dataset_cache/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_cache/</guid><description>Caches the elements in this dataset.
dataset_cache(dataset, filename = NULL) Arguments dataset A dataset
filename String with the name of a directory on the filesystem to use for caching tensors in this Dataset. If a filename is not provided, the dataset will be cached in memory.
Value A dataset
See also Other dataset methods: dataset_batch(), dataset_collect(), dataset_concatenate(), dataset_decode_delim(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_prefetch(), dataset_repeat(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Collects a dataset</title><link>/reference/tfdatasets/dataset_collect/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_collect/</guid><description>Iterates throught the dataset collecting every element into a list. It&#39;s useful for looking at the full result of the dataset. Note: You may run out of memory if your dataset is too big.
dataset_collect(dataset, iter_max = Inf) Arguments dataset A dataset
iter_max Maximum number of iterations. Inf until the end of the dataset
See also Other dataset methods: dataset_batch(), dataset_cache(), dataset_concatenate(), dataset_decode_delim(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_prefetch(), dataset_repeat(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Combines consecutive elements of this dataset into batches.</title><link>/reference/tfdatasets/dataset_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_batch/</guid><description>Combines consecutive elements of this dataset into batches.
dataset_batch(dataset, batch_size, drop_remainder = FALSE) Arguments dataset A dataset
batch_size An integer, representing the number of consecutive elements of this dataset to combine in a single batch.
drop_remainder Ensure that batches have a fixed size by omitting any final smaller batch if it&#39;s present. Note that this is required for use with the Keras tensor inputs to fit/evaluate/etc.</description></item><item><title>Combines consecutive elements of this dataset into padded batches</title><link>/reference/tfdatasets/dataset_padded_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_padded_batch/</guid><description>This method combines multiple consecutive elements of this dataset, which might have different shapes, into a single element. The tensors in the resulting element have an additional outer dimension, and are padded to the respective shape in padded_shapes.
dataset_padded_batch( dataset, batch_size, padded_shapes, padding_values = NULL, drop_remainder = FALSE ) Arguments dataset A dataset
batch_size An integer, representing the number of consecutive elements of this dataset to combine in a single batch.</description></item><item><title>Combines input elements into a dataset of windows.</title><link>/reference/tfdatasets/dataset_window/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_window/</guid><description>Combines input elements into a dataset of windows.
dataset_window(dataset, size, shift = NULL, stride = 1, drop_remainder = FALSE) Arguments dataset A dataset
size representing the number of elements of the input dataset to combine into a window.
shift epresenting the forward shift of the sliding window in each iteration. Defaults to size.
stride representing the stride of the input elements in the sliding window.</description></item><item><title>Construct a tfestimators input function from a dataset</title><link>/reference/tfdatasets/input_fn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/input_fn/</guid><description>Construct a tfestimators input function from a dataset
# S3 method for tf_dataset input_fn(dataset, features, response = NULL) Arguments dataset A dataset
features The names of feature variables to be used.
response The name of the response variable.
Value An input_fn suitable for use with tfestimators train, evaluate, and predict methods
Details Creating an input_fn from a dataset requires that the dataset consist of a set of named output tensors (e.</description></item><item><title>Create a categorical column with identity</title><link>/reference/tfdatasets/step_categorical_column_with_identity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_categorical_column_with_identity/</guid><description>Use this when your inputs are integers in the range [0-num_buckets).
step_categorical_column_with_identity( spec, ..., num_buckets, default_value = NULL ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
num_buckets Range of inputs and outputs is [0, num_buckets).
default_value If NULL, this column&#39;s graph operations will fail for out-of-range inputs.</description></item><item><title>Create an operation that can be run to initialize this iterator</title><link>/reference/tfdatasets/iterator_make_initializer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/iterator_make_initializer/</guid><description> Create an operation that can be run to initialize this iterator
iterator_make_initializer(iterator, dataset, name = NULL) Arguments iterator An iterator
dataset A dataset
name (Optional) A name for the created operation.
Value A tf$Operation that can be run to initialize this iterator on the given dataset.
See also Other iterator functions: iterator_get_next(), iterator_initializer(), iterator_string_handle(), make-iterator</description></item><item><title>Creates a categorical column specification</title><link>/reference/tfdatasets/step_categorical_column_with_vocabulary_list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_categorical_column_with_vocabulary_list/</guid><description>Creates a categorical column specification
step_categorical_column_with_vocabulary_list( spec, ..., vocabulary_list = NULL, dtype = NULL, default_value = -1L, num_oov_buckets = 0L ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
vocabulary_list An ordered iterable defining the vocabulary. Each feature is mapped to the index of its value (if present) in vocabulary_list.</description></item><item><title>Creates a categorical column with hash buckets specification</title><link>/reference/tfdatasets/step_categorical_column_with_hash_bucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_categorical_column_with_hash_bucket/</guid><description>Represents sparse feature where ids are set by hashing.
step_categorical_column_with_hash_bucket( spec, ..., hash_bucket_size, dtype = tf$string ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
hash_bucket_size An int &amp;gt; 1. The number of buckets.
dtype The type of features. Only string and integer types are supported.</description></item><item><title>Creates a categorical column with vocabulary file</title><link>/reference/tfdatasets/step_categorical_column_with_vocabulary_file/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_categorical_column_with_vocabulary_file/</guid><description>Use this function when the vocabulary of a categorical variable is written to a file.
step_categorical_column_with_vocabulary_file( spec, ..., vocabulary_file, vocabulary_size = NULL, dtype = tf$string, default_value = NULL, num_oov_buckets = 0L ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
vocabulary_file The vocabulary file name.</description></item><item><title>Creates a dataset by concatenating given dataset with this dataset.</title><link>/reference/tfdatasets/dataset_concatenate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_concatenate/</guid><description>Creates a dataset by concatenating given dataset with this dataset.
dataset_concatenate(dataset, other) Arguments dataset A dataset
other Dataset to be concatenated
Value A dataset
Note Input dataset and dataset to be concatenated should have same nested structures and output types.
See also Other dataset methods: dataset_batch(), dataset_cache(), dataset_collect(), dataset_decode_delim(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_prefetch(), dataset_repeat(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Creates a dataset by zipping together the given datasets.</title><link>/reference/tfdatasets/zip_datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/zip_datasets/</guid><description>Merges datasets together into pairs or tuples that contain an element from each dataset.
zip_datasets(...) Arguments ... Datasets to zip (or a single argument with a list or list of lists of datasets).
Value A dataset</description></item><item><title>Creates a dataset of a step-separated range of values.</title><link>/reference/tfdatasets/range_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/range_dataset/</guid><description> Creates a dataset of a step-separated range of values.
range_dataset(from = 0, to = 0, by = 1) Arguments from Range start
to Range end (exclusive)
by Increment of the sequence</description></item><item><title>Creates a dataset that includes only 1 / num_shards of this dataset.</title><link>/reference/tfdatasets/dataset_shard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_shard/</guid><description>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.
dataset_shard(dataset, num_shards, index) Arguments dataset A dataset
num_shards A integer representing the number of shards operating in parallel.
index A integer, representing the worker index.
Value A dataset</description></item><item><title>Creates a Dataset that prefetches elements from this dataset.</title><link>/reference/tfdatasets/dataset_prefetch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_prefetch/</guid><description> Creates a Dataset that prefetches elements from this dataset.
dataset_prefetch(dataset, buffer_size) Arguments dataset A dataset
buffer_size An integer, representing the maximum number elements that will be buffered when prefetching.
Value A dataset
See also Other dataset methods: dataset_batch(), dataset_cache(), dataset_collect(), dataset_concatenate(), dataset_decode_delim(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_repeat(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Creates a dataset that skips count elements from this dataset</title><link>/reference/tfdatasets/dataset_skip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_skip/</guid><description>Creates a dataset that skips count elements from this dataset
dataset_skip(dataset, count) Arguments dataset A dataset
count An integer, representing the number of elements of this dataset that should be skipped to form the new dataset. If count is greater than the size of this dataset, the new dataset will contain no elements. If count is -1, skips the entire dataset.
Value A dataset</description></item><item><title>Creates a dataset whose elements are slices of the given tensors.</title><link>/reference/tfdatasets/tensor_slices_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/tensor_slices_dataset/</guid><description> Creates a dataset whose elements are slices of the given tensors.
tensor_slices_dataset(tensors) Arguments tensors A nested structure of tensors, each having the same size in the 0th dimension.
Value A dataset.
See also Other tensor datasets: sparse_tensor_slices_dataset(), tensors_dataset()</description></item><item><title>Creates a dataset with a single element, comprising the given tensors.</title><link>/reference/tfdatasets/tensors_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/tensors_dataset/</guid><description> Creates a dataset with a single element, comprising the given tensors.
tensors_dataset(tensors) Arguments tensors A nested structure of tensors.
Value A dataset.
See also Other tensor datasets: sparse_tensor_slices_dataset(), tensor_slices_dataset()</description></item><item><title>Creates a dataset with at most count elements from this dataset</title><link>/reference/tfdatasets/dataset_take/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_take/</guid><description>Creates a dataset with at most count elements from this dataset
dataset_take(dataset, count) Arguments dataset A dataset
count Integer representing the number of elements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this dataset, the new dataset will contain all elements of this dataset.
Value A dataset</description></item><item><title>Creates a feature specification.</title><link>/reference/tfdatasets/feature_spec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/feature_spec/</guid><description>Used to create initialize a feature columns specification.
feature_spec(dataset, x, y = NULL) Arguments dataset A TensorFlow dataset.
x Features to include can use tidyselect::select_helpers() or a formula.
y (Optional) The response variable. Can also be specified using a formula in the x argument.
Value a FeatureSpec object.
Details After creating the feature_spec object you can add steps using the step functions.</description></item><item><title>Creates a list of inputs from a dataset</title><link>/reference/tfdatasets/layer_input_from_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/layer_input_from_dataset/</guid><description> Create a list ok Keras input layers that can be used together with keras::layer_dense_features().
layer_input_from_dataset(dataset) Arguments dataset a TensorFlow dataset or a data.frame
Value a list of Keras input layers
Examples if (FALSE) { library(tfdatasets) data(hearts) hearts &amp;lt;- tensor_slices_dataset(hearts) %&amp;gt;% dataset_batch(32) # use the formula interface spec &amp;lt;- feature_spec(hearts, target ~ age + slope) %&amp;gt;% step_numeric_column(age, slope) %&amp;gt;% step_bucketized_column(age, boundaries = c(10, 20, 30)) spec &amp;lt;- fit(spec) dataset &amp;lt;- hearts %&amp;gt;% dataset_use_spec(spec) input &amp;lt;- layer_input_from_dataset(dataset) }</description></item><item><title>Creates a numeric column specification</title><link>/reference/tfdatasets/step_numeric_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_numeric_column/</guid><description>step_numeric_column creates a numeric column specification. It can also be used to normalize numeric columns.
step_numeric_column( spec, ..., shape = 1L, default_value = NULL, dtype = tf$float32, normalizer_fn = NULL ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
shape An iterable of integers specifies the shape of the Tensor.</description></item><item><title>Creates a step that can remove columns</title><link>/reference/tfdatasets/step_remove_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_remove_column/</guid><description>Removes features of the feature specification.
step_remove_column(spec, ...) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
Value a FeatureSpec object.
See also steps for a complete list of allowed steps.
Other Feature Spec Functions: dataset_use_spec(), feature_spec(), fit.FeatureSpec(), step_bucketized_column(), step_categorical_column_with_hash_bucket(), step_categorical_column_with_identity(), step_categorical_column_with_vocabulary_file(), step_categorical_column_with_vocabulary_list(), step_crossed_column(), step_embedding_column(), step_indicator_column(), step_numeric_column(), step_shared_embeddings_column(), steps</description></item><item><title>Creates an instance of a min max scaler</title><link>/reference/tfdatasets/scaler_min_max/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/scaler_min_max/</guid><description> This scaler will learn the min and max of the numeric variable and use this to create a normalizer_fn.
scaler_min_max() See also scaler to a complete list of normalizers
Other scaler: scaler_standard()</description></item><item><title>Creates an instance of a standard scaler</title><link>/reference/tfdatasets/scaler_standard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/scaler_standard/</guid><description> This scaler will learn the mean and the standard deviation and use this to create a normalizer_fn.
scaler_standard() See also scaler to a complete list of normalizers
Other scaler: scaler_min_max()</description></item><item><title>Creates an iterator for enumerating the elements of this dataset.</title><link>/reference/tfdatasets/make-iterator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/make-iterator/</guid><description>Creates an iterator for enumerating the elements of this dataset.
make_iterator_one_shot(dataset) make_iterator_initializable(dataset, shared_name = NULL) make_iterator_from_structure( output_types, output_shapes = NULL, shared_name = NULL ) make_iterator_from_string_handle( string_handle, output_types, output_shapes = NULL ) Arguments dataset A dataset
shared_name (Optional) If non-empty, the returned iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server).</description></item><item><title>Creates bucketized columns</title><link>/reference/tfdatasets/step_bucketized_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_bucketized_column/</guid><description>Use this step to create bucketized columns from numeric columns.
step_bucketized_column(spec, ..., boundaries) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
boundaries A sorted list or tuple of floats specifying the boundaries.
Value a FeatureSpec object.
See also steps for a complete list of allowed steps.</description></item><item><title>Creates crosses of categorical columns</title><link>/reference/tfdatasets/step_crossed_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_crossed_column/</guid><description>Use this step to create crosses between categorical columns.
step_crossed_column(spec, ..., hash_bucket_size, hash_key = NULL) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
hash_bucket_size An int &amp;gt; 1. The number of buckets.
hash_key (optional) Specify the hash_key that will be used by the FingerprintCat64 function to combine the crosses fingerprints on SparseCrossOp.</description></item><item><title>Creates embeddings columns</title><link>/reference/tfdatasets/step_embedding_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_embedding_column/</guid><description>Use this step to create ambeddings columns from categorical columns.
step_embedding_column( spec, ..., dimension = function(x) { as.integer(x^0.25) }, combiner = &#34;mean&#34;, initializer = NULL, ckpt_to_load_from = NULL, tensor_name_in_ckpt = NULL, max_norm = NULL, trainable = TRUE ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
dimension An integer specifying dimension of the embedding, must be &amp;gt; 0.</description></item><item><title>Creates Image embeddings columns</title><link>/reference/tfdatasets/step_image_embedding_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_image_embedding_column/</guid><description> Use this step to create image embeddings columns from image columns.
step_image_embedding_column(spec, ..., module_spec) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
module_spec A string handle or a _ModuleSpec identifying the module.</description></item><item><title>Creates Indicator Columns</title><link>/reference/tfdatasets/step_indicator_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_indicator_column/</guid><description>Use this step to create indicator columns from categorical columns.
step_indicator_column(spec, ...) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
Value a FeatureSpec object.
See also steps for a complete list of allowed steps.
Other Feature Spec Functions: dataset_use_spec(), feature_spec(), fit.FeatureSpec(), step_bucketized_column(), step_categorical_column_with_hash_bucket(), step_categorical_column_with_identity(), step_categorical_column_with_vocabulary_file(), step_categorical_column_with_vocabulary_list(), step_crossed_column(), step_embedding_column(), step_numeric_column(), step_remove_column(), step_shared_embeddings_column(), steps</description></item><item><title>Creates shared embeddings for categorical columns</title><link>/reference/tfdatasets/step_shared_embeddings_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_shared_embeddings_column/</guid><description>This is similar to step_embedding_column, except that it produces a list of embedding columns that share the same embedding weights.
step_shared_embeddings_column( spec, ..., dimension, combiner = &#34;mean&#34;, initializer = NULL, shared_embedding_collection_name = NULL, ckpt_to_load_from = NULL, tensor_name_in_ckpt = NULL, max_norm = NULL, trainable = TRUE ) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step.</description></item><item><title>Creates text embeddings columns</title><link>/reference/tfdatasets/step_text_embedding_column/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/step_text_embedding_column/</guid><description>Use this step to create text embeddings columns from string columns.
step_text_embedding_column(spec, ..., module_spec, trainable = FALSE) Arguments spec A feature specification created with feature_spec().
... Comma separated list of variable names to apply the step. selectors can also be used.
module_spec A string handle or a _ModuleSpec identifying the module.
trainable Whether or not the Module is trainable.</description></item><item><title>Dense Features</title><link>/reference/tfdatasets/dense_features/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dense_features/</guid><description>Retrives the Dense Features from a spec.
dense_features(spec) Arguments spec A feature specification created with feature_spec().
Value A list of feature columns.</description></item><item><title>Execute code that traverses a dataset</title><link>/reference/tfdatasets/with_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/with_dataset/</guid><description> Execute code that traverses a dataset
with_dataset(expr) Arguments expr Expression to execute
Details When a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by wrapping your iteration code in a call to with_dataset() (see the example below for an illustration).
Examples if (FALSE) { library(tfdatasets) dataset &amp;lt;- text_line_dataset(&#34;mtcars.csv&#34;, record_spec = mtcars_spec) %&amp;gt;% dataset_prepare(x = c(mpg, disp), y = cyl) %&amp;gt;% dataset_batch(128) %&amp;gt;% dataset_repeat(10) iter &amp;lt;- make_iterator_one_shot(dataset) next_batch &amp;lt;- iterator_get_next(iter) with_dataset({ while(TRUE) { batch &amp;lt;- sess$run(next_batch) # use batch$x and batch$y tensors } }) }</description></item><item><title>Execute code that traverses a dataset until an out of range condition occurs</title><link>/reference/tfdatasets/until_out_of_range/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/until_out_of_range/</guid><description>Execute code that traverses a dataset until an out of range condition occurs
until_out_of_range(expr) out_of_range_handler(e) Arguments expr Expression to execute (will be executed multiple times until the condition occurs)
e Error object
Details When a dataset iterator reaches the end, an out of range runtime error will occur. This function will catch and ignore the error when it occurs.
Examples if (FALSE) { library(tfdatasets) dataset &amp;lt;- text_line_dataset(&#34;</description></item><item><title>Filter a dataset by a predicate</title><link>/reference/tfdatasets/dataset_filter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_filter/</guid><description>Filter a dataset by a predicate
dataset_filter(dataset, predicate) Arguments dataset A dataset
predicate A function mapping a nested structure of tensors (having shapes and types defined by output_shapes() and output_types() to a scalar tf$bool tensor.
Value A dataset composed of records that matched the predicate.
Details Note that the functions used inside the predicate must be tensor operations (e.g. tf$not_equal, tf$less, etc.</description></item><item><title>Find all nominal variables.</title><link>/reference/tfdatasets/all_nominal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/all_nominal/</guid><description> Currently we only consider &#34;string&#34; type as nominal.
all_nominal() See also Other Selectors: all_numeric(), has_type()</description></item><item><title>Fits a feature specification.</title><link>/reference/tfdatasets/fit.featurespec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/fit.featurespec/</guid><description>This function will fit the specification. Depending on the steps added to the specification it will compute for example, the levels of categorical features, normalization constants, etc.
# S3 method for FeatureSpec fit(object, dataset = NULL, ...) Arguments object A feature specification created with feature_spec().
dataset (Optional) A TensorFlow dataset. If NULL it will use the dataset provided when initilializing the feature_spec.
.</description></item><item><title>Fused implementation of dataset_map() and dataset_batch()</title><link>/reference/tfdatasets/dataset_map_and_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_map_and_batch/</guid><description>Maps `map_func`` across batch_size consecutive elements of this dataset and then combines them into a batch. Functionally, it is equivalent to map followed by batch. However, by fusing the two transformations together, the implementation can be more efficient.
dataset_map_and_batch( dataset, map_func, batch_size, num_parallel_batches = NULL, drop_remainder = FALSE, num_parallel_calls = NULL ) Arguments dataset A dataset
map_func A function mapping a nested structure of tensors (having shapes and types defined by output_shapes() and output_types() to another nested structure of tensors.</description></item><item><title>Get next element from iterator</title><link>/reference/tfdatasets/iterator_get_next/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/iterator_get_next/</guid><description> Returns a nested list of tensors that when evaluated will yield the next element(s) in the dataset.
iterator_get_next(iterator, name = NULL) Arguments iterator An iterator
name (Optional) A name for the created operation.
Value A nested list of tensors
See also Other iterator functions: iterator_initializer(), iterator_make_initializer(), iterator_string_handle(), make-iterator</description></item><item><title>Heart Disease Data Set</title><link>/reference/tfdatasets/hearts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/hearts/</guid><description>Heart disease (angiographic disease status) dataset.
hearts Format A data frame with 303 rows and 14 variables:
ageage in years
sexsex (1 = male; 0 = female)
cpchest pain type: Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic
trestbpsresting blood pressure (in mm Hg on admission to the hospital)
cholserum cholestoral in mg/dl
fbs(fasting blood sugar &amp;gt; 120 mg/dl) (1 = true; 0 = false)</description></item><item><title>Identify the type of the variable.</title><link>/reference/tfdatasets/has_type/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/has_type/</guid><description> Can only be used inside the steps specifications to find variables by type.
has_type(match = &#34;float32&#34;) Arguments match A list of types to match.
See also Other Selectors: all_nominal(), all_numeric()</description></item><item><title>List of pre-made scalers</title><link>/reference/tfdatasets/scaler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/scaler/</guid><description> scaler_standard: mean and standard deviation normalizer.
scaler_min_max: min max normalizer
See also step_numeric_column</description></item><item><title>Map a function across a dataset.</title><link>/reference/tfdatasets/dataset_map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_map/</guid><description>Map a function across a dataset.
dataset_map(dataset, map_func, num_parallel_calls = NULL) Arguments dataset A dataset
map_func A function mapping a nested structure of tensors (having shapes and types defined by output_shapes() and output_types() to another nested structure of tensors. It also supports purrr style lambda functions powered by rlang::as_function().
num_parallel_calls (Optional) An integer, representing the number of elements to process in parallel If not specified, elements will be processed sequentially.</description></item><item><title>Maps map_func across this dataset and flattens the result.</title><link>/reference/tfdatasets/dataset_flat_map/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_flat_map/</guid><description>Maps map_func across this dataset and flattens the result.
dataset_flat_map(dataset, map_func) Arguments dataset A dataset
map_func A function mapping a nested structure of tensors (having shapes and types defined by output_shapes() and output_types() to a dataset.
Value A dataset</description></item><item><title>Maps map_func across this dataset, and interleaves the results</title><link>/reference/tfdatasets/dataset_interleave/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_interleave/</guid><description>Maps map_func across this dataset, and interleaves the results
dataset_interleave(dataset, map_func, cycle_length, block_length = 1) Arguments dataset A dataset
map_func A function mapping a nested structure of tensors (having shapes and types defined by output_shapes() and output_types() to a dataset.
cycle_length The number of elements from this dataset that will be processed concurrently.
block_length The number of consecutive elements to produce from each input element before cycling to another input element.</description></item><item><title>Objects exported from other packages</title><link>/reference/tfdatasets/reexports/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/reexports/</guid><description> These objects are imported from other packages. Follow the links below to see their documentation.
genericsfit
tensorflowinstall_tensorflow, shape, tf
tidyselectcontains, ends_with, everything, matches, num_range, one_of, starts_with</description></item><item><title>Output types and shapes</title><link>/reference/tfdatasets/output_types/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/output_types/</guid><description>Output types and shapes
output_types(object) output_shapes(object) Arguments object A dataset or iterator
Value output_types() returns the type of each component of an element of this object; output_shapes() returns the shape of each component of an element of this object</description></item><item><title>Pipe operator</title><link>/reference/tfdatasets/pipe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/pipe/</guid><description> See %&amp;gt;% for more details.
lhs %&amp;gt;% rhs</description></item><item><title>Prepare a dataset for analysis</title><link>/reference/tfdatasets/dataset_prepare/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_prepare/</guid><description>Transform a dataset with named columns into a list with features (x) and response (y) elements.
dataset_prepare( dataset, x, y = NULL, named = TRUE, named_features = FALSE, parallel_records = NULL, batch_size = NULL, num_parallel_batches = NULL, drop_remainder = FALSE ) Arguments dataset A dataset
x Features to include. When named_features is FALSE all features will be stacked into a single tensor so must have an identical data type.</description></item><item><title>Randomly shuffles the elements of this dataset.</title><link>/reference/tfdatasets/dataset_shuffle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_shuffle/</guid><description>Randomly shuffles the elements of this dataset.
dataset_shuffle( dataset, buffer_size, seed = NULL, reshuffle_each_iteration = NULL ) Arguments dataset A dataset
buffer_size An integer, representing the number of elements from this dataset from which the new dataset will sample.
seed (Optional) An integer, representing the random seed that will be used to create the distribution.
reshuffle_each_iteration (Optional) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over.</description></item><item><title>Read a dataset from a set of files</title><link>/reference/tfdatasets/read_files/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/read_files/</guid><description>Read files into a dataset, optionally processing them in parallel.
read_files( files, reader, ..., parallel_files = 1, parallel_interleave = 1, num_shards = NULL, shard_index = NULL ) Arguments files List of filenames or glob pattern for files (e.g. &#34;*.csv&#34;)
reader Function that maps a file into a dataset (e.g. text_line_dataset() or tfrecord_dataset()).
... Additional arguments to pass to reader function</description></item><item><title>Reads CSV files into a batched dataset</title><link>/reference/tfdatasets/make_csv_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/make_csv_dataset/</guid><description>Reads CSV files into a dataset, where each element is a (features, labels) list that corresponds to a batch of CSV rows. The features dictionary maps feature column names to tensors containing the corresponding feature data, and labels is a tensor containing the batch&#39;s label data.
make_csv_dataset( file_pattern, batch_size, column_names = NULL, column_defaults = NULL, label_name = NULL, select_columns = NULL, field_delim = &#34;,&#34;, use_quote_delim = TRUE, na_value = &#34;</description></item><item><title>Repeats a dataset count times.</title><link>/reference/tfdatasets/dataset_repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_repeat/</guid><description>Repeats a dataset count times.
dataset_repeat(dataset, count = NULL) Arguments dataset A dataset
count (Optional.) An integer value representing the number of times the elements of this dataset should be repeated. The default behavior (if count is NULL or -1) is for the elements to be repeated indefinitely.
Value A dataset
See also Other dataset methods: dataset_batch(), dataset_cache(), dataset_collect(), dataset_concatenate(), dataset_decode_delim(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_prefetch(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Samples elements at random from the datasets in &lt;code&gt;datasets&lt;/code&gt;.</title><link>/reference/tfdatasets/sample_from_datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/sample_from_datasets/</guid><description>Samples elements at random from the datasets in datasets.
sample_from_datasets(datasets, weights = NULL, seed = NULL) Arguments datasets A list ofobjects with compatible structure.
weights (Optional.) A list of length(datasets) floating-point values where weights[[i]] represents the probability with which an element should be sampled from datasets[[i]], or a dataset object where each element is such a list. Defaults to a uniform distribution across datasets.</description></item><item><title>Selectors</title><link>/reference/tfdatasets/selectors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/selectors/</guid><description> List of selectors that can be used to specify variables inside steps.
cur_info_env Format An object of class environment of length 0.
Selectors has_type()
all_numeric()
all_nominal()
starts_with()
ends_with()
one_of()
matches()
contains()
everything()</description></item><item><title>Shuffles and repeats a dataset returning a new permutation for each epoch.</title><link>/reference/tfdatasets/dataset_shuffle_and_repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_shuffle_and_repeat/</guid><description>Shuffles and repeats a dataset returning a new permutation for each epoch.
dataset_shuffle_and_repeat(dataset, buffer_size, count = NULL, seed = NULL) Arguments dataset A dataset
buffer_size An integer, representing the number of elements from this dataset from which the new dataset will sample.
count (Optional.) An integer value representing the number of times the elements of this dataset should be repeated. The default behavior (if count is NULL or -1) is for the elements to be repeated indefinitely.</description></item><item><title>Specification for reading a record from a text file with delimited values</title><link>/reference/tfdatasets/delim_record_spec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/delim_record_spec/</guid><description>Specification for reading a record from a text file with delimited values
delim_record_spec( example_file, delim = &#34;,&#34;, skip = 0, names = NULL, types = NULL, defaults = NULL ) csv_record_spec( example_file, skip = 0, names = NULL, types = NULL, defaults = NULL ) tsv_record_spec( example_file, skip = 0, names = NULL, types = NULL, defaults = NULL ) Arguments example_file File that provides an example of the records to be read.</description></item><item><title>Speciy all numeric variables.</title><link>/reference/tfdatasets/all_numeric/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/all_numeric/</guid><description> Find all the variables with the following types: &#34;float16&#34;, &#34;float32&#34;, &#34;float64&#34;, &#34;int16&#34;, &#34;int32&#34;, &#34;int64&#34;, &#34;half&#34;, &#34;double&#34;.
all_numeric() See also Other Selectors: all_nominal(), has_type()</description></item><item><title>Splits each rank-N &lt;code&gt;tf$SparseTensor&lt;/code&gt; in this dataset row-wise.</title><link>/reference/tfdatasets/sparse_tensor_slices_dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/sparse_tensor_slices_dataset/</guid><description> Splits each rank-N tf$SparseTensor in this dataset row-wise.
sparse_tensor_slices_dataset(sparse_tensor) Arguments sparse_tensor A tf$SparseTensor.
Value A dataset of rank-(N-1) sparse tensors.
See also Other tensor datasets: tensor_slices_dataset(), tensors_dataset()</description></item><item><title>Steps for feature columns specification.</title><link>/reference/tfdatasets/steps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/steps/</guid><description>List of steps that can be used to specify columns in the feature_spec interface.
Steps step_numeric_column() to define numeric columns.
step_categorical_column_with_vocabulary_list() to define categorical columns.
step_categorical_column_with_hash_bucket() to define categorical columns where ids are set by hashing.
step_categorical_column_with_identity() to define categorical columns represented by integers in the range [0-num_buckets).
step_categorical_column_with_vocabulary_file() to define categorical columns when their vocabulary is available in a file.
step_indicator_column() to create indicator columns from categorical columns.</description></item><item><title>String-valued tensor that represents this iterator</title><link>/reference/tfdatasets/iterator_string_handle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/iterator_string_handle/</guid><description> String-valued tensor that represents this iterator
iterator_string_handle(iterator, name = NULL) Arguments iterator An iterator
name (Optional) A name for the created operation.
Value Scalar tensor of type string
See also Other iterator functions: iterator_get_next(), iterator_initializer(), iterator_make_initializer(), make-iterator</description></item><item><title>Tensor(s) for retreiving the next batch from a dataset</title><link>/reference/tfdatasets/next_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/next_batch/</guid><description>Tensor(s) for retreiving the next batch from a dataset
next_batch(dataset) Arguments dataset A dataset
Value Tensor(s) that can be evaluated to yield the next batch of training data.
Details To access the underlying data within the dataset you iteratively evaluate the tensor(s) to read batches of data.
Note that in many cases you won&#39;t need to explicitly evaluate the tensors. Rather, you will pass the tensors to another function that will perform the evaluation (e.</description></item><item><title>Transform a dataset with delimted text lines into a dataset with named columns</title><link>/reference/tfdatasets/dataset_decode_delim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_decode_delim/</guid><description>Transform a dataset with delimted text lines into a dataset with named columns
dataset_decode_delim(dataset, record_spec, parallel_records = NULL) Arguments dataset Dataset containing delimited text lines (e.g. a CSV)
record_spec Specification of column names and types (see delim_record_spec()).
parallel_records (Optional) An integer, representing the number of records to decode in parallel. If not specified, records will be processed sequentially.
See also Other dataset methods: dataset_batch(), dataset_cache(), dataset_collect(), dataset_concatenate(), dataset_filter(), dataset_interleave(), dataset_map_and_batch(), dataset_map(), dataset_padded_batch(), dataset_prefetch_to_device(), dataset_prefetch(), dataset_repeat(), dataset_shuffle_and_repeat(), dataset_shuffle(), dataset_skip(), dataset_take(), dataset_window()</description></item><item><title>Transform the dataset using the provided spec.</title><link>/reference/tfdatasets/dataset_use_spec/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/tfdatasets/dataset_use_spec/</guid><description>Prepares the dataset to be used directly in a model.The transformed dataset is prepared to return tuples (x,y) that can be used directly in Keras.
dataset_use_spec(dataset, spec) Arguments dataset A TensorFlow dataset.
spec A feature specification created with feature_spec().
Value A TensorFlow dataset.
See also feature_spec() to initialize the feature specification.
fit.FeatureSpec() to create a tensorflow dataset prepared to modeling.</description></item></channel></rss>