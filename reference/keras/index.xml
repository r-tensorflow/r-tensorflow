<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TensorFlow for R</title><link>/reference/keras/</link><description>Recent content on TensorFlow for R</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/reference/keras/index.xml" rel="self" type="application/rss+xml"/><item><title>1D convolution layer (e.g. temporal convolution).</title><link>/reference/keras/layer_conv_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_1d/</guid><description>This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is TRUE, a bias vector is created and added to the outputs. Finally, if activation is not NULL, it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (list of integers or NULL , e.</description></item><item><title>1D convolution.</title><link>/reference/keras/k_conv1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_conv1d/</guid><description>1D convolution.
k_conv1d( x, kernel, strides = 1, padding = &#34;valid&#34;, data_format = NULL, dilation_rate = 1 ) Arguments x Tensor or variable.
kernel kernel tensor.
strides stride integer.
padding string, &#34;same&#34;, &#34;causal&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
dilation_rate integer dilate rate.
Value A tensor, result of 1D convolution.</description></item><item><title>2D convolution layer (e.g. spatial convolution over images).</title><link>/reference/keras/layer_conv_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_2d/</guid><description>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is TRUE, a bias vector is created and added to the outputs. Finally, if activation is not NULL, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (list of integers, does not include the sample axis), e.</description></item><item><title>2D convolution with separable filters.</title><link>/reference/keras/k_separable_conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_separable_conv2d/</guid><description>2D convolution with separable filters.
k_separable_conv2d( x, depthwise_kernel, pointwise_kernel, strides = c(1, 1), padding = &#34;valid&#34;, data_format = NULL, dilation_rate = c(1, 1) ) Arguments x input tensor
depthwise_kernel convolution kernel for the depthwise convolution.
pointwise_kernel kernel for the 1x1 convolution.
strides strides list (length 2).
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;</description></item><item><title>2D convolution.</title><link>/reference/keras/k_conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_conv2d/</guid><description>2D convolution.
k_conv2d( x, kernel, strides = c(1, 1), padding = &#34;valid&#34;, data_format = NULL, dilation_rate = c(1, 1) ) Arguments x Tensor or variable.
kernel kernel tensor.
strides strides
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;. Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs.
dilation_rate vector of 2 integers.</description></item><item><title>2D deconvolution (i.e. transposed convolution).</title><link>/reference/keras/k_conv2d_transpose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_conv2d_transpose/</guid><description>2D deconvolution (i.e. transposed convolution).
k_conv2d_transpose( x, kernel, output_shape, strides = c(1, 1), padding = &#34;valid&#34;, data_format = NULL ) Arguments x Tensor or variable.
kernel kernel tensor.
output_shape 1D int tensor for the output shape.
strides strides list.
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;. Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs.</description></item><item><title>2D Pooling.</title><link>/reference/keras/k_pool2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_pool2d/</guid><description>2D Pooling.
k_pool2d( x, pool_size, strides = c(1, 1), padding = &#34;valid&#34;, data_format = NULL, pool_mode = &#34;max&#34; ) Arguments x Tensor or variable.
pool_size list of 2 integers.
strides list of 2 integers.
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
pool_mode string, &#34;max&#34; or &#34;avg&#34;.
Value A tensor, result of 2D pooling.</description></item><item><title>3D array representation of images</title><link>/reference/keras/image_to_array/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/image_to_array/</guid><description>3D array that represents an image with dimensions (height,width,channels) or (channels,height,width) depending on the data_format.
image_to_array(img, data_format = c(&#34;channels_last&#34;, &#34;channels_first&#34;)) image_array_resize( img, height, width, data_format = c(&#34;channels_last&#34;, &#34;channels_first&#34;) ) image_array_save( img, path, data_format = NULL, file_format = NULL, scale = TRUE ) Arguments img Image
data_format Image data format (&#34;channels_last&#34; or &#34;channels_first&#34;)
height Height to resize to
width Width to resize to</description></item><item><title>3D convolution layer (e.g. spatial convolution over volumes).</title><link>/reference/keras/layer_conv_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_3d/</guid><description>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is TRUE, a bias vector is created and added to the outputs. Finally, if activation is not NULL, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (list of integers, does not include the sample axis), e.</description></item><item><title>3D convolution.</title><link>/reference/keras/k_conv3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_conv3d/</guid><description>3D convolution.
k_conv3d( x, kernel, strides = c(1, 1, 1), padding = &#34;valid&#34;, data_format = NULL, dilation_rate = c(1, 1, 1) ) Arguments x Tensor or variable.
kernel kernel tensor.
strides strides
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;. Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs.
dilation_rate list of 3 integers.</description></item><item><title>3D deconvolution (i.e. transposed convolution).</title><link>/reference/keras/k_conv3d_transpose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_conv3d_transpose/</guid><description>3D deconvolution (i.e. transposed convolution).
k_conv3d_transpose( x, kernel, output_shape, strides = c(1, 1, 1), padding = &#34;valid&#34;, data_format = NULL ) Arguments x input tensor.
kernel kernel tensor.
output_shape 1D int tensor for the output shape.
strides strides
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;. Whether to use Theano or TensorFlow/CNTK data format for inputs/kernels/outputs.</description></item><item><title>3D Pooling.</title><link>/reference/keras/k_pool3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_pool3d/</guid><description>3D Pooling.
k_pool3d( x, pool_size, strides = c(1, 1, 1), padding = &#34;valid&#34;, data_format = NULL, pool_mode = &#34;max&#34; ) Arguments x Tensor or variable.
pool_size list of 3 integers.
strides list of 3 integers.
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
pool_mode string, &#34;max&#34; or &#34;avg&#34;.
Value A tensor, result of 3D pooling.</description></item><item><title>Activation functions</title><link>/reference/keras/activation_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/activation_relu/</guid><description>Activations functions can either be used through layer_activation(), or through the activation argument supported by all forward layers.
activation_relu(x, alpha = 0, max_value = NULL, threshold = 0) activation_elu(x, alpha = 1) activation_selu(x) activation_hard_sigmoid(x) activation_linear(x) activation_sigmoid(x) activation_softmax(x, axis = -1) activation_softplus(x) activation_softsign(x) activation_tanh(x) activation_exponential(x) Arguments x Tensor
alpha Alpha value
max_value Max value
threshold Threshold value for thresholded activation.</description></item><item><title>Active Keras backend</title><link>/reference/keras/k_backend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_backend/</guid><description>Active Keras backend
k_backend() Value The name of the backend Keras is currently using.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Adadelta optimizer.</title><link>/reference/keras/optimizer_adadelta/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_adadelta/</guid><description>Adadelta optimizer as described in ADADELTA: An Adaptive Learning Rate Method.
optimizer_adadelta( lr = 1, rho = 0.95, epsilon = NULL, decay = 0, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
rho float &amp;gt;= 0. Decay factor.
epsilon float &amp;gt;= 0. Fuzz factor. If NULL, defaults to k_epsilon().
decay float &amp;gt;= 0.</description></item><item><title>Adagrad optimizer.</title><link>/reference/keras/optimizer_adagrad/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_adagrad/</guid><description>Adagrad optimizer as described in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.
optimizer_adagrad( lr = 0.01, epsilon = NULL, decay = 0, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
epsilon float &amp;gt;= 0. Fuzz factor. If NULL, defaults to k_epsilon().
decay float &amp;gt;= 0. Learning rate decay over each update.
clipnorm Gradients will be clipped when their L2 norm exceeds this value.</description></item><item><title>Adam optimizer</title><link>/reference/keras/optimizer_adam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_adam/</guid><description>Adam optimizer as described in Adam - A Method for Stochastic Optimization.
optimizer_adam( lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = NULL, decay = 0, amsgrad = FALSE, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
beta_1 The exponential decay rate for the 1st moment estimates. float, 0 &amp;lt; beta &amp;lt; 1. Generally close to 1.</description></item><item><title>Adamax optimizer</title><link>/reference/keras/optimizer_adamax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_adamax/</guid><description>Adamax optimizer from Section 7 of the Adam paper. It is a variant of Adam based on the infinity norm.
optimizer_adamax( lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = NULL, decay = 0, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
beta_1 The exponential decay rate for the 1st moment estimates. float, 0 &amp;lt; beta &amp;lt; 1.</description></item><item><title>Add a densely-connected NN layer to an output</title><link>/reference/keras/layer_dense/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_dense/</guid><description>Implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is TRUE). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.</description></item><item><title>Adds a 1-sized dimension at index &lt;code&gt;axis&lt;/code&gt;.</title><link>/reference/keras/k_expand_dims/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_expand_dims/</guid><description>Adds a 1-sized dimension at index axis.
k_expand_dims(x, axis = -1) Arguments x A tensor or variable.
axis Position where to add a new axis (axis indexes are 1-based). Pass -1 (the default) to select the last axis.
Value A tensor with expanded dimensions.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Adds a bias vector to a tensor.</title><link>/reference/keras/k_bias_add/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_bias_add/</guid><description>Adds a bias vector to a tensor.
k_bias_add(x, bias, data_format = NULL) Arguments x Tensor or variable.
bias Bias tensor to add.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
Value Output tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Applies Alpha Dropout to the input.</title><link>/reference/keras/layer_alpha_dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_alpha_dropout/</guid><description>Alpha Dropout is a dropout that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout.
layer_alpha_dropout( object, rate, noise_shape = NULL, seed = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
rate float, drop probability (as with layer_dropout()).</description></item><item><title>Applies batch normalization on x given mean, var, beta and gamma.</title><link>/reference/keras/k_batch_normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_batch_normalization/</guid><description>i.e. returns output &amp;lt;- (x - mean) / (sqrt(var) + epsilon) * gamma + beta
k_batch_normalization(x, mean, var, beta, gamma, axis = -1, epsilon = 0.001) Arguments x Input tensor or variable.
mean Mean of batch.
var Variance of batch.
beta Tensor with which to center the input.
gamma Tensor by which to scale the input.</description></item><item><title>Applies Dropout to the input.</title><link>/reference/keras/layer_dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_dropout/</guid><description>Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.
layer_dropout( object, rate, noise_shape = NULL, seed = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
rate float between 0 and 1. Fraction of the input units to drop.</description></item><item><title>Apply 1D conv with un-shared weights.</title><link>/reference/keras/k_local_conv1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_local_conv1d/</guid><description>Apply 1D conv with un-shared weights.
k_local_conv1d(inputs, kernel, kernel_size, strides, data_format = NULL) Arguments inputs 3D tensor with shape: (batch_size, steps, input_dim)
kernel the unshared weight for convolution, with shape (output_length, feature_dim, filters)
kernel_size a list of a single integer, specifying the length of the 1D convolution window
strides a list of a single integer, specifying the stride length of the convolution</description></item><item><title>Apply 2D conv with un-shared weights.</title><link>/reference/keras/k_local_conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_local_conv2d/</guid><description>Apply 2D conv with un-shared weights.
k_local_conv2d( inputs, kernel, kernel_size, strides, output_shape, data_format = NULL ) Arguments inputs 4D tensor with shape: (batch_size, filters, new_rows, new_cols) if data_format=&#39;channels_first&#39; or 4D tensor with shape: (batch_size, new_rows, new_cols, filters) if data_format=&#39;channels_last&#39;.
kernel the unshared weight for convolution, with shape (output_items, feature_dim, filters)
kernel_size a list of 2 integers, specifying the width and height of the 2D convolution window.</description></item><item><title>Apply a layer to every temporal slice of an input.</title><link>/reference/keras/time_distributed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/time_distributed/</guid><description>The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.
time_distributed( object, layer, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
layer A layer instance.
input_shape Dimensionality of the input (integer) not including the samples axis.</description></item><item><title>Apply additive zero-centered Gaussian noise.</title><link>/reference/keras/layer_gaussian_noise/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_gaussian_noise/</guid><description>This is useful to mitigate overfitting (you could see it as a form of random data augmentation). Gaussian Noise (GS) is a natural choice as corruption process for real valued inputs. As it is a regularization layer, it is only active at training time.
layer_gaussian_noise( object, stddev, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Apply an activation function to an output.</title><link>/reference/keras/layer_activation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation/</guid><description>Apply an activation function to an output.
layer_activation( object, activation, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
activation Name of activation function to use. If you don&#39;t specify anything, no activation is applied (ie. &#34;linear&#34; activation: a(x) = x).
input_shape Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.</description></item><item><title>Apply multiplicative 1-centered Gaussian noise.</title><link>/reference/keras/layer_gaussian_dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_gaussian_dropout/</guid><description>As it is a regularization layer, it is only active at training time.
layer_gaussian_dropout( object, rate, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
rate float, drop probability (as with Dropout). The multiplicative noise will have standard deviation sqrt(rate / (1 - rate)).</description></item><item><title>Assign values to names</title><link>/reference/keras/multi-assign/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/multi-assign/</guid><description> See %&amp;lt;-% for more details.
x %&amp;lt;-% value</description></item><item><title>Average pooling for temporal data.</title><link>/reference/keras/layer_average_pooling_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_average_pooling_1d/</guid><description>Average pooling for temporal data.
layer_average_pooling_1d( object, pool_size = 2L, strides = NULL, padding = &#34;valid&#34;, data_format = &#34;channels_last&#34;, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size Integer, size of the average pooling windows.
strides Integer, or NULL. Factor by which to downscale. E.g. 2 will halve the input.</description></item><item><title>Average pooling operation for 3D data (spatial or spatio-temporal).</title><link>/reference/keras/layer_average_pooling_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_average_pooling_3d/</guid><description>Average pooling operation for 3D data (spatial or spatio-temporal).
layer_average_pooling_3d( object, pool_size = c(2L, 2L, 2L), strides = NULL, padding = &#34;valid&#34;, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size list of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension.</description></item><item><title>Average pooling operation for spatial data.</title><link>/reference/keras/layer_average_pooling_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_average_pooling_2d/</guid><description>Average pooling operation for spatial data.
layer_average_pooling_2d( object, pool_size = c(2L, 2L), strides = NULL, padding = &#34;valid&#34;, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size integer or list of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension.</description></item><item><title>Base R6 class for Keras callbacks</title><link>/reference/keras/kerascallback/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/kerascallback/</guid><description>Base R6 class for Keras callbacks
Format An R6Class generator object
Value KerasCallback.
Details The logs named list that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch.
Currently, the fit.keras.engine.training.Model() method for sequential models will include the following quantities in the logs that it passes to its callbacks:
on_epoch_end: logs include acc and loss, and optionally include val_loss (if validation is enabled in fit), and val_acc (if validation and accuracy monitoring are enabled).</description></item><item><title>Base R6 class for Keras constraints</title><link>/reference/keras/kerasconstraint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/kerasconstraint/</guid><description>Base R6 class for Keras constraints
Format An R6Class generator object
Details You can implement a custom constraint either by creating an R function that accepts a weights (w) parameter, or by creating an R6 class that derives from KerasConstraint and implements a call method.
Note Models which use custom constraints cannot be serialized using save_model_hdf5(). Rather, the weights of the model should be saved and restored using save_model_weights_hdf5().</description></item><item><title>Base R6 class for Keras layers</title><link>/reference/keras/keraslayer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keraslayer/</guid><description> Base R6 class for Keras layers
Format An R6Class generator object #&#39;
Value KerasLayer.
Methods
build(input_shape)Creates the layer weights (must be implemented by all layers that have weights)
call(inputs,mask)Call the layer on an input tensor.
compute_output_shape(input_shape)Compute the output shape for the layer.
add_loss(losses, inputs)Add losses to the layer.
add_weight(name,shape,dtype,initializer,regularizer,trainable,constraint)Adds a weight variable to the layer.</description></item><item><title>Base R6 class for Keras wrappers</title><link>/reference/keras/keraswrapper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keraswrapper/</guid><description>Base R6 class for Keras wrappers
Format An R6Class generator object
Value KerasWrapper.
Methods
build(input_shape)Builds the wrapped layer. Subclasses can extend this to perform custom operations on that layer.
call(inputs,mask)Calls the wrapped layer on an input tensor.
compute_output_shape(input_shape)Computes the output shape for the wrapped layer.
add_loss(losses, inputs)Subclasses can use this to add losses to the wrapped layer.
add_weight(name,shape,dtype,initializer,regularizer,trainable,constraint)Subclasses can use this to add weights to the wrapped layer.</description></item><item><title>Batch normalization layer (Ioffe and Szegedy, 2014).</title><link>/reference/keras/layer_batch_normalization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_batch_normalization/</guid><description>Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.
layer_batch_normalization( object, axis = -1L, momentum = 0.99, epsilon = 0.001, center = TRUE, scale = TRUE, beta_initializer = &#34;zeros&#34;, gamma_initializer = &#34;ones&#34;, moving_mean_initializer = &#34;zeros&#34;, moving_variance_initializer = &#34;ones&#34;, beta_regularizer = NULL, gamma_regularizer = NULL, beta_constraint = NULL, gamma_constraint = NULL, renorm = FALSE, renorm_clipping = NULL, renorm_momentum = 0.</description></item><item><title>Batchwise dot product.</title><link>/reference/keras/k_batch_dot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_batch_dot/</guid><description>batch_dot is used to compute dot product of x and y when x and y are data in batch, i.e. in a shape of (batch_size). batch_dot results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use expand_dims to make sure that ndim is at least 2.
k_batch_dot(x, y, axes) Arguments x Keras tensor or variable with 2 more more axes.</description></item><item><title>Bidirectional wrapper for RNNs.</title><link>/reference/keras/bidirectional/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/bidirectional/</guid><description>Bidirectional wrapper for RNNs.
bidirectional( object, layer, merge_mode = &#34;concat&#34;, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
layer Recurrent instance.
merge_mode Mode by which outputs of the forward and backward RNNs will be combined. One of &#39;sum&#39;, &#39;mul&#39;, &#39;concat&#39;, &#39;ave&#39;, NULL.</description></item><item><title>Binary crossentropy between an output tensor and a target tensor.</title><link>/reference/keras/k_binary_crossentropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_binary_crossentropy/</guid><description>Binary crossentropy between an output tensor and a target tensor.
k_binary_crossentropy(target, output, from_logits = FALSE) Arguments target A tensor with the same shape as output.
output A tensor.
from_logits Whether output is expected to be a logits tensor. By default, we consider that output encodes a probability distribution.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Bitwise reduction (logical AND).</title><link>/reference/keras/k_all/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_all/</guid><description>Bitwise reduction (logical AND).
k_all(x, axis = NULL, keepdims = FALSE) Arguments x Tensor or variable.
axis Axis along which to perform the reduction (axis indexes are 1-based).
keepdims whether the drop or broadcast the reduction axes.
Value A uint8 tensor (0s and 1s).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Bitwise reduction (logical OR).</title><link>/reference/keras/k_any/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_any/</guid><description>Bitwise reduction (logical OR).
k_any(x, axis = NULL, keepdims = FALSE) Arguments x Tensor or variable.
axis Axis along which to perform the reduction (axis indexes are 1-based).
keepdims whether the drop or broadcast the reduction axes.
Value A uint8 tensor (0s and 1s).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Boston housing price regression dataset</title><link>/reference/keras/dataset_boston_housing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_boston_housing/</guid><description>Dataset taken from the StatLib library which is maintained at Carnegie Mellon University.
dataset_boston_housing( path = &#34;boston_housing.npz&#34;, test_split = 0.2, seed = 113L ) Arguments path Path where to cache the dataset locally (relative to ~/.keras/datasets).
test_split fraction of the data to reserve as test set.
seed Random seed for shuffling the data before computing the test split.
Value Lists of training and test data: train$x, train$y, test$x, test$y.</description></item><item><title>Callback that prints metrics to stdout.</title><link>/reference/keras/callback_progbar_logger/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_progbar_logger/</guid><description>Callback that prints metrics to stdout.
callback_progbar_logger(count_mode = &#34;samples&#34;, stateful_metrics = NULL) Arguments count_mode One of &#34;steps&#34; or &#34;samples&#34;. Whether the progress bar should count samples seens or steps (batches) seen.
stateful_metrics List of metric names that should not be averaged onver an epoch. Metrics in this list will be logged as-is in on_epoch_end. All others will be averaged in on_epoch_end.
See also Other callbacks: callback_csv_logger(), callback_early_stopping(), callback_lambda(), callback_learning_rate_scheduler(), callback_model_checkpoint(), callback_reduce_lr_on_plateau(), callback_remote_monitor(), callback_tensorboard(), callback_terminate_on_naan()</description></item><item><title>Callback that streams epoch results to a csv file</title><link>/reference/keras/callback_csv_logger/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_csv_logger/</guid><description>Supports all values that can be represented as a string
callback_csv_logger(filename, separator = &#34;,&#34;, append = FALSE) Arguments filename filename of the csv file, e.g. &#39;run/log.csv&#39;.
separator string used to separate elements in the csv file.
append TRUE: append if file exists (useful for continuing training). FALSE: overwrite existing file,
See also Other callbacks: callback_early_stopping(), callback_lambda(), callback_learning_rate_scheduler(), callback_model_checkpoint(), callback_progbar_logger(), callback_reduce_lr_on_plateau(), callback_remote_monitor(), callback_tensorboard(), callback_terminate_on_naan()</description></item><item><title>Callback that terminates training when a NaN loss is encountered.</title><link>/reference/keras/callback_terminate_on_naan/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_terminate_on_naan/</guid><description> Callback that terminates training when a NaN loss is encountered.
callback_terminate_on_naan() See also Other callbacks: callback_csv_logger(), callback_early_stopping(), callback_lambda(), callback_learning_rate_scheduler(), callback_model_checkpoint(), callback_progbar_logger(), callback_reduce_lr_on_plateau(), callback_remote_monitor(), callback_tensorboard()</description></item><item><title>Callback used to stream events to a server.</title><link>/reference/keras/callback_remote_monitor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_remote_monitor/</guid><description>Callback used to stream events to a server.
callback_remote_monitor( root = &#34;http://localhost:9000&#34;, path = &#34;/publish/epoch/end/&#34;, field = &#34;data&#34;, headers = NULL, send_as_json = FALSE ) Arguments root root url of the target server.
path path relative to root to which the events will be sent.
field JSON field under which the data will be stored.
headers Optional named list of custom HTTP headers.</description></item><item><title>Cast an array to the default Keras float type.</title><link>/reference/keras/k_cast_to_floatx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_cast_to_floatx/</guid><description>Cast an array to the default Keras float type.
k_cast_to_floatx(x) Arguments x Array.
Value The same array, cast to its new type.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Casts a tensor to a different dtype and returns it.</title><link>/reference/keras/k_cast/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_cast/</guid><description>You can cast a Keras variable but it still returns a Keras tensor.
k_cast(x, dtype) Arguments x Keras tensor (or variable).
dtype String, either (&#39;float16&#39;, &#39;float32&#39;, or &#39;float64&#39;).
Value Keras tensor with dtype dtype.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Categorical crossentropy between an output tensor and a target tensor.</title><link>/reference/keras/k_categorical_crossentropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_categorical_crossentropy/</guid><description>Categorical crossentropy between an output tensor and a target tensor.
k_categorical_crossentropy(target, output, from_logits = FALSE, axis = -1) Arguments target A tensor of the same shape as output.
output A tensor resulting from a softmax (unless from_logits is TRUE, in which case output is expected to be the logits).
from_logits Logical, whether output is the result of a softmax, or is a tensor of logits.</description></item><item><title>Categorical crossentropy with integer targets.</title><link>/reference/keras/k_sparse_categorical_crossentropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sparse_categorical_crossentropy/</guid><description>Categorical crossentropy with integer targets.
k_sparse_categorical_crossentropy( target, output, from_logits = FALSE, axis = -1 ) Arguments target An integer tensor.
output A tensor resulting from a softmax (unless from_logits is TRUE, in which case output is expected to be the logits).
from_logits Boolean, whether output is the result of a softmax, or is a tensor of logits.
axis Axis (axis indexes are 1-based).</description></item><item><title>Check if Keras is Available</title><link>/reference/keras/is_keras_available/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/is_keras_available/</guid><description>Probe to see whether the Keras python package is available in the current system environment.
is_keras_available(version = NULL) Arguments version Minimum required version of Keras (defaults to NULL, no required version).
Value Logical indicating whether Keras (or the specified minimum version of Keras) is available.
Examples if (FALSE) { # testthat utilty for skipping tests when Keras isn&#39;t available skip_if_no_keras &amp;lt;- function(version = NULL) { if (!</description></item><item><title>CIFAR10 small image classification</title><link>/reference/keras/dataset_cifar10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_cifar10/</guid><description>Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.
dataset_cifar10() Value Lists of training and test data: train$x, train$y, test$x, test$y.
The x data is an array of RGB image data with shape (num_samples, 3, 32, 32).
The y data is an array of category labels (integers in range 0-9) with shape (num_samples).
See also Other datasets: dataset_boston_housing(), dataset_cifar100(), dataset_fashion_mnist(), dataset_imdb(), dataset_mnist(), dataset_reuters()</description></item><item><title>CIFAR100 small image classification</title><link>/reference/keras/dataset_cifar100/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_cifar100/</guid><description>Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.
dataset_cifar100(label_mode = c(&#34;fine&#34;, &#34;coarse&#34;)) Arguments label_mode one of &#34;fine&#34;, &#34;coarse&#34;.
Value Lists of training and test data: train$x, train$y, test$x, test$y.
The x data is an array of RGB image data with shape (num_samples, 3, 32, 32).
The y data is an array of category labels with shape (num_samples).</description></item><item><title>Clone a model instance.</title><link>/reference/keras/clone_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/clone_model/</guid><description>Model cloning is similar to calling a model on new inputs, except that it creates new layers (and thus new weights) instead of sharing the weights of the existing layers.
clone_model(model, input_tensors = NULL) Arguments model Instance of Keras model (could be a functional model or a Sequential model).
input_tensors Optional list of input tensors to build the model upon. If not provided, placeholders will be created.</description></item><item><title>Compute the moving average of a variable.</title><link>/reference/keras/k_moving_average_update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_moving_average_update/</guid><description>Compute the moving average of a variable.
k_moving_average_update(x, value, momentum) Arguments x A Variable.
value A tensor with the same shape as x.
momentum The moving average momentum.
Value An operation to update the variable.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Computes cos of x element-wise.</title><link>/reference/keras/k_cos/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_cos/</guid><description>Computes cos of x element-wise.
k_cos(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Computes log(sum(exp(elements across dimensions of a tensor))).</title><link>/reference/keras/k_logsumexp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_logsumexp/</guid><description>This function is more numerically stable than log(sum(exp(x))). It avoids overflows caused by taking the exp of large inputs and underflows caused by taking the log of small inputs.
k_logsumexp(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, the axis to reduce over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not.</description></item><item><title>Computes mean and std for batch then apply batch_normalization on batch.</title><link>/reference/keras/k_normalize_batch_in_training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_normalize_batch_in_training/</guid><description>Computes mean and std for batch then apply batch_normalization on batch.
k_normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon = 0.001) Arguments x Input tensor or variable.
gamma Tensor by which to scale the input.
beta Tensor with which to center the input.
reduction_axes iterable of integers, axes over which to normalize.
epsilon Fuzz factor.
Value A list length of 3, (normalized_tensor, mean, variance).</description></item><item><title>Computes sin of x element-wise.</title><link>/reference/keras/k_sin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sin/</guid><description>Computes sin of x element-wise.
k_sin(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Computes the one-hot representation of an integer tensor.</title><link>/reference/keras/k_one_hot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_one_hot/</guid><description>Computes the one-hot representation of an integer tensor.
k_one_hot(indices, num_classes) Arguments indices nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
num_classes Integer, number of classes to consider.
Value (n + 1)D one hot representation of the input with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Concatenates a list of tensors alongside the specified axis.</title><link>/reference/keras/k_concatenate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_concatenate/</guid><description>Concatenates a list of tensors alongside the specified axis.
k_concatenate(tensors, axis = -1) Arguments tensors list of tensors to concatenate.
axis concatenation axis (axis indexes are 1-based). Pass -1 (the default) to select the last axis.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Configure a Keras model for training</title><link>/reference/keras/compile.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/compile.keras.engine.training.model/</guid><description>Configure a Keras model for training
# S3 method for keras.engine.training.Model compile( object, optimizer, loss, metrics = NULL, loss_weights = NULL, sample_weight_mode = NULL, weighted_metrics = NULL, target_tensors = NULL, ... ) Arguments object Model object to compile.
optimizer Name of optimizer or optimizer instance.
loss Name of objective function or objective function. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of objectives.</description></item><item><title>Constructs a DenseFeatures.</title><link>/reference/keras/layer_dense_features/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_dense_features/</guid><description>A layer that produces a dense Tensor based on given feature_columns.
layer_dense_features( object, feature_columns, name = NULL, trainable = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, weights = NULL ) Arguments object Model or layer object
feature_columns An iterable containing the FeatureColumns to use as inputs to your model. All items should be instances of classes derived from DenseColumn such as numeric_column, embedding_column, bucketized_column, indicator_column.</description></item><item><title>Convert a list of sequences into a matrix.</title><link>/reference/keras/sequences_to_matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/sequences_to_matrix/</guid><description> Convert a list of sequences into a matrix.
sequences_to_matrix( tokenizer, sequences, mode = c(&#34;binary&#34;, &#34;count&#34;, &#34;tfidf&#34;, &#34;freq&#34;) ) Arguments tokenizer Tokenizer
sequences List of sequences (a sequence is a list of integer word indices).
mode one of &#34;binary&#34;, &#34;count&#34;, &#34;tfidf&#34;, &#34;freq&#34;.
Value A matrix
See also Other text tokenization: fit_text_tokenizer(), save_text_tokenizer(), text_tokenizer(), texts_to_matrix(), texts_to_sequences_generator(), texts_to_sequences()</description></item><item><title>Convert a list of texts to a matrix.</title><link>/reference/keras/texts_to_matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/texts_to_matrix/</guid><description> Convert a list of texts to a matrix.
texts_to_matrix(tokenizer, texts, mode = c(&#34;binary&#34;, &#34;count&#34;, &#34;tfidf&#34;, &#34;freq&#34;)) Arguments tokenizer Tokenizer
texts Vector/list of texts (strings).
mode one of &#34;binary&#34;, &#34;count&#34;, &#34;tfidf&#34;, &#34;freq&#34;.
Value A matrix
See also Other text tokenization: fit_text_tokenizer(), save_text_tokenizer(), sequences_to_matrix(), text_tokenizer(), texts_to_sequences_generator(), texts_to_sequences()</description></item><item><title>Convert text to a sequence of words (or tokens).</title><link>/reference/keras/text_to_word_sequence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/text_to_word_sequence/</guid><description>Convert text to a sequence of words (or tokens).
text_to_word_sequence( text, filters = &#34;!\&#34;#$%&amp;amp;()*+,-./:;&amp;lt;=&amp;gt;?@[\\]^_`{|}~\t\n&#34;, lower = TRUE, split = &#34; &#34; ) Arguments text Input text (string).
filters Sequence of characters to filter out such as punctuation. Default includes basic punctuation, tabs, and newlines.
lower Whether to convert the input to lowercase.
split Sentence split marker (string).</description></item><item><title>Converts a class vector (integers) to binary class matrix.</title><link>/reference/keras/to_categorical/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/to_categorical/</guid><description>Converts a class vector (integers) to binary class matrix.
to_categorical(y, num_classes = NULL, dtype = &#34;float32&#34;) Arguments y Class vector to be converted into a matrix (integers from 0 to num_classes).
num_classes Total number of classes.
dtype The data type expected by the input, as a string
Value A binary matrix representation of the input.
Details E.g. for use with loss_categorical_crossentropy().</description></item><item><title>Converts a sparse tensor into a dense tensor and returns it.</title><link>/reference/keras/k_to_dense/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_to_dense/</guid><description>Converts a sparse tensor into a dense tensor and returns it.
k_to_dense(tensor) Arguments tensor A tensor instance (potentially sparse).
Value A dense tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Converts a text to a sequence of indexes in a fixed-size hashing space.</title><link>/reference/keras/text_hashing_trick/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/text_hashing_trick/</guid><description>Converts a text to a sequence of indexes in a fixed-size hashing space.
text_hashing_trick( text, n, hash_function = NULL, filters = &#34;!\&#34;#$%&amp;amp;()*+,-./:;&amp;lt;=&amp;gt;?@[\\]^_`{|}~\t\n&#34;, lower = TRUE, split = &#34; &#34; ) Arguments text Input text (string).
n Dimension of the hashing space.
hash_function if NULL uses python hash function, can be &#39;md5&#39; or any function that takes in input a string and returns a int.</description></item><item><title>Converts CTC labels from dense to sparse.</title><link>/reference/keras/k_ctc_label_dense_to_sparse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ctc_label_dense_to_sparse/</guid><description>Converts CTC labels from dense to sparse.
k_ctc_label_dense_to_sparse(labels, label_lengths) Arguments labels dense CTC labels.
label_lengths length of the labels.
Value A sparse tensor representation of the labels.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Convolutional LSTM.</title><link>/reference/keras/layer_conv_lstm_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_lstm_2d/</guid><description>It is similar to an LSTM layer, but the input transformations and recurrent transformations are both convolutional.
layer_conv_lstm_2d( object, filters, kernel_size, strides = c(1L, 1L), padding = &#34;valid&#34;, data_format = NULL, dilation_rate = c(1L, 1L), activation = &#34;tanh&#34;, recurrent_activation = &#34;hard_sigmoid&#34;, use_bias = TRUE, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;orthogonal&#34;, bias_initializer = &#34;zeros&#34;, unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL, return_sequences = FALSE, go_backwards = FALSE, stateful = FALSE, dropout = 0, recurrent_dropout = 0, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL, input_shape = NULL ) Arguments object Model or layer object</description></item><item><title>Count the total number of scalars composing the weights.</title><link>/reference/keras/count_params/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/count_params/</guid><description> Count the total number of scalars composing the weights.
count_params(object) Arguments object Layer or model object
Value An integer count
See also Other layer methods: get_config(), get_input_at(), get_weights(), reset_states()</description></item><item><title>Create a custom callback</title><link>/reference/keras/callback_lambda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_lambda/</guid><description>This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as:
on_epoch_begin and on_epoch_end expect two positional arguments: epoch, logs
on_batch_*, on_train_batch_*, on_predict_batch_* and on_test_batch_*, expect two positional arguments: batch, logs
on_train_*, on_test_* and on_predict_* expect one positional argument: logs
callback_lambda( on_epoch_begin = NULL, on_epoch_end = NULL, on_batch_begin = NULL, on_batch_end = NULL, on_train_batch_begin = NULL, on_train_batch_end = NULL, on_train_begin = NULL, on_train_end = NULL, on_predict_batch_begin = NULL, on_predict_batch_end = NULL, on_predict_begin = NULL, on_predict_end = NULL, on_test_batch_begin = NULL, on_test_batch_end = NULL, on_test_begin = NULL, on_test_end = NULL ) Arguments on_epoch_begin called at the beginning of every epoch.</description></item><item><title>Create a Keras custom model</title><link>/reference/keras/keras_model_custom/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keras_model_custom/</guid><description>Create a Keras custom model
keras_model_custom(model_fn, name = NULL) Arguments model_fn Function that returns an R custom model
name Optional name for model
Value A Keras model
Details For documentation on using custom models, see https://keras.rstudio.com/articles/custom_models.html.</description></item><item><title>Create a Keras Layer</title><link>/reference/keras/create_layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/create_layer/</guid><description>Create a Keras Layer
create_layer(layer_class, object, args = list()) Arguments layer_class Python layer class or R6 class of type KerasLayer
object Object to compose layer with. This is either a keras_model_sequential() to add the layer to, or another Layer which this layer will call.
args List of arguments to layer constructor function
Value A Keras layer
Note The object parameter can be missing, in which case the layer is created without a connection to an existing graph.</description></item><item><title>Create a Keras Wrapper</title><link>/reference/keras/create_wrapper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/create_wrapper/</guid><description>Create a Keras Wrapper
create_wrapper(wrapper_class, object, args = list()) Arguments wrapper_class R6 class of type KerasWrapper
object Object to compose layer with. This is either a keras_model_sequential() to add the layer to, or another Layer which this layer will call.
args List of arguments to layer constructor function
Value A Keras wrapper
Note The object parameter can be missing, in which case the layer is created without a connection to an existing graph.</description></item><item><title>Creates a 1D tensor containing a sequence of integers.</title><link>/reference/keras/k_arange/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_arange/</guid><description>The function arguments use the same convention as Theano&#39;s arange: if only one argument is provided, it is in fact the &#34;stop&#34; argument. The default type of the returned tensor is &#39;int32&#39; to match TensorFlow&#39;s default.
k_arange(start, stop = NULL, step = 1, dtype = &#34;int32&#34;) Arguments start Start value.
stop Stop value.
step Difference between two successive values.
dtype Integer dtype to use.</description></item><item><title>Creates a constant tensor.</title><link>/reference/keras/k_constant/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_constant/</guid><description>Creates a constant tensor.
k_constant(value, dtype = NULL, shape = NULL, name = NULL) Arguments value A constant value
dtype The type of the elements of the resulting tensor.
shape Optional dimensions of resulting tensor.
name Optional name for the tensor.
Value A Constant Tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Creates a tensor by tiling &lt;code&gt;x&lt;/code&gt; by &lt;code&gt;n&lt;/code&gt;.</title><link>/reference/keras/k_tile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_tile/</guid><description>Creates a tensor by tiling x by n.
k_tile(x, n) Arguments x A tensor or variable
n A list of integers. The length must be the same as the number of dimensions in x.
Value A tiled tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Cropping layer for 1D input (e.g. temporal sequence).</title><link>/reference/keras/layer_cropping_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_cropping_1d/</guid><description>It crops along the time dimension (axis 1).
layer_cropping_1d( object, cropping = c(1L, 1L), batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
cropping int or list of int (length 2) How many units should be trimmed off at the beginning and end of the cropping dimension (axis 1). If a single int is provided, the same value will be used for both.</description></item><item><title>Cropping layer for 2D input (e.g. picture).</title><link>/reference/keras/layer_cropping_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_cropping_2d/</guid><description>It crops along spatial dimensions, i.e. width and height.
layer_cropping_2d( object, cropping = list(c(0L, 0L), c(0L, 0L)), data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
cropping int, or list of 2 ints, or list of 2 lists of 2 ints.
If int: the same symmetric cropping is applied to width and height.</description></item><item><title>Cropping layer for 3D data (e.g. spatial or spatio-temporal).</title><link>/reference/keras/layer_cropping_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_cropping_3d/</guid><description>Cropping layer for 3D data (e.g. spatial or spatio-temporal).
layer_cropping_3d( object, cropping = list(c(1L, 1L), c(1L, 1L), c(1L, 1L)), data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
cropping int, or list of 3 ints, or list of 3 lists of 2 ints.
If int: the same symmetric cropping is applied to depth, height, and width.</description></item><item><title>Cumulative product of the values in a tensor, alongside the specified axis.</title><link>/reference/keras/k_cumprod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_cumprod/</guid><description>Cumulative product of the values in a tensor, alongside the specified axis.
k_cumprod(x, axis = 1) Arguments x A tensor or variable.
axis An integer, the axis to compute the product (axis indexes are 1-based).
Value A tensor of the cumulative product of values of x along axis.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Cumulative sum of the values in a tensor, alongside the specified axis.</title><link>/reference/keras/k_cumsum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_cumsum/</guid><description>Cumulative sum of the values in a tensor, alongside the specified axis.
k_cumsum(x, axis = 1) Arguments x A tensor or variable.
axis An integer, the axis to compute the sum (axis indexes are 1-based).
Value A tensor of the cumulative sum of values of x along axis.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Decodes the output of a softmax.</title><link>/reference/keras/k_ctc_decode/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ctc_decode/</guid><description>Can use either greedy search (also known as best path) or a constrained dictionary search.
k_ctc_decode( y_pred, input_length, greedy = TRUE, beam_width = 100L, top_paths = 1 ) Arguments y_pred tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.
input_length tensor (samples, ) containing the sequence length for each batch item in y_pred.
greedy perform much faster best-path search if TRUE.</description></item><item><title>Decodes the prediction of an ImageNet model.</title><link>/reference/keras/imagenet_decode_predictions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/imagenet_decode_predictions/</guid><description>Decodes the prediction of an ImageNet model.
imagenet_decode_predictions(preds, top = 5) Arguments preds Tensor encoding a batch of predictions.
top integer, how many top-guesses to return.
Value List of data frames with variables class_name, class_description, and score (one data frame per sample in batch input).</description></item><item><title>Default float type</title><link>/reference/keras/k_floatx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_floatx/</guid><description>Default float type
k_floatx() k_set_floatx(floatx) Arguments floatx String, &#39;float16&#39;, &#39;float32&#39;, or &#39;float64&#39;.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Default image data format convention (&#39;channels_first&#39; or &#39;channels_last&#39;).</title><link>/reference/keras/k_image_data_format/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_image_data_format/</guid><description>Default image data format convention (&#39;channels_first&#39; or &#39;channels_last&#39;).
k_image_data_format() k_set_image_data_format(data_format) Arguments data_format string. &#39;channels_first&#39; or &#39;channels_last&#39;.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Depthwise 2D convolution with separable filters.</title><link>/reference/keras/k_depthwise_conv2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_depthwise_conv2d/</guid><description>Depthwise 2D convolution with separable filters.
k_depthwise_conv2d( x, depthwise_kernel, strides = c(1, 1), padding = &#34;valid&#34;, data_format = NULL, dilation_rate = c(1, 1) ) Arguments x input tensor
depthwise_kernel convolution kernel for the depthwise convolution.
strides strides (length 2).
padding string, &#34;same&#34; or &#34;valid&#34;.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
dilation_rate vector of integers, dilation rates for the separable convolution.</description></item><item><title>Depthwise separable 1D convolution.</title><link>/reference/keras/layer_separable_conv_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_separable_conv_1d/</guid><description>Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block.</description></item><item><title>Depthwise separable 2D convolution.</title><link>/reference/keras/layer_depthwise_conv_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_depthwise_conv_2d/</guid><description>Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step.
layer_depthwise_conv_2d( object, kernel_size, strides = c(1, 1), padding = &#34;valid&#34;, depth_multiplier = 1, data_format = NULL, activation = NULL, use_bias = TRUE, depthwise_initializer = &#34;glorot_uniform&#34;, bias_initializer = &#34;zeros&#34;, depthwise_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, depthwise_constraint = NULL, bias_constraint = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Destroys the current TF graph and creates a new one.</title><link>/reference/keras/k_clear_session/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_clear_session/</guid><description>Useful to avoid clutter from old models / layers.
k_clear_session() Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Downloads a file from a URL if it not already in the cache.</title><link>/reference/keras/get_file/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_file/</guid><description>Passing the MD5 hash will verify the file after download as well as if it is already present in the cache.
get_file( fname, origin, file_hash = NULL, cache_subdir = &#34;datasets&#34;, hash_algorithm = &#34;auto&#34;, extract = FALSE, archive_format = &#34;auto&#34;, cache_dir = NULL ) Arguments fname Name of the file. If an absolute path /path/to/file.txt is specified the file will be saved at that location.
origin Original URL of the file.</description></item><item><title>Element-wise absolute value.</title><link>/reference/keras/k_abs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_abs/</guid><description>Element-wise absolute value.
k_abs(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise equality between two tensors.</title><link>/reference/keras/k_equal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_equal/</guid><description>Element-wise equality between two tensors.
k_equal(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise exponential.</title><link>/reference/keras/k_exp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_exp/</guid><description>Element-wise exponential.
k_exp(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise exponentiation.</title><link>/reference/keras/k_pow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_pow/</guid><description>Element-wise exponentiation.
k_pow(x, a) Arguments x Tensor or variable.
a R integer.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise inequality between two tensors.</title><link>/reference/keras/k_not_equal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_not_equal/</guid><description>Element-wise inequality between two tensors.
k_not_equal(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise log.</title><link>/reference/keras/k_log/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_log/</guid><description>Element-wise log.
k_log(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise maximum of two tensors.</title><link>/reference/keras/k_maximum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_maximum/</guid><description>Element-wise maximum of two tensors.
k_maximum(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise minimum of two tensors.</title><link>/reference/keras/k_minimum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_minimum/</guid><description>Element-wise minimum of two tensors.
k_minimum(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise rounding to the closest integer.</title><link>/reference/keras/k_round/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_round/</guid><description>In case of tie, the rounding mode used is &#34;half to even&#34;.
k_round(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise sigmoid.</title><link>/reference/keras/k_sigmoid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sigmoid/</guid><description>Element-wise sigmoid.
k_sigmoid(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise sign.</title><link>/reference/keras/k_sign/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sign/</guid><description>Element-wise sign.
k_sign(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise square root.</title><link>/reference/keras/k_sqrt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sqrt/</guid><description>Element-wise square root.
k_sqrt(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise square.</title><link>/reference/keras/k_square/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_square/</guid><description>Element-wise square.
k_square(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise tanh.</title><link>/reference/keras/k_tanh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_tanh/</guid><description>Element-wise tanh.
k_tanh(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Element-wise truth value of (x &amp;gt; y).</title><link>/reference/keras/k_greater/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_greater/</guid><description>Element-wise truth value of (x &amp;gt; y).
k_greater(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise truth value of (x &amp;gt;= y).</title><link>/reference/keras/k_greater_equal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_greater_equal/</guid><description>Element-wise truth value of (x &amp;gt;= y).
k_greater_equal(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise truth value of (x &amp;lt; y).</title><link>/reference/keras/k_less/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_less/</guid><description>Element-wise truth value of (x &amp;lt; y).
k_less(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise truth value of (x &amp;lt;= y).</title><link>/reference/keras/k_less_equal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_less_equal/</guid><description>Element-wise truth value of (x &amp;lt;= y).
k_less_equal(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A bool tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Element-wise value clipping.</title><link>/reference/keras/k_clip/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_clip/</guid><description>Element-wise value clipping.
k_clip(x, min_value, max_value) Arguments x Tensor or variable.
min_value Float or integer.
max_value Float or integer.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Evaluate a Keras model</title><link>/reference/keras/evaluate.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/evaluate.keras.engine.training.model/</guid><description>Evaluate a Keras model
# S3 method for keras.engine.training.Model evaluate( object, x = NULL, y = NULL, batch_size = NULL, verbose = 1, sample_weight = NULL, steps = NULL, callbacks = NULL, ... ) Arguments object Model object to evaluate
x Vector, matrix, or array of test data (or list if the model has multiple inputs). If all inputs in the model are named, you can also pass a list mapping input names to data.</description></item><item><title>Evaluates the model on a data generator.</title><link>/reference/keras/evaluate_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/evaluate_generator/</guid><description>The generator should return the same kind of data as accepted by test_on_batch().
evaluate_generator( object, generator, steps, max_queue_size = 10, workers = 1, callbacks = NULL ) Arguments object Model object to evaluate
generator Generator yielding lists (inputs, targets) or (inputs, targets, sample_weights)
steps Total number of steps (batches of samples) to yield from generator before stopping.
max_queue_size Maximum size for the generator queue.</description></item><item><title>Evaluates the value of a variable.</title><link>/reference/keras/k_eval/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_eval/</guid><description>Evaluates the value of a variable.
k_eval(x) Arguments x A variable.
Value An R array.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Exponential Linear Unit.</title><link>/reference/keras/layer_activation_elu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_elu/</guid><description>It follows: f(x) = alpha * (exp(x) - 1.0) for x &amp;lt; 0, f(x) = x for x &amp;gt;= 0.
layer_activation_elu( object, alpha = 1, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
alpha Scale for the negative factor.
input_shape Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.</description></item><item><title>Exponential linear unit.</title><link>/reference/keras/k_elu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_elu/</guid><description>Exponential linear unit.
k_elu(x, alpha = 1) Arguments x A tensor or variable to compute the activation function for.
alpha A scalar, slope of negative section.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.</description></item><item><title>Export a Saved Model</title><link>/reference/keras/export_savedmodel.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/export_savedmodel.keras.engine.training.model/</guid><description>Serialize a model to disk.
# S3 method for keras.engine.training.Model export_savedmodel( object, export_dir_base, overwrite = TRUE, versioned = !overwrite, remove_learning_phase = TRUE, as_text = FALSE, ... ) Arguments object An R object.
export_dir_base A string containing a directory in which to export the SavedModel.
overwrite Should the export_dir_base directory be overwritten?
versioned Should the model be exported under a versioned subdirectory?</description></item><item><title>Export to Saved Model format</title><link>/reference/keras/model_to_saved_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/model_to_saved_model/</guid><description>Export to Saved Model format
model_to_saved_model( model, saved_model_path, custom_objects = NULL, as_text = FALSE, input_signature = NULL, serving_only = FALSE ) Arguments model A Keras model to be saved. If the model is subclassed, the flag serving_only must be set to TRUE.
saved_model_path a string specifying the path to the SavedModel directory.
custom_objects Optional dictionary mapping string names to custom classes or functions (e.</description></item><item><title>Fashion-MNIST database of fashion articles</title><link>/reference/keras/dataset_fashion_mnist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_fashion_mnist/</guid><description>Dataset of 60,000 28x28 grayscale images of the 10 fashion article classes, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are encoded as integers from 0-9 which correspond to T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt,
dataset_fashion_mnist() Value Lists of training and test data: train$x, train$y, test$x, test$y, where x is an array of grayscale image data with shape (num_samples, 28, 28) and y is an array of article labels (integers in range 0-9) with shape (num_samples).</description></item><item><title>Fast GRU implementation backed by &lt;a href=&#39;https://developer.nvidia.com/cudnn&#39;&gt;CuDNN&lt;/a&gt;.</title><link>/reference/keras/layer_cudnn_gru/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_cudnn_gru/</guid><description>Can only be run on GPU, with the TensorFlow backend.
layer_cudnn_gru( object, units, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;orthogonal&#34;, bias_initializer = &#34;zeros&#34;, kernel_regularizer = NULL, recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL, return_sequences = FALSE, return_state = FALSE, stateful = FALSE, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Fast LSTM implementation backed by &lt;a href=&#39;https://developer.nvidia.com/cudnn&#39;&gt;CuDNN&lt;/a&gt;.</title><link>/reference/keras/layer_cudnn_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_cudnn_lstm/</guid><description>Can only be run on GPU, with the TensorFlow backend.
layer_cudnn_lstm( object, units, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;orthogonal&#34;, bias_initializer = &#34;zeros&#34;, unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL, return_sequences = FALSE, return_state = FALSE, stateful = FALSE, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Fit image data generator internal statistics to some sample data.</title><link>/reference/keras/fit_image_data_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/fit_image_data_generator/</guid><description>Required for featurewise_center, featurewise_std_normalization and zca_whitening.
fit_image_data_generator(object, x, augment = FALSE, rounds = 1, seed = NULL) Arguments object image_data_generator()
x array, the data to fit on (should have rank 4). In case of grayscale data, the channels axis should have value 1, and in case of RGB data, it should have value 3.
augment Whether to fit on randomly augmented samples</description></item><item><title>Fits the model on data yielded batch-by-batch by a generator.</title><link>/reference/keras/fit_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/fit_generator/</guid><description>The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.
fit_generator( object, generator, steps_per_epoch, epochs = 1, verbose = getOption(&#34;keras.fit_verbose&#34;, default = 1), callbacks = NULL, view_metrics = getOption(&#34;keras.view_metrics&#34;, default = &#34;auto&#34;), validation_data = NULL, validation_steps = NULL, class_weight = NULL, max_queue_size = 10, workers = 1, initial_epoch = 0 ) Arguments object Keras model object</description></item><item><title>Fits the state of the preprocessing layer to the data being passed.</title><link>/reference/keras/adapt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/adapt/</guid><description>Fits the state of the preprocessing layer to the data being passed.
adapt(object, data, reset_state = NULL) Arguments object Preprocessing layer object
data The data to train on. It can be passed either as a tf.data Dataset, or as an R array.
reset_state Optional argument specifying whether to clear the state of the layer at the start of the call to adapt, or whether to start from the existing state.</description></item><item><title>Flatten a tensor.</title><link>/reference/keras/k_flatten/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_flatten/</guid><description>Flatten a tensor.
k_flatten(x) Arguments x A tensor or variable.
Value A tensor, reshaped into 1-D
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Flattens an input</title><link>/reference/keras/layer_flatten/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_flatten/</guid><description>Flatten a given input, does not affect the batch size.
layer_flatten( object, data_format = NULL, input_shape = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format A string. one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another.</description></item><item><title>Freeze and unfreeze weights</title><link>/reference/keras/freeze_weights/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/freeze_weights/</guid><description>Freeze weights in a model or layer so that they are no longer trainable.
freeze_weights(object, from = NULL, to = NULL) unfreeze_weights(object, from = NULL, to = NULL) Arguments object Keras model or layer object
from Layer instance, layer name, or layer index within model
to Layer instance, layer name, or layer index within model
Note The from and to layer arguments are both inclusive.</description></item><item><title>Fully-connected RNN where the output is to be fed back to input.</title><link>/reference/keras/layer_simple_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_simple_rnn/</guid><description>Fully-connected RNN where the output is to be fed back to input.
layer_simple_rnn( object, units, activation = &#34;tanh&#34;, use_bias = TRUE, return_sequences = FALSE, return_state = FALSE, go_backwards = FALSE, stateful = FALSE, unroll = FALSE, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;orthogonal&#34;, bias_initializer = &#34;zeros&#34;, kernel_regularizer = NULL, recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0, recurrent_dropout = 0, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Fuzz factor used in numeric expressions.</title><link>/reference/keras/k_epsilon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_epsilon/</guid><description>Fuzz factor used in numeric expressions.
k_epsilon() k_set_epsilon(e) Arguments e float. New value of epsilon.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Gated Recurrent Unit - Cho et al.</title><link>/reference/keras/layer_gru/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_gru/</guid><description>There are two variants. The default one is based on 1406.1078v3 and has reset gate applied to hidden state before matrix multiplication. The other one is based on original 1406.1078v1 and has the order reversed.
layer_gru( object, units, activation = &#34;tanh&#34;, recurrent_activation = &#34;hard_sigmoid&#34;, use_bias = TRUE, return_sequences = FALSE, return_state = FALSE, go_backwards = FALSE, stateful = FALSE, unroll = FALSE, reset_after = FALSE, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;</description></item><item><title>Generate batches of image data with real-time data augmentation. The data will be looped over (in batches).</title><link>/reference/keras/image_data_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/image_data_generator/</guid><description>Generate batches of image data with real-time data augmentation. The data will be looped over (in batches).
image_data_generator( featurewise_center = FALSE, samplewise_center = FALSE, featurewise_std_normalization = FALSE, samplewise_std_normalization = FALSE, zca_whitening = FALSE, zca_epsilon = 1e-06, rotation_range = 0, width_shift_range = 0, height_shift_range = 0, brightness_range = NULL, shear_range = 0, zoom_range = 0, channel_shift_range = 0, fill_mode = &#34;nearest&#34;, cval = 0, horizontal_flip = FALSE, vertical_flip = FALSE, rescale = NULL, preprocessing_function = NULL, data_format = NULL, validation_split = 0 ) Arguments featurewise_center Set input mean to 0 over the dataset, feature-wise.</description></item><item><title>Generate predictions from a Keras model</title><link>/reference/keras/predict.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/predict.keras.engine.training.model/</guid><description>Generates output predictions for the input samples, processing the samples in a batched way.
# S3 method for keras.engine.training.Model predict( object, x, batch_size = NULL, verbose = 0, steps = NULL, callbacks = NULL, ... ) Arguments object Keras model
x Input data (vector, matrix, or array)
batch_size Integer. If unspecified, it will default to 32.
verbose Verbosity mode, 0 or 1.</description></item><item><title>Generates a word rank-based probabilistic sampling table.</title><link>/reference/keras/make_sampling_table/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/make_sampling_table/</guid><description>Generates a word rank-based probabilistic sampling table.
make_sampling_table(size, sampling_factor = 1e-05) Arguments size Int, number of possible words to sample.
sampling_factor The sampling factor in the word2vec formula.
Value An array of length size where the ith entry is the probability that a word of rank i should be sampled.
Details Used for generating the sampling_table argument for skipgrams(). sampling_table[[i]] is the probability of sampling the word i-th most common word in a dataset (more common words should be sampled less frequently, for balance).</description></item><item><title>Generates batches of augmented/normalized data from image data and labels</title><link>/reference/keras/flow_images_from_data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/flow_images_from_data/</guid><description>Generates batches of augmented/normalized data from image data and labels
flow_images_from_data( x, y = NULL, generator = image_data_generator(), batch_size = 32, shuffle = TRUE, sample_weight = NULL, seed = NULL, save_to_dir = NULL, save_prefix = &#34;&#34;, save_format = &#34;png&#34;, subset = NULL ) Arguments x data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, it should have value 3.</description></item><item><title>Generates batches of data from images in a directory (with optional augmented/normalized data)</title><link>/reference/keras/flow_images_from_directory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/flow_images_from_directory/</guid><description>Generates batches of data from images in a directory (with optional augmented/normalized data)
flow_images_from_directory( directory, generator = image_data_generator(), target_size = c(256, 256), color_mode = &#34;rgb&#34;, classes = NULL, class_mode = &#34;categorical&#34;, batch_size = 32, shuffle = TRUE, seed = NULL, save_to_dir = NULL, save_prefix = &#34;&#34;, save_format = &#34;png&#34;, follow_links = FALSE, subset = NULL, interpolation = &#34;nearest&#34; ) Arguments directory path to the target directory. It should contain one subdirectory per class.</description></item><item><title>Generates predictions for the input samples from a data generator.</title><link>/reference/keras/predict_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/predict_generator/</guid><description>The generator should return the same kind of data as accepted by predict_on_batch().
predict_generator( object, generator, steps, max_queue_size = 10, workers = 1, verbose = 0, callbacks = NULL ) Arguments object Keras model object
generator Generator yielding batches of input samples.
steps Total number of steps (batches of samples) to yield from generator before stopping.
max_queue_size Maximum size for the generator queue.</description></item><item><title>Generates probability or class probability predictions for the input samples.</title><link>/reference/keras/predict_proba/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/predict_proba/</guid><description>Generates probability or class probability predictions for the input samples.
predict_proba(object, x, batch_size = NULL, verbose = 0, steps = NULL) predict_classes(object, x, batch_size = NULL, verbose = 0, steps = NULL) Arguments object Keras model object
x Input data (vector, matrix, or array)
batch_size Integer. If unspecified, it will default to 32.
verbose Verbosity mode, 0 or 1.</description></item><item><title>Generates skipgram word pairs.</title><link>/reference/keras/skipgrams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/skipgrams/</guid><description>Generates skipgram word pairs.
skipgrams( sequence, vocabulary_size, window_size = 4, negative_samples = 1, shuffle = TRUE, categorical = FALSE, sampling_table = NULL, seed = NULL ) Arguments sequence A word sequence (sentence), encoded as a list of word indices (integers). If using a sampling_table, word indices are expected to match the rank of the words in a reference dataset (e.g. 10 would encode the 10-th most frequently occuring token).</description></item><item><title>Get the uid for the default graph.</title><link>/reference/keras/k_get_uid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_get_uid/</guid><description>Get the uid for the default graph.
k_get_uid(prefix = &#34;&#34;) Arguments prefix An optional prefix of the graph.
Value A unique identifier for the graph.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Get the vocabulary for text vectorization layers</title><link>/reference/keras/get_vocabulary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_vocabulary/</guid><description> Get the vocabulary for text vectorization layers
get_vocabulary(object) Arguments object a text vectorization layer
See also set_vocabulary()</description></item><item><title>Global Average pooling operation for 3D data.</title><link>/reference/keras/layer_global_average_pooling_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_average_pooling_3d/</guid><description>Global Average pooling operation for 3D data.
layer_global_average_pooling_3d( object, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3).</description></item><item><title>Global average pooling operation for spatial data.</title><link>/reference/keras/layer_global_average_pooling_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_average_pooling_2d/</guid><description>Global average pooling operation for spatial data.
layer_global_average_pooling_2d( object, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width).</description></item><item><title>Global average pooling operation for temporal data.</title><link>/reference/keras/layer_global_average_pooling_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_average_pooling_1d/</guid><description>Global average pooling operation for temporal data.
layer_global_average_pooling_1d( object, data_format = &#34;channels_last&#34;, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format One of channels_last (default) or channels_first. The ordering of the dimensions in the inputs.
batch_size Fixed batch size for layer
name An optional name string for the layer.</description></item><item><title>Global Max pooling operation for 3D data.</title><link>/reference/keras/layer_global_max_pooling_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_max_pooling_3d/</guid><description>Global Max pooling operation for 3D data.
layer_global_max_pooling_3d( object, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, spatial_dim1, spatial_dim2, spatial_dim3, channels) while channels_first corresponds to inputs with shape (batch, channels, spatial_dim1, spatial_dim2, spatial_dim3).</description></item><item><title>Global max pooling operation for spatial data.</title><link>/reference/keras/layer_global_max_pooling_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_max_pooling_2d/</guid><description>Global max pooling operation for spatial data.
layer_global_max_pooling_2d( object, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width).</description></item><item><title>Global max pooling operation for temporal data.</title><link>/reference/keras/layer_global_max_pooling_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_global_max_pooling_1d/</guid><description>Global max pooling operation for temporal data.
layer_global_max_pooling_1d( object, data_format = &#34;channels_last&#34;, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
data_format One of channels_last (default) or channels_first. The ordering of the dimensions in the inputs.
batch_size Fixed batch size for layer
name An optional name string for the layer.</description></item><item><title>Glorot normal initializer, also called Xavier normal initializer.</title><link>/reference/keras/initializer_glorot_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_glorot_normal/</guid><description>It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
initializer_glorot_normal(seed = NULL) Arguments seed Integer used to seed the random generator.
References Glorot &amp;amp; Bengio, AISTATS 2010 http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
See also Other initializers: initializer_constant(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Glorot uniform initializer, also called Xavier uniform initializer.</title><link>/reference/keras/initializer_glorot_uniform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_glorot_uniform/</guid><description>It draws samples from a uniform distribution within -limit, limit where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.
initializer_glorot_uniform(seed = NULL) Arguments seed Integer used to seed the random generator.
References Glorot &amp;amp; Bengio, AISTATS 2010 http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>He normal initializer.</title><link>/reference/keras/initializer_he_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_he_normal/</guid><description>It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.
initializer_he_normal(seed = NULL) Arguments seed Integer used to seed the random generator.
References He et al., http://arxiv.org/abs/1502.01852
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>He uniform variance scaling initializer.</title><link>/reference/keras/initializer_he_uniform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_he_uniform/</guid><description> It draws samples from a uniform distribution within -limit, limit where limit`` is sqrt(6 / fan_in)where fan_in` is the number of input units in the weight tensor.
initializer_he_uniform(seed = NULL) Arguments seed Integer used to seed the random generator.
References He et al., http://arxiv.org/abs/1502.01852
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>IMDB Movie reviews sentiment classification</title><link>/reference/keras/dataset_imdb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_imdb/</guid><description>Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer &#34;3&#34; encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: &#34;only consider the top 10,000 most common words, but eliminate the top 20 most common words&#34;</description></item><item><title>Inception V3 model, with weights pre-trained on ImageNet.</title><link>/reference/keras/application_inception_v3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_inception_v3/</guid><description>Inception V3 model, with weights pre-trained on ImageNet.
application_inception_v3( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) inception_v3_preprocess_input(x) Arguments include_top whether to include the fully-connected layer at the top of the network.
weights NULL (random initialization), imagenet (ImageNet weights), or the path to the weights file to be loaded.
input_tensor optional Keras tensor to use as image input for the model.</description></item><item><title>Inception-ResNet v2 model, with weights trained on ImageNet</title><link>/reference/keras/application_inception_resnet_v2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_inception_resnet_v2/</guid><description>Inception-ResNet v2 model, with weights trained on ImageNet
application_inception_resnet_v2( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) inception_resnet_v2_preprocess_input(x) Arguments include_top whether to include the fully-connected layer at the top of the network.
weights NULL (random initialization), imagenet (ImageNet weights), or the path to the weights file to be loaded.
input_tensor optional Keras tensor to use as image input for the model.</description></item><item><title>Initializer capable of adapting its scale to the shape of weights.</title><link>/reference/keras/initializer_variance_scaling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_variance_scaling/</guid><description>With distribution=&#34;normal&#34;, samples are drawn from a truncated normal distribution centered on zero, with stddev = sqrt(scale / n) where n is:
number of input units in the weight tensor, if mode = &#34;fan_in&#34;
number of output units, if mode = &#34;fan_out&#34;
average of the numbers of input and output units, if mode = &#34;fan_avg&#34;
initializer_variance_scaling( scale = 1, mode = c(&#34;fan_in&#34;, &#34;fan_out&#34;, &#34;fan_avg&#34;), distribution = c(&#34;</description></item><item><title>Initializer that generates a random orthogonal matrix.</title><link>/reference/keras/initializer_orthogonal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_orthogonal/</guid><description> Initializer that generates a random orthogonal matrix.
initializer_orthogonal(gain = 1, seed = NULL) Arguments gain Multiplicative factor to apply to the orthogonal matrix.
seed Integer used to seed the random generator.
References Saxe et al., http://arxiv.org/abs/1312.6120
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Initializer that generates a truncated normal distribution.</title><link>/reference/keras/initializer_truncated_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_truncated_normal/</guid><description>These values are similar to values from an initializer_random_normal() except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.
initializer_truncated_normal(mean = 0, stddev = 0.05, seed = NULL) Arguments mean Mean of the random values to generate.
stddev Standard deviation of the random values to generate.
seed Integer used to seed the random generator.</description></item><item><title>Initializer that generates tensors initialized to 0.</title><link>/reference/keras/initializer_zeros/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_zeros/</guid><description> Initializer that generates tensors initialized to 0.
initializer_zeros() See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling()</description></item><item><title>Initializer that generates tensors initialized to 1.</title><link>/reference/keras/initializer_ones/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_ones/</guid><description> Initializer that generates tensors initialized to 1.
initializer_ones() See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Initializer that generates tensors initialized to a constant value.</title><link>/reference/keras/initializer_constant/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_constant/</guid><description> Initializer that generates tensors initialized to a constant value.
initializer_constant(value = 0) Arguments value float; the value of the generator tensors.
See also Other initializers: initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Initializer that generates tensors with a normal distribution.</title><link>/reference/keras/initializer_random_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_random_normal/</guid><description>Initializer that generates tensors with a normal distribution.
initializer_random_normal(mean = 0, stddev = 0.05, seed = NULL) Arguments mean Mean of the random values to generate.
stddev Standard deviation of the random values to generate.
seed Integer used to seed the random generator.
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Initializer that generates tensors with a uniform distribution.</title><link>/reference/keras/initializer_random_uniform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_random_uniform/</guid><description>Initializer that generates tensors with a uniform distribution.
initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = NULL) Arguments minval Lower bound of the range of random values to generate.
maxval Upper bound of the range of random values to generate. Defaults to 1 for float types.
seed seed
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Initializer that generates the identity matrix.</title><link>/reference/keras/initializer_identity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_identity/</guid><description> Only use for square 2D matrices.
initializer_identity(gain = 1) Arguments gain Multiplicative factor to apply to the identity matrix
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_lecun_normal(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Input layer</title><link>/reference/keras/layer_input/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_input/</guid><description>Layer to be used as an entry point into a graph.
layer_input( shape = NULL, batch_shape = NULL, name = NULL, dtype = NULL, sparse = FALSE, tensor = NULL ) Arguments shape Shape, not including the batch size. For instance, shape=c(32) indicates that the expected input will be batches of 32-dimensional vectors.
batch_shape Shape, including the batch size. For instance, shape = c(10,32) indicates that the expected input will be batches of 10 32-dimensional vectors.</description></item><item><title>Install Keras and the TensorFlow backend</title><link>/reference/keras/install_keras/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/install_keras/</guid><description>Keras and TensorFlow will be installed into an &#34;r-tensorflow&#34; virtual or conda environment. Note that &#34;virtualenv&#34; is not available on Windows (as this isn&#39;t supported by TensorFlow).
install_keras( method = c(&#34;auto&#34;, &#34;virtualenv&#34;, &#34;conda&#34;), conda = &#34;auto&#34;, version = &#34;default&#34;, tensorflow = &#34;default&#34;, extra_packages = c(&#34;tensorflow-hub&#34;), ... ) Arguments method Installation method (&#34;virtualenv&#34; or &#34;conda&#34;)
conda Path to conda executable (or &#34;auto&#34; to find conda using the PATH and other conventional install locations).</description></item><item><title>Instantiate an identity matrix and returns it.</title><link>/reference/keras/k_eye/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_eye/</guid><description>Instantiate an identity matrix and returns it.
k_eye(size, dtype = NULL, name = NULL) Arguments size Integer, number of rows/columns.
dtype String, data type of returned Keras variable.
name String, name of returned Keras variable.
Value A Keras variable, an identity matrix.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Instantiates a Keras function</title><link>/reference/keras/k_function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_function/</guid><description>Instantiates a Keras function
k_function(inputs, outputs, updates = NULL, ...) Arguments inputs List of placeholder tensors.
outputs List of output tensors.
updates List of update ops.
... Named arguments passed to tf$Session$run.
Value Output values as R arrays.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Instantiates a NASNet model.</title><link>/reference/keras/application_nasnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_nasnet/</guid><description>Note that only TensorFlow is supported for now, therefore it only works with the data format image_data_format=&#39;channels_last&#39; in your Keras config at ~/.keras/keras.json.
application_nasnet( input_shape = NULL, penultimate_filters = 4032L, num_blocks = 6L, stem_block_filters = 96L, skip_reduction = TRUE, filter_multiplier = 2L, include_top = TRUE, weights = NULL, input_tensor = NULL, pooling = NULL, classes = 1000, default_size = NULL ) application_nasnetlarge( input_shape = NULL, include_top = TRUE, weights = NULL, input_tensor = NULL, pooling = NULL, classes = 1000 ) application_nasnetmobile( input_shape = NULL, include_top = TRUE, weights = NULL, input_tensor = NULL, pooling = NULL, classes = 1000 ) nasnet_preprocess_input(x) Arguments input_shape Optional shape list, the input shape is by default (331, 331, 3) for NASNetLarge and (224, 224, 3) for NASNetMobile It should have exactly 3 inputs channels, and width and height should be no smaller than 32.</description></item><item><title>Instantiates a placeholder tensor and returns it.</title><link>/reference/keras/k_placeholder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_placeholder/</guid><description>Instantiates a placeholder tensor and returns it.
k_placeholder( shape = NULL, ndim = NULL, dtype = NULL, sparse = FALSE, name = NULL ) Arguments shape Shape of the placeholder (integer list, may include NULL entries).
ndim Number of axes of the tensor. At least one of shape, ndim must be specified. If both are specified, shape is used.
dtype Placeholder type.</description></item><item><title>Instantiates a variable and returns it.</title><link>/reference/keras/k_variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_variable/</guid><description>Instantiates a variable and returns it.
k_variable(value, dtype = NULL, name = NULL, constraint = NULL) Arguments value Numpy array, initial value of the tensor.
dtype Tensor type.
name Optional name string for the tensor.
constraint Optional projection function to be applied to the variable after an optimizer update.
Value A variable instance (with Keras metadata included).</description></item><item><title>Instantiates a variable with values drawn from a normal distribution.</title><link>/reference/keras/k_random_normal_variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_random_normal_variable/</guid><description>Instantiates a variable with values drawn from a normal distribution.
k_random_normal_variable( shape, mean, scale, dtype = NULL, name = NULL, seed = NULL ) Arguments shape Tuple of integers, shape of returned Keras variable.
mean Float, mean of the normal distribution.
scale Float, standard deviation of the normal distribution.
dtype String, dtype of returned Keras variable.
name String, name of returned Keras variable.</description></item><item><title>Instantiates a variable with values drawn from a uniform distribution.</title><link>/reference/keras/k_random_uniform_variable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_random_uniform_variable/</guid><description>Instantiates a variable with values drawn from a uniform distribution.
k_random_uniform_variable( shape, low, high, dtype = NULL, name = NULL, seed = NULL ) Arguments shape Tuple of integers, shape of returned Keras variable.
low Float, lower boundary of the output interval.
high Float, upper boundary of the output interval.
dtype String, dtype of returned Keras variable.</description></item><item><title>Instantiates an all-ones tensor variable and returns it.</title><link>/reference/keras/k_ones/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ones/</guid><description>Instantiates an all-ones tensor variable and returns it.
k_ones(shape, dtype = NULL, name = NULL) Arguments shape Tuple of integers, shape of returned Keras variable.
dtype String, data type of returned Keras variable.
name String, name of returned Keras variable.
Value A Keras variable, filled with 1.0.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Instantiates an all-ones variable of the same shape as another tensor.</title><link>/reference/keras/k_ones_like/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ones_like/</guid><description>Instantiates an all-ones variable of the same shape as another tensor.
k_ones_like(x, dtype = NULL, name = NULL) Arguments x Keras variable or tensor.
dtype String, dtype of returned Keras variable. NULL uses the dtype of x.
name String, name for the variable to create.
Value A Keras variable with the shape of x filled with ones.</description></item><item><title>Instantiates an all-zeros variable and returns it.</title><link>/reference/keras/k_zeros/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_zeros/</guid><description>Instantiates an all-zeros variable and returns it.
k_zeros(shape, dtype = NULL, name = NULL) Arguments shape Tuple of integers, shape of returned Keras variable
dtype String, data type of returned Keras variable
name String, name of returned Keras variable
Value A variable (including Keras metadata), filled with 0.0.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Instantiates an all-zeros variable of the same shape as another tensor.</title><link>/reference/keras/k_zeros_like/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_zeros_like/</guid><description>Instantiates an all-zeros variable of the same shape as another tensor.
k_zeros_like(x, dtype = NULL, name = NULL) Arguments x Keras variable or Keras tensor.
dtype String, dtype of returned Keras variable. NULL uses the dtype of x.
name String, name for the variable to create.
Value A Keras variable with the shape of x filled with zeros.</description></item><item><title>Instantiates the DenseNet architecture.</title><link>/reference/keras/application_densenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_densenet/</guid><description>Instantiates the DenseNet architecture.
application_densenet( blocks, include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) application_densenet121( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) application_densenet169( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) application_densenet201( include_top = TRUE, weights = &#34;</description></item><item><title>Iterates over the time dimension of a tensor</title><link>/reference/keras/k_rnn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_rnn/</guid><description>Iterates over the time dimension of a tensor
k_rnn( step_function, inputs, initial_states, go_backwards = FALSE, mask = NULL, constants = NULL, unroll = FALSE, input_length = NULL ) Arguments step_function RNN step function.
inputs Tensor with shape (samples, ...) (no time dimension), representing input for the batch of samples at a certain time step.
initial_states Tensor with shape (samples, output_dim) (no time dimension), containing the initial values for the states used in the step function.</description></item><item><title>Keras array object</title><link>/reference/keras/keras_array/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keras_array/</guid><description>Convert an R vector, matrix, or array object to an array that has the optimal in-memory layout and floating point data type for the current Keras backend.
keras_array(x, dtype = NULL) Arguments x Object or list of objects to convert
dtype NumPy data type (e.g. float32, float64). If this is unspecified then R doubles will be converted to the default floating point type for the current Keras backend.</description></item><item><title>Keras backend tensor engine</title><link>/reference/keras/backend/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/backend/</guid><description>Obtain a reference to the keras.backend Python module used to implement tensor operations.
backend(convert = TRUE) Arguments convert TRUE to automatically convert Python objects to their R equivalent. If you pass FALSE you can do manual conversion using the py_to_r() function.
Value Reference to Keras backend python module.
Note See the documentation here https://keras.io/backend/ for additional details on the available functions.</description></item><item><title>Keras implementation</title><link>/reference/keras/implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/implementation/</guid><description>Obtain a reference to the Python module used for the implementation of Keras.
implementation() Value Reference to the Python module used for the implementation of Keras.
Details There are currently two Python modules which implement Keras:
keras (&#34;keras&#34;)
tensorflow.keras (&#34;tensorflow&#34;)
This function returns a reference to the implementation being currently used by the keras package. The default implementation is &#34;keras&#34;. You can override this by setting the KERAS_IMPLEMENTATION environment variable to &#34;</description></item><item><title>Keras Model</title><link>/reference/keras/keras_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keras_model/</guid><description> A model is a directed acyclic graph of layers.
keras_model(inputs, outputs = NULL) Arguments inputs Input layer
outputs Output layer
See also Other model functions: compile.keras.engine.training.Model(), evaluate.keras.engine.training.Model(), evaluate_generator(), fit.keras.engine.training.Model(), fit_generator(), get_config(), get_layer(), keras_model_sequential(), multi_gpu_model(), pop_layer(), predict.keras.engine.training.Model(), predict_generator(), predict_on_batch(), predict_proba(), summary.keras.engine.training.Model(), train_on_batch()
Examples if (FALSE) { library(keras) # input layer inputs &amp;lt;- layer_input(shape = c(784)) # outputs compose input + dense layers predictions &amp;lt;- inputs %&amp;gt;% layer_dense(units = 64, activation = &#39;relu&#39;) %&amp;gt;% layer_dense(units = 64, activation = &#39;relu&#39;) %&amp;gt;% layer_dense(units = 10, activation = &#39;softmax&#39;) # create and compile model model &amp;lt;- keras_model(inputs = inputs, outputs = predictions) model %&amp;gt;% compile( optimizer = &#39;rmsprop&#39;, loss = &#39;categorical_crossentropy&#39;, metrics = c(&#39;accuracy&#39;) ) }</description></item><item><title>Keras Model composed of a linear stack of layers</title><link>/reference/keras/keras_model_sequential/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keras_model_sequential/</guid><description>Keras Model composed of a linear stack of layers
keras_model_sequential(layers = NULL, name = NULL) Arguments layers List of layers to add to the model
name Name of model
Note The first layer passed to a Sequential model should have a defined input shape. What that means is that it should have received an input_shape or batch_input_shape argument, or for some type of layers (recurrent, Dense.</description></item><item><title>L1 and L2 regularization</title><link>/reference/keras/regularizer_l1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/regularizer_l1/</guid><description> L1 and L2 regularization
regularizer_l1(l = 0.01) regularizer_l2(l = 0.01) regularizer_l1_l2(l1 = 0.01, l2 = 0.01) Arguments l Regularization factor.
l1 L1 regularization factor.
l2 L2 regularization factor.</description></item><item><title>Layer that adds a list of inputs.</title><link>/reference/keras/layer_add/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_add/</guid><description>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
layer_add( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer that applies an update to the cost function based input activity.</title><link>/reference/keras/layer_activity_regularization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activity_regularization/</guid><description>Layer that applies an update to the cost function based input activity.
layer_activity_regularization( object, l1 = 0, l2 = 0, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
l1 L1 regularization factor (positive float).
l2 L2 regularization factor (positive float).</description></item><item><title>Layer that averages a list of inputs.</title><link>/reference/keras/layer_average/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_average/</guid><description>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
layer_average( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer that computes a dot product between samples in two tensors.</title><link>/reference/keras/layer_dot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_dot/</guid><description>Layer that computes a dot product between samples in two tensors.
layer_dot( inputs, axes, normalize = FALSE, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
axes Integer or list of integers, axis or axes along which to take the dot product.
normalize Whether to L2-normalize samples along the dot product axis before taking the dot product.</description></item><item><title>Layer that computes the maximum (element-wise) a list of inputs.</title><link>/reference/keras/layer_maximum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_maximum/</guid><description>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
layer_maximum( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer that computes the minimum (element-wise) a list of inputs.</title><link>/reference/keras/layer_minimum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_minimum/</guid><description>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
layer_minimum( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer that concatenates a list of inputs.</title><link>/reference/keras/layer_concatenate/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_concatenate/</guid><description>It takes as input a list of tensors, all of the same shape expect for the concatenation axis, and returns a single tensor, the concatenation of all inputs.
layer_concatenate( inputs, axis = -1, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
axis Concatenation axis.
batch_size Fixed batch size for layer</description></item><item><title>Layer that multiplies (element-wise) a list of inputs.</title><link>/reference/keras/layer_multiply/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_multiply/</guid><description>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).
layer_multiply( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (at least 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer that subtracts two inputs.</title><link>/reference/keras/layer_subtract/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_subtract/</guid><description>It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[[1]] - inputs[[2]]), also of the same shape.
layer_subtract( inputs, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments inputs A list of input tensors (exactly 2).
batch_size Fixed batch size for layer
dtype The data type expected by the input, as a string (float32, float64, int32.</description></item><item><title>Layer/Model configuration</title><link>/reference/keras/get_config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_config/</guid><description>A layer config is an object returned from get_config() that contains the configuration of a layer or model. The same layer or model can be reinstantiated later (without its trained weights) from this configuration using from_config(). The config does not include connectivity information, nor the class name (those are handled externally).
get_config(object) from_config(config) Arguments object Layer or model object
config Object with layer or model configuration</description></item><item><title>Layer/Model weights as R arrays</title><link>/reference/keras/get_weights/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_weights/</guid><description> Layer/Model weights as R arrays
get_weights(object) set_weights(object, weights) Arguments object Layer or model object
weights Weights as R array
See also Other model persistence: model_to_json(), model_to_yaml(), save_model_hdf5(), save_model_tf(), save_model_weights_hdf5(), serialize_model()
Other layer methods: count_params(), get_config(), get_input_at(), reset_states()</description></item><item><title>Leaky version of a Rectified Linear Unit.</title><link>/reference/keras/layer_activation_leaky_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_leaky_relu/</guid><description>Allows a small gradient when the unit is not active: f(x) = alpha * x for x &amp;lt; 0, f(x) = x for x &amp;gt;= 0.
layer_activation_leaky_relu( object, alpha = 0.3, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
alpha float &amp;gt;= 0. Negative slope coefficient.</description></item><item><title>Learning rate scheduler.</title><link>/reference/keras/callback_learning_rate_scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_learning_rate_scheduler/</guid><description> Learning rate scheduler.
callback_learning_rate_scheduler(schedule) Arguments schedule a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float).
See also Other callbacks: callback_csv_logger(), callback_early_stopping(), callback_lambda(), callback_model_checkpoint(), callback_progbar_logger(), callback_reduce_lr_on_plateau(), callback_remote_monitor(), callback_tensorboard(), callback_terminate_on_naan()</description></item><item><title>LeCun normal initializer.</title><link>/reference/keras/initializer_lecun_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_lecun_normal/</guid><description>It draws samples from a truncated normal distribution centered on 0 with stddev &amp;lt;- sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor..
initializer_lecun_normal(seed = NULL) Arguments seed A Python integer. Used to seed the random generator.
References Self-Normalizing Neural Networks
Efficient Backprop, LeCun, Yann et al. 1998
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_uniform(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>LeCun uniform initializer.</title><link>/reference/keras/initializer_lecun_uniform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/initializer_lecun_uniform/</guid><description>It draws samples from a uniform distribution within -limit, limit where limit is sqrt(3 / fan_in) where fan_in is the number of input units in the weight tensor.
initializer_lecun_uniform(seed = NULL) Arguments seed Integer used to seed the random generator.
References LeCun 98, Efficient Backprop,
See also Other initializers: initializer_constant(), initializer_glorot_normal(), initializer_glorot_uniform(), initializer_he_normal(), initializer_he_uniform(), initializer_identity(), initializer_lecun_normal(), initializer_ones(), initializer_orthogonal(), initializer_random_normal(), initializer_random_uniform(), initializer_truncated_normal(), initializer_variance_scaling(), initializer_zeros()</description></item><item><title>Load a Keras model from the Saved Model format</title><link>/reference/keras/model_from_saved_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/model_from_saved_model/</guid><description>Load a Keras model from the Saved Model format
model_from_saved_model(saved_model_path, custom_objects = NULL) Arguments saved_model_path a string specifying the path to the SavedModel directory.
custom_objects Optional dictionary mapping string names to custom classes or functions (e.g. custom loss functions).
Value a Keras model.
Note This functionality is experimental and only works with TensorFlow version &amp;gt;= &#34;2.0&#34;.
See also Other saved_model: model_to_saved_model()</description></item><item><title>Loads an image into PIL format.</title><link>/reference/keras/image_load/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/image_load/</guid><description>Loads an image into PIL format.
image_load( path, grayscale = FALSE, target_size = NULL, interpolation = &#34;nearest&#34; ) Arguments path Path to image file
grayscale Boolean, whether to load the image as grayscale.
target_size Either NULL (default to original size) or integer vector (img_height, img_width).
interpolation Interpolation method used to resample the image if the target size is different from that of the loaded image.</description></item><item><title>Locally-connected layer for 1D inputs.</title><link>/reference/keras/layer_locally_connected_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_locally_connected_1d/</guid><description>layer_locally_connected_1d() works similarly to layer_conv_1d() , except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.
layer_locally_connected_1d( object, filters, kernel_size, strides = 1L, padding = &#34;valid&#34;, data_format = NULL, activation = NULL, use_bias = TRUE, kernel_initializer = &#34;glorot_uniform&#34;, bias_initializer = &#34;zeros&#34;, kernel_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, bias_constraint = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Locally-connected layer for 2D inputs.</title><link>/reference/keras/layer_locally_connected_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_locally_connected_2d/</guid><description>layer_locally_connected_2d works similarly to layer_conv_2d(), except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.
layer_locally_connected_2d( object, filters, kernel_size, strides = c(1L, 1L), padding = &#34;valid&#34;, data_format = NULL, activation = NULL, use_bias = TRUE, kernel_initializer = &#34;glorot_uniform&#34;, bias_initializer = &#34;zeros&#34;, kernel_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, bias_constraint = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Long Short-Term Memory unit - Hochreiter 1997.</title><link>/reference/keras/layer_lstm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_lstm/</guid><description>For a step-by-step description of the algorithm, see this tutorial.
layer_lstm( object, units, activation = &#34;tanh&#34;, recurrent_activation = &#34;hard_sigmoid&#34;, use_bias = TRUE, return_sequences = FALSE, return_state = FALSE, go_backwards = FALSE, stateful = FALSE, unroll = FALSE, kernel_initializer = &#34;glorot_uniform&#34;, recurrent_initializer = &#34;orthogonal&#34;, bias_initializer = &#34;zeros&#34;, unit_forget_bias = TRUE, kernel_regularizer = NULL, recurrent_regularizer = NULL, bias_regularizer = NULL, activity_regularizer = NULL, kernel_constraint = NULL, recurrent_constraint = NULL, bias_constraint = NULL, dropout = 0, recurrent_dropout = 0, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Map the function fn over the elements elems and return the outputs.</title><link>/reference/keras/k_map_fn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_map_fn/</guid><description>Map the function fn over the elements elems and return the outputs.
k_map_fn(fn, elems, name = NULL, dtype = NULL) Arguments fn Function that will be called upon each element in elems
elems tensor
name A string name for the map node in the graph
dtype Output data type.
Value Tensor with dtype dtype.</description></item><item><title>Masks a sequence by using a mask value to skip timesteps.</title><link>/reference/keras/layer_masking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_masking/</guid><description>For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking). If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.
layer_masking( object, mask_value = 0, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Max pooling operation for 3D data (spatial or spatio-temporal).</title><link>/reference/keras/layer_max_pooling_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_max_pooling_3d/</guid><description>Max pooling operation for 3D data (spatial or spatio-temporal).
layer_max_pooling_3d( object, pool_size = c(2L, 2L, 2L), strides = NULL, padding = &#34;valid&#34;, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size list of 3 integers, factors by which to downscale (dim1, dim2, dim3). (2, 2, 2) will halve the size of the 3D input in each dimension.</description></item><item><title>Max pooling operation for spatial data.</title><link>/reference/keras/layer_max_pooling_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_max_pooling_2d/</guid><description>Max pooling operation for spatial data.
layer_max_pooling_2d( object, pool_size = c(2L, 2L), strides = NULL, padding = &#34;valid&#34;, data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size integer or list of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension.</description></item><item><title>Max pooling operation for temporal data.</title><link>/reference/keras/layer_max_pooling_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_max_pooling_1d/</guid><description>Max pooling operation for temporal data.
layer_max_pooling_1d( object, pool_size = 2L, strides = NULL, padding = &#34;valid&#34;, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
pool_size Integer, size of the max pooling windows.
strides Integer, or NULL. Factor by which to downscale. E.g. 2 will halve the input. If NULL, it will default to pool_size.</description></item><item><title>Maximum value in a tensor.</title><link>/reference/keras/k_max/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_max/</guid><description>Maximum value in a tensor.
k_max(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, the axis to find maximum values (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>Mean of a tensor, alongside the specified axis.</title><link>/reference/keras/k_mean/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_mean/</guid><description>Mean of a tensor, alongside the specified axis.
k_mean(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis A list of axes to compute the mean over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1 for each entry in axis.</description></item><item><title>Minimum value in a tensor.</title><link>/reference/keras/k_min/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_min/</guid><description>Minimum value in a tensor.
k_min(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, axis to find minimum values (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>MNIST database of handwritten digits</title><link>/reference/keras/dataset_mnist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_mnist/</guid><description>Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.
dataset_mnist(path = &#34;mnist.npz&#34;) Arguments path Path where to cache the dataset locally (relative to ~/.keras/datasets).
Value Lists of training and test data: train$x, train$y, test$x, test$y, where x is an array of grayscale image data with shape (num_samples, 28, 28) and y is an array of digit labels (integers in range 0-9) with shape (num_samples).</description></item><item><title>MobileNet model architecture.</title><link>/reference/keras/application_mobilenet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_mobilenet/</guid><description>MobileNet model architecture.
application_mobilenet( input_shape = NULL, alpha = 1, depth_multiplier = 1, dropout = 0.001, include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, pooling = NULL, classes = 1000 ) mobilenet_preprocess_input(x) mobilenet_decode_predictions(preds, top = 5) mobilenet_load_model_hdf5(filepath) Arguments input_shape optional shape list, only to be specified if include_top is FALSE (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format).</description></item><item><title>MobileNetV2 model architecture</title><link>/reference/keras/application_mobilenet_v2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_mobilenet_v2/</guid><description>MobileNetV2 model architecture
application_mobilenet_v2( input_shape = NULL, alpha = 1, include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, pooling = NULL, classes = 1000 ) mobilenet_v2_preprocess_input(x) mobilenet_v2_decode_predictions(preds, top = 5) mobilenet_v2_load_model_hdf5(filepath) Arguments input_shape optional shape list, only to be specified if include_top is FALSE (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format).</description></item><item><title>Model configuration as JSON</title><link>/reference/keras/model_to_json/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/model_to_json/</guid><description>Save and re-load models configurations as JSON. Note that the representation does not include the weights, only the architecture.
model_to_json(object) model_from_json(json, custom_objects = NULL) Arguments object Model object to save
json JSON with model configuration
custom_objects Optional named list mapping names to custom classes or functions to be considered during deserialization.
See also Other model persistence: get_weights(), model_to_yaml(), save_model_hdf5(), save_model_tf(), save_model_weights_hdf5(), serialize_model()</description></item><item><title>Model configuration as YAML</title><link>/reference/keras/model_to_yaml/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/model_to_yaml/</guid><description>Save and re-load models configurations as YAML Note that the representation does not include the weights, only the architecture.
model_to_yaml(object) model_from_yaml(yaml, custom_objects = NULL) Arguments object Model object to save
yaml YAML with model configuration
custom_objects Optional named list mapping names to custom classes or functions to be considered during deserialization.
See also Other model persistence: get_weights(), model_to_json(), save_model_hdf5(), save_model_tf(), save_model_weights_hdf5(), serialize_model()</description></item><item><title>Model loss functions</title><link>/reference/keras/loss_mean_squared_error/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/loss_mean_squared_error/</guid><description>Model loss functions
loss_mean_squared_error(y_true, y_pred) loss_mean_absolute_error(y_true, y_pred) loss_mean_absolute_percentage_error(y_true, y_pred) loss_mean_squared_logarithmic_error(y_true, y_pred) loss_squared_hinge(y_true, y_pred) loss_hinge(y_true, y_pred) loss_categorical_hinge(y_true, y_pred) loss_logcosh(y_true, y_pred) loss_categorical_crossentropy(y_true, y_pred) loss_sparse_categorical_crossentropy(y_true, y_pred) loss_binary_crossentropy(y_true, y_pred) loss_kullback_leibler_divergence(y_true, y_pred) loss_poisson(y_true, y_pred) loss_cosine_proximity(y_true, y_pred) loss_cosine_similarity(y_true, y_pred) Arguments y_true True labels (Tensor)
y_pred Predictions (Tensor of the same shape as y_true)
Details Loss functions are to be supplied in the loss parameter of the compile.</description></item><item><title>Model performance metrics</title><link>/reference/keras/metric_binary_accuracy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/metric_binary_accuracy/</guid><description>Model performance metrics
metric_binary_accuracy(y_true, y_pred) metric_binary_crossentropy(y_true, y_pred) metric_categorical_accuracy(y_true, y_pred) metric_categorical_crossentropy(y_true, y_pred) metric_cosine_proximity(y_true, y_pred) metric_hinge(y_true, y_pred) metric_kullback_leibler_divergence(y_true, y_pred) metric_mean_absolute_error(y_true, y_pred) metric_mean_absolute_percentage_error(y_true, y_pred) metric_mean_squared_error(y_true, y_pred) metric_mean_squared_logarithmic_error(y_true, y_pred) metric_poisson(y_true, y_pred) metric_sparse_categorical_crossentropy(y_true, y_pred) metric_squared_hinge(y_true, y_pred) metric_top_k_categorical_accuracy(y_true, y_pred, k = 5) metric_sparse_top_k_categorical_accuracy(y_true, y_pred, k = 5) custom_metric(name, metric_fn) Arguments y_true True labels (tensor)
y_pred Predictions (tensor of the same shape as y_true).
k An integer, number of top elements to consider.</description></item><item><title>Multiplies 2 tensors (and/or variables) and returns a &lt;em&gt;tensor&lt;/em&gt;.</title><link>/reference/keras/k_dot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_dot/</guid><description>When attempting to multiply a nD tensor with a nD tensor, it reproduces the Theano behavior. (e.g. (2, 3) * (4, 3, 5) -&amp;gt; (2, 4, 5))
k_dot(x, y) Arguments x Tensor or variable.
y Tensor or variable.
Value A tensor, dot product of x and y.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Multiplies the values in a tensor, alongside the specified axis.</title><link>/reference/keras/k_prod/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_prod/</guid><description>Multiplies the values in a tensor, alongside the specified axis.
k_prod(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, axis to compute the product over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>Nesterov Adam optimizer</title><link>/reference/keras/optimizer_nadam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_nadam/</guid><description>Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop with Nesterov momentum.
optimizer_nadam( lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = NULL, schedule_decay = 0.004, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
beta_1 The exponential decay rate for the 1st moment estimates. float, 0 &amp;lt; beta &amp;lt; 1. Generally close to 1.</description></item><item><title>Normalize a matrix or nd-array</title><link>/reference/keras/normalize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/normalize/</guid><description>Normalize a matrix or nd-array
normalize(x, axis = -1, order = 2) Arguments x Matrix or array to normalize
axis Axis along which to normalize. Axis indexes are 1-based (pass -1 to select the last axis).
order Normalization order (e.g. 2 for L2 norm)
Value A normalized copy of the array.</description></item><item><title>Normalizes a tensor wrt the L2 norm alongside the specified axis.</title><link>/reference/keras/k_l2_normalize/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_l2_normalize/</guid><description>Normalizes a tensor wrt the L2 norm alongside the specified axis.
k_l2_normalize(x, axis = NULL) Arguments x Tensor or variable.
axis Axis along which to perform normalization (axis indexes are 1-based)
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Objects exported from other packages</title><link>/reference/keras/reexports/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/reexports/</guid><description> These objects are imported from other packages. Follow the links below to see their documentation.
genericscompile, fit
reticulatearray_reshape, tuple, use_condaenv, use_python, use_virtualenv
tensorflowevaluate, export_savedmodel, shape, tensorboard, use_session_with_seed
tfrunsflag_boolean, flag_integer, flag_numeric, flag_string, flags, run_dir</description></item><item><title>One-hot encode a text into a list of word indexes in a vocabulary of size n.</title><link>/reference/keras/text_one_hot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/text_one_hot/</guid><description>One-hot encode a text into a list of word indexes in a vocabulary of size n.
text_one_hot( text, n, filters = &#34;!\&#34;#$%&amp;amp;()*+,-./:;&amp;lt;=&amp;gt;?@[\\]^_`{|}~\t\n&#34;, lower = TRUE, split = &#34; &#34; ) Arguments text Input text (string).
n Size of vocabulary (integer)
filters Sequence of characters to filter out such as punctuation. Default includes basic punctuation, tabs, and newlines.
lower Whether to convert the input to lowercase.</description></item><item><title>Pads 5D tensor with zeros along the depth, height, width dimensions.</title><link>/reference/keras/k_spatial_3d_padding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_spatial_3d_padding/</guid><description>Pads these dimensions with respectively padding[[1]], padding[[2]], and padding[[3]] zeros left and right. For &#39;channels_last&#39; data_format, the 2nd, 3rd and 4th dimension will be padded. For &#39;channels_first&#39; data_format, the 3rd, 4th and 5th dimension will be padded.
k_spatial_3d_padding( x, padding = list(list(1, 1), list(1, 1), list(1, 1)), data_format = NULL ) Arguments x Tensor or variable.
padding List of 3 lists, padding pattern.</description></item><item><title>Pads sequences to the same length</title><link>/reference/keras/pad_sequences/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/pad_sequences/</guid><description>Pads sequences to the same length
pad_sequences( sequences, maxlen = NULL, dtype = &#34;int32&#34;, padding = &#34;pre&#34;, truncating = &#34;pre&#34;, value = 0 ) Arguments sequences List of lists where each element is a sequence
maxlen int, maximum length of all sequences
dtype type of the output sequences
padding &#39;pre&#39; or &#39;post&#39;, pad either before or after each sequence.</description></item><item><title>Pads the 2nd and 3rd dimensions of a 4D tensor.</title><link>/reference/keras/k_spatial_2d_padding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_spatial_2d_padding/</guid><description>Pads the 2nd and 3rd dimensions of a 4D tensor.
k_spatial_2d_padding( x, padding = list(list(1, 1), list(1, 1)), data_format = NULL ) Arguments x Tensor or variable.
padding Tuple of 2 lists, padding pattern.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
Value A padded 4D tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Pads the middle dimension of a 3D tensor.</title><link>/reference/keras/k_temporal_padding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_temporal_padding/</guid><description>Pads the middle dimension of a 3D tensor.
k_temporal_padding(x, padding = c(1, 1)) Arguments x Tensor or variable.
padding List of 2 integers, how many zeros to add at the start and end of dim 1.
Value A padded 3D tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Parametric Rectified Linear Unit.</title><link>/reference/keras/layer_activation_parametric_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_parametric_relu/</guid><description>It follows: f(x) = alpha * x`` for x &amp;lt; 0, f(x) = xforx &amp;gt;= 0`, where alpha is a learned array with the same shape as x.
layer_activation_parametric_relu( object, alpha_initializer = &#34;zeros&#34;, alpha_regularizer = NULL, alpha_constraint = NULL, shared_axes = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object</description></item><item><title>Permute the dimensions of an input according to a given pattern</title><link>/reference/keras/layer_permute/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_permute/</guid><description>Permute the dimensions of an input according to a given pattern
layer_permute( object, dims, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
dims List of integers. Permutation pattern, does not include the samples dimension. Indexing starts at 1. For instance, (2, 1) permutes the first and second dimension of the input.</description></item><item><title>Permutes axes in a tensor.</title><link>/reference/keras/k_permute_dimensions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_permute_dimensions/</guid><description>Permutes axes in a tensor.
k_permute_dimensions(x, pattern) Arguments x Tensor or variable.
pattern A list of dimension indices, e.g. (1, 3, 2). Dimension indices are 1-based.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.</description></item><item><title>Pipe operator</title><link>/reference/keras/pipe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/pipe/</guid><description> See %&amp;gt;% for more details.
lhs %&amp;gt;% rhs</description></item><item><title>Plot training history</title><link>/reference/keras/plot.keras_training_history/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/plot.keras_training_history/</guid><description>Plots metrics recorded during training.
# S3 method for keras_training_history plot( x, y, metrics = NULL, method = c(&#34;auto&#34;, &#34;ggplot2&#34;, &#34;base&#34;), smooth = getOption(&#34;keras.plot.history.smooth&#34;, TRUE), theme_bw = getOption(&#34;keras.plot.history.theme_bw&#34;, FALSE), ... ) Arguments x Training history object returned from fit.keras.engine.training.Model().
y Unused.
metrics One or more metrics to plot (e.g. c(&#39;loss&#39;, &#39;accuracy&#39;)). Defaults to plotting all captured metrics.
method Method to use for plotting.</description></item><item><title>Preprocesses a tensor or array encoding a batch of images.</title><link>/reference/keras/imagenet_preprocess_input/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/imagenet_preprocess_input/</guid><description>Preprocesses a tensor or array encoding a batch of images.
imagenet_preprocess_input(x, data_format = NULL, mode = &#34;caffe&#34;) Arguments x Input Numpy or symbolic tensor, 3D or 4D.
data_format Data format of the image tensor/array.
mode One of &#34;caffe&#34;, &#34;tf&#34;, or &#34;torch&#34;
caffe: will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.</description></item><item><title>Print a summary of a Keras model</title><link>/reference/keras/summary.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/summary.keras.engine.training.model/</guid><description>Print a summary of a Keras model
# S3 method for keras.engine.training.Model summary(object, line_length = getOption(&#34;width&#34;), positions = NULL, ...) Arguments object Keras model instance
line_length Total length of printed lines
positions Relative or absolute positions of log elements in each line. If not provided, defaults to c(0.33, 0.55, 0.67, 1.0).
... Unused
See also Other model functions: compile.</description></item><item><title>Prints &lt;code&gt;message&lt;/code&gt; and the tensor value when evaluated.</title><link>/reference/keras/k_print_tensor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_print_tensor/</guid><description>Note that print_tensor returns a new tensor identical to x which should be used in the following code. Otherwise the print operation is not taken into account during evaluation.
k_print_tensor(x, message = &#34;&#34;) Arguments x Tensor to print.
message Message to print jointly with the tensor.
Value The same tensor x, unchanged.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Provide a scope with mappings of names to custom objects</title><link>/reference/keras/with_custom_object_scope/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/with_custom_object_scope/</guid><description>Provide a scope with mappings of names to custom objects
with_custom_object_scope(objects, expr) Arguments objects Named list of objects
expr Expression to evaluate
Details There are many elements of Keras models that can be customized with user objects (e.g. losses, metrics, regularizers, etc.). When loading saved models that use these functions you typically need to explicitily map names to user objects via the custom_objects parmaeter.</description></item><item><title>R interface to Keras</title><link>/reference/keras/keras-package/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/keras-package/</guid><description>Keras is a high-level neural networks API, developed with a focus on enabling fast experimentation. Keras has the following key features:
Details Allows the same code to run on CPU or on GPU, seamlessly.
User-friendly API which makes it easy to quickly prototype deep learning models.
Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.
Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc.</description></item><item><title>Rectified Linear Unit activation function</title><link>/reference/keras/layer_activation_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_relu/</guid><description>Rectified Linear Unit activation function
layer_activation_relu( object, max_value = NULL, negative_slope = 0, threshold = 0, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
max_value loat, the maximum output value.
negative_slope float &amp;gt;= 0 Negative slope coefficient.
threshold float.</description></item><item><title>Rectified linear unit.</title><link>/reference/keras/k_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_relu/</guid><description>With default values, it returns element-wise max(x, 0).
k_relu(x, alpha = 0, max_value = NULL) Arguments x A tensor or variable.
alpha A scalar, slope of negative section (default=0.).
max_value Saturation threshold.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Reduce elems using fn to combine them from left to right.</title><link>/reference/keras/k_foldl/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_foldl/</guid><description>Reduce elems using fn to combine them from left to right.
k_foldl(fn, elems, initializer = NULL, name = NULL) Arguments fn Function that will be called upon each element in elems and an accumulator
elems tensor
initializer The first value used (first element of elems in case of `NULL``)
name A string name for the foldl node in the graph</description></item><item><title>Reduce elems using fn to combine them from right to left.</title><link>/reference/keras/k_foldr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_foldr/</guid><description>Reduce elems using fn to combine them from right to left.
k_foldr(fn, elems, initializer = NULL, name = NULL) Arguments fn Function that will be called upon each element in elems and an accumulator
elems tensor
initializer The first value used (last element of elems in case of NULL)
name A string name for the foldr node in the graph</description></item><item><title>Reduce learning rate when a metric has stopped improving.</title><link>/reference/keras/callback_reduce_lr_on_plateau/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_reduce_lr_on_plateau/</guid><description>Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a &#39;patience&#39; number of epochs, the learning rate is reduced.
callback_reduce_lr_on_plateau( monitor = &#34;val_loss&#34;, factor = 0.1, patience = 10, verbose = 0, mode = c(&#34;auto&#34;, &#34;min&#34;, &#34;max&#34;), min_delta = 1e-04, cooldown = 0, min_lr = 0 ) Arguments monitor quantity to be monitored.</description></item><item><title>Remove the last layer in a model</title><link>/reference/keras/pop_layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/pop_layer/</guid><description> Remove the last layer in a model
pop_layer(object) Arguments object Keras model object
See also Other model functions: compile.keras.engine.training.Model(), evaluate.keras.engine.training.Model(), evaluate_generator(), fit.keras.engine.training.Model(), fit_generator(), get_config(), get_layer(), keras_model_sequential(), keras_model(), multi_gpu_model(), predict.keras.engine.training.Model(), predict_generator(), predict_on_batch(), predict_proba(), summary.keras.engine.training.Model(), train_on_batch()</description></item><item><title>Removes a 1-dimension from the tensor at index &lt;code&gt;axis&lt;/code&gt;.</title><link>/reference/keras/k_squeeze/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_squeeze/</guid><description>Removes a 1-dimension from the tensor at index axis.
k_squeeze(x, axis) Arguments x A tensor or variable.
axis Axis to drop (axis indexes are 1-based).
Value A tensor with the same data as x but reduced dimensions.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Repeats a 2D tensor.</title><link>/reference/keras/k_repeat/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_repeat/</guid><description>If x has shape (samples, dim) and n is 2, the output will have shape (samples, 2, dim).
k_repeat(x, n) Arguments x Tensor or variable.
n Integer, number of times to repeat.
Value A tensor
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Repeats the elements of a tensor along an axis.</title><link>/reference/keras/k_repeat_elements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_repeat_elements/</guid><description>If x has shape (s1, s2, s3) and axis is 2, the output will have shape (s1, s2 * rep, s3).
k_repeat_elements(x, rep, axis) Arguments x Tensor or variable.
rep Integer, number of times to repeat.
axis Axis along which to repeat (axis indexes are 1-based)
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Repeats the input n times.</title><link>/reference/keras/layer_repeat_vector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_repeat_vector/</guid><description>Repeats the input n times.
layer_repeat_vector( object, n, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
n integer, repetition factor.
batch_size Fixed batch size for layer
name An optional name string for the layer. Should be unique in a model (do not reuse the same name twice).</description></item><item><title>Replicates a model on different GPUs.</title><link>/reference/keras/multi_gpu_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/multi_gpu_model/</guid><description>Replicates a model on different GPUs.
multi_gpu_model(model, gpus = NULL, cpu_merge = TRUE, cpu_relocation = FALSE) Arguments model A Keras model instance. To avoid OOM errors, this model could have been built on CPU, for instance (see usage example below).
gpus NULL to use all available GPUs (default). Integer &amp;gt;= 2 or list of integers, number of GPUs or list of GPU IDs on which to create model replicas.</description></item><item><title>Representation of HDF5 dataset to be used instead of an R array</title><link>/reference/keras/hdf5_matrix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/hdf5_matrix/</guid><description>Representation of HDF5 dataset to be used instead of an R array
hdf5_matrix(datapath, dataset, start = 0, end = NULL, normalizer = NULL) Arguments datapath string, path to a HDF5 file
dataset string, name of the HDF5 dataset in the file specified in datapath
start int, start of desired slice of the specified dataset
end int, end of desired slice of the specified dataset</description></item><item><title>Reset graph identifiers.</title><link>/reference/keras/k_reset_uids/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_reset_uids/</guid><description>Reset graph identifiers.
k_reset_uids() Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Reset the states for a layer</title><link>/reference/keras/reset_states/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/reset_states/</guid><description> Reset the states for a layer
reset_states(object) Arguments object Model or layer object
See also Other layer methods: count_params(), get_config(), get_input_at(), get_weights()</description></item><item><title>Reshapes a tensor to the specified shape.</title><link>/reference/keras/k_reshape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_reshape/</guid><description>Reshapes a tensor to the specified shape.
k_reshape(x, shape) Arguments x Tensor or variable.
shape Target shape list.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Reshapes an output to a certain shape.</title><link>/reference/keras/layer_reshape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_reshape/</guid><description>Reshapes an output to a certain shape.
layer_reshape( object, target_shape, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
target_shape List of integers, does not include the samples dimension (batch size).
input_shape Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.</description></item><item><title>Resizes the images contained in a 4D tensor.</title><link>/reference/keras/k_resize_images/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_resize_images/</guid><description>Resizes the images contained in a 4D tensor.
k_resize_images(x, height_factor, width_factor, data_format) Arguments x Tensor or variable to resize.
height_factor Positive integer.
width_factor Positive integer.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Resizes the volume contained in a 5D tensor.</title><link>/reference/keras/k_resize_volumes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_resize_volumes/</guid><description>Resizes the volume contained in a 5D tensor.
k_resize_volumes(x, depth_factor, height_factor, width_factor, data_format) Arguments x Tensor or variable to resize.
depth_factor Positive integer.
height_factor Positive integer.
width_factor Positive integer.
data_format string, &#34;channels_last&#34; or &#34;channels_first&#34;.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>ResNet50 model for Keras.</title><link>/reference/keras/application_resnet50/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_resnet50/</guid><description>ResNet50 model for Keras.
application_resnet50( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) Arguments include_top whether to include the fully-connected layer at the top of the network.
weights NULL (random initialization), imagenet (ImageNet weights), or the path to the weights file to be loaded.
input_tensor optional Keras tensor to use as image input for the model.</description></item><item><title>Retrieve tensors for layers with multiple nodes</title><link>/reference/keras/get_input_at/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_input_at/</guid><description>Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a &#34;node&#34; to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 1, 2, 3. These functions enable you to retrieve various tensor properties of layers with multiple nodes.</description></item><item><title>Retrieve the next item from a generator</title><link>/reference/keras/generator_next/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/generator_next/</guid><description> Use to retrieve items from generators (e.g. image_data_generator()). Will return either the next item or NULL if there are no more items.
generator_next(generator, completed = NULL) Arguments generator Generator
completed Sentinel value to return from generator_next() if the iteration completes (defaults to NULL but can be any R value you specify).</description></item><item><title>Retrieves a layer based on either its name (unique) or index.</title><link>/reference/keras/get_layer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/get_layer/</guid><description>Indices are based on order of horizontal graph traversal (bottom-up) and are 1-based. If name and index are both provided, index will take precedence.
get_layer(object, name = NULL, index = NULL) Arguments object Keras model object
name String, name of layer.
index Integer, index of layer (0-based)
Value A layer instance.
See also Other model functions: compile.keras.engine.training.Model(), evaluate.</description></item><item><title>Retrieves the elements of indices &lt;code&gt;indices&lt;/code&gt; in the tensor &lt;code&gt;reference&lt;/code&gt;.</title><link>/reference/keras/k_gather/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_gather/</guid><description>Retrieves the elements of indices indices in the tensor reference.
k_gather(reference, indices) Arguments reference A tensor.
indices Indices. Dimension indices are 1-based. Note however that if you pass a tensor for indices they will be passed as-is, in which case indices will be 0 based because no normalizing of R 1-based axes to Python 0-based axes is performed.
Value A tensor of same type as reference.</description></item><item><title>Returns &lt;code&gt;variables&lt;/code&gt; but with zero gradient w.r.t. every other variable.</title><link>/reference/keras/k_stop_gradient/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_stop_gradient/</guid><description>Returns variables but with zero gradient w.r.t. every other variable.
k_stop_gradient(variables) Arguments variables tensor or list of tensors to consider constant with respect to any other variable.
Value A single tensor or a list of tensors (depending on the passed argument) that has constant gradient with respect to any other variable.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Returns a tensor with normal distribution of values.</title><link>/reference/keras/k_random_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_random_normal/</guid><description>Returns a tensor with normal distribution of values.
k_random_normal(shape, mean = 0, stddev = 1, dtype = NULL, seed = NULL) Arguments shape A list of integers, the shape of tensor to create.
mean A float, mean of the normal distribution to draw samples.
stddev A float, standard deviation of the normal distribution to draw samples.
dtype String, dtype of returned tensor.</description></item><item><title>Returns a tensor with random binomial distribution of values.</title><link>/reference/keras/k_random_binomial/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_random_binomial/</guid><description>Returns a tensor with random binomial distribution of values.
k_random_binomial(shape, p = 0, dtype = NULL, seed = NULL) Arguments shape A list of integers, the shape of tensor to create.
p A float, 0. &amp;lt;= p &amp;lt;= 1, probability of binomial distribution.
dtype String, dtype of returned tensor.
seed Integer, random seed.
Value A tensor.</description></item><item><title>Returns a tensor with the same content as the input tensor.</title><link>/reference/keras/k_identity/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_identity/</guid><description>Returns a tensor with the same content as the input tensor.
k_identity(x, name = NULL) Arguments x The input tensor.
name String, name for the variable to create.
Value A tensor of the same shape, type and content.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Returns a tensor with truncated random normal distribution of values.</title><link>/reference/keras/k_truncated_normal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_truncated_normal/</guid><description>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is more than two standard deviations from the mean are dropped and re-picked.
k_truncated_normal(shape, mean = 0, stddev = 1, dtype = NULL, seed = NULL) Arguments shape A list of integers, the shape of tensor to create.
mean Mean of the values.
stddev Standard deviation of the values.</description></item><item><title>Returns a tensor with uniform distribution of values.</title><link>/reference/keras/k_random_uniform/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_random_uniform/</guid><description>Returns a tensor with uniform distribution of values.
k_random_uniform(shape, minval = 0, maxval = 1, dtype = NULL, seed = NULL) Arguments shape A list of integers, the shape of tensor to create.
minval A float, lower boundary of the uniform distribution to draw samples.
maxval A float, upper boundary of the uniform distribution to draw samples.
dtype String, dtype of returned tensor.</description></item><item><title>Returns predictions for a single batch of samples.</title><link>/reference/keras/predict_on_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/predict_on_batch/</guid><description> Returns predictions for a single batch of samples.
predict_on_batch(object, x) Arguments object Keras model object
x Input data (vector, matrix, or array)
Value array of predictions.
See also Other model functions: compile.keras.engine.training.Model(), evaluate.keras.engine.training.Model(), evaluate_generator(), fit.keras.engine.training.Model(), fit_generator(), get_config(), get_layer(), keras_model_sequential(), keras_model(), multi_gpu_model(), pop_layer(), predict.keras.engine.training.Model(), predict_generator(), predict_proba(), summary.keras.engine.training.Model(), train_on_batch()</description></item><item><title>Returns the dtype of a Keras tensor or variable, as a string.</title><link>/reference/keras/k_dtype/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_dtype/</guid><description>Returns the dtype of a Keras tensor or variable, as a string.
k_dtype(x) Arguments x Tensor or variable.
Value String, dtype of x.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns the gradients of &lt;code&gt;variables&lt;/code&gt; w.r.t. &lt;code&gt;loss&lt;/code&gt;.</title><link>/reference/keras/k_gradients/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_gradients/</guid><description>Returns the gradients of variables w.r.t. loss.
k_gradients(loss, variables) Arguments loss Scalar tensor to minimize.
variables List of variables.
Value A gradients tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns the index of the maximum value along an axis.</title><link>/reference/keras/k_argmax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_argmax/</guid><description>Returns the index of the maximum value along an axis.
k_argmax(x, axis = -1) Arguments x Tensor or variable.
axis Axis along which to perform the reduction (axis indexes are 1-based). Pass -1 (the default) to select the last axis.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Returns the index of the minimum value along an axis.</title><link>/reference/keras/k_argmin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_argmin/</guid><description>Returns the index of the minimum value along an axis.
k_argmin(x, axis = -1) Arguments x Tensor or variable.
axis Axis along which to perform the reduction (axis indexes are 1-based). Pass -1 (the default) to select the last axis.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Returns the learning phase flag.</title><link>/reference/keras/k_learning_phase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_learning_phase/</guid><description>The learning phase flag is a bool tensor (0 = test, 1 = train) to be passed as input to any Keras function that uses a different behavior at train time and test time.
k_learning_phase() Value Learning phase (scalar integer tensor or R integer).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Returns the number of axes in a tensor, as an integer.</title><link>/reference/keras/k_ndim/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ndim/</guid><description>Returns the number of axes in a tensor, as an integer.
k_ndim(x) Arguments x Tensor or variable.
Value Integer (scalar), number of axes.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns the shape of a variable.</title><link>/reference/keras/k_get_variable_shape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_get_variable_shape/</guid><description>Returns the shape of a variable.
k_get_variable_shape(x) Arguments x A variable.
Value A vector of integers.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Returns the shape of tensor or variable as a list of int or NULL entries.</title><link>/reference/keras/k_int_shape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_int_shape/</guid><description>Returns the shape of tensor or variable as a list of int or NULL entries.
k_int_shape(x) Arguments x Tensor or variable.
Value A list of integers (or NULL entries).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns the static number of elements in a Keras variable or tensor.</title><link>/reference/keras/k_count_params/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_count_params/</guid><description>Returns the static number of elements in a Keras variable or tensor.
k_count_params(x) Arguments x Keras variable or tensor.
Value Integer, the number of elements in x, i.e., the product of the array&#39;s static dimensions.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.</description></item><item><title>Returns the symbolic shape of a tensor or variable.</title><link>/reference/keras/k_shape/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_shape/</guid><description>Returns the symbolic shape of a tensor or variable.
k_shape(x) Arguments x A tensor or variable.
Value A symbolic shape (which is itself a tensor).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns the value of a variable.</title><link>/reference/keras/k_get_value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_get_value/</guid><description>Returns the value of a variable.
k_get_value(x) Arguments x input variable.
Value An R array.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Returns the value of more than one tensor variable.</title><link>/reference/keras/k_batch_get_value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_batch_get_value/</guid><description>Returns the value of more than one tensor variable.
k_batch_get_value(ops) Arguments ops List of ops to evaluate.
Value A list of arrays.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns whether &lt;code&gt;x&lt;/code&gt; is a Keras tensor.</title><link>/reference/keras/k_is_keras_tensor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_is_keras_tensor/</guid><description>A &#34;Keras tensor&#34; is a tensor that was returned by a Keras layer
k_is_keras_tensor(x) Arguments x A candidate tensor.
Value A logical: Whether the argument is a Keras tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns whether &lt;code&gt;x&lt;/code&gt; is a placeholder.</title><link>/reference/keras/k_is_placeholder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_is_placeholder/</guid><description>Returns whether x is a placeholder.
k_is_placeholder(x) Arguments x A candidate placeholder.
Value A logical
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Returns whether &lt;code&gt;x&lt;/code&gt; is a symbolic tensor.</title><link>/reference/keras/k_is_tensor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_is_tensor/</guid><description>Returns whether x is a symbolic tensor.
k_is_tensor(x) Arguments x A candidate tensor.
Value A logical: Whether the argument is a symbolic tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Returns whether a tensor is a sparse tensor.</title><link>/reference/keras/k_is_sparse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_is_sparse/</guid><description>Returns whether a tensor is a sparse tensor.
k_is_sparse(tensor) Arguments tensor A tensor instance.
Value A logical
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Returns whether the &lt;code&gt;targets&lt;/code&gt; are in the top &lt;code&gt;k&lt;/code&gt; &lt;code&gt;predictions&lt;/code&gt;.</title><link>/reference/keras/k_in_top_k/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_in_top_k/</guid><description>Returns whether the targets are in the top k predictions.
k_in_top_k(predictions, targets, k) Arguments predictions A tensor of shape (batch_size, classes) and type float32.
targets A 1D tensor of length batch_size and type int32 or int64.
k An int, number of top elements to consider.
Value A 1D tensor of length batch_size and type bool. output[[i]] is TRUE if predictions[i, targets[[i]] is within top-k values of predictions[[i]].</description></item><item><title>Reuters newswire topics classification</title><link>/reference/keras/dataset_reuters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/dataset_reuters/</guid><description>Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with dataset_imdb() , each wire is encoded as a sequence of word indexes (same conventions).
dataset_reuters( path = &#34;reuters.npz&#34;, num_words = NULL, skip_top = 0L, maxlen = NULL, test_split = 0.2, seed = 113L, start_char = 1L, oov_char = 2L, index_from = 3L ) dataset_reuters_word_index(path = &#34;reuters_word_index.pkl&#34;) Arguments path Where to cache the data (relative to ~/.</description></item><item><title>Reverse a tensor along the specified axes.</title><link>/reference/keras/k_reverse/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_reverse/</guid><description>Reverse a tensor along the specified axes.
k_reverse(x, axes) Arguments x Tensor to reverse.
axes Integer or list of integers of axes to reverse (axis indexes are 1-based).
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.</description></item><item><title>RMSProp optimizer</title><link>/reference/keras/optimizer_rmsprop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_rmsprop/</guid><description>RMSProp optimizer
optimizer_rmsprop( lr = 0.001, rho = 0.9, epsilon = NULL, decay = 0, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
rho float &amp;gt;= 0. Decay factor.
epsilon float &amp;gt;= 0. Fuzz factor. If NULL, defaults to k_epsilon().
decay float &amp;gt;= 0. Learning rate decay over each update.
clipnorm Gradients will be clipped when their L2 norm exceeds this value.</description></item><item><title>Runs CTC loss algorithm on each batch element.</title><link>/reference/keras/k_ctc_batch_cost/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_ctc_batch_cost/</guid><description>Runs CTC loss algorithm on each batch element.
k_ctc_batch_cost(y_true, y_pred, input_length, label_length) Arguments y_true tensor (samples, max_string_length) containing the truth labels.
y_pred tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.
input_length tensor (samples, 1) containing the sequence length for each batch item in y_pred.
label_length tensor (samples, 1) containing the sequence length for each batch item in y_true.</description></item><item><title>Save a text tokenizer to an external file</title><link>/reference/keras/save_text_tokenizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/save_text_tokenizer/</guid><description>Enables persistence of text tokenizers alongside saved models.
save_text_tokenizer(object, filename) load_text_tokenizer(filename) Arguments object Text tokenizer fit with fit_text_tokenizer()
filename File to save/load
Details You should always use the same text tokenizer for training and prediction. In many cases however prediction will occur in another session with a version of the model loaded via load_model_hdf5().
In this case you need to save the text tokenizer object after training and then reload it prior to prediction.</description></item><item><title>Save model weights in the SavedModel format</title><link>/reference/keras/save_model_weights_tf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/save_model_weights_tf/</guid><description>Save model weights in the SavedModel format
save_model_weights_tf(object, filepath, overwrite = TRUE) load_model_weights_tf( object, filepath, by_name = FALSE, skip_mismatch = FALSE, reshape = FALSE ) Arguments object Model object to save/load
filepath Path to the file
overwrite Whether to silently overwrite any existing file at the target location
by_name Whether to load weights by name or by topological order.</description></item><item><title>Save the model after every epoch.</title><link>/reference/keras/callback_model_checkpoint/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_model_checkpoint/</guid><description>filepath can contain named formatting options, which will be filled the value of epoch and keys in logs (passed in on_epoch_end). For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, then the model checkpoints will be saved with the epoch number and the validation loss in the filename.
callback_model_checkpoint( filepath, monitor = &#34;val_loss&#34;, verbose = 0, save_best_only = FALSE, save_weights_only = FALSE, mode = c(&#34;auto&#34;, &#34;min&#34;, &#34;max&#34;), period = NULL, save_freq = &#34;</description></item><item><title>Save/Load model weights using HDF5 files</title><link>/reference/keras/save_model_weights_hdf5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/save_model_weights_hdf5/</guid><description>Save/Load model weights using HDF5 files
save_model_weights_hdf5(object, filepath, overwrite = TRUE) load_model_weights_hdf5( object, filepath, by_name = FALSE, skip_mismatch = FALSE, reshape = FALSE ) Arguments object Model object to save/load
filepath Path to the file
overwrite Whether to silently overwrite any existing file at the target location
by_name Whether to load weights by name or by topological order.</description></item><item><title>Save/Load models using HDF5 files</title><link>/reference/keras/save_model_hdf5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/save_model_hdf5/</guid><description>Save/Load models using HDF5 files
save_model_hdf5(object, filepath, overwrite = TRUE, include_optimizer = TRUE) load_model_hdf5(filepath, custom_objects = NULL, compile = TRUE) Arguments object Model object to save
filepath File path
overwrite Overwrite existing file if necessary
include_optimizer If TRUE, save optimizer&#39;s state.
custom_objects Mapping class names (or function names) of custom (non-Keras) objects to class/functions (for example, custom metrics or custom loss functions).</description></item><item><title>Save/Load models using SavedModel format</title><link>/reference/keras/save_model_tf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/save_model_tf/</guid><description>Save/Load models using SavedModel format
save_model_tf( object, filepath, overwrite = TRUE, include_optimizer = TRUE, signatures = NULL, options = NULL ) load_model_tf(filepath, custom_objects = NULL, compile = TRUE) Arguments object Model object to save
filepath File path
overwrite Overwrite existing file if necessary
include_optimizer If TRUE, save optimizer&#39;s state.
signatures Signatures to save with the SavedModel.</description></item><item><title>Scaled Exponential Linear Unit.</title><link>/reference/keras/layer_activation_selu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_selu/</guid><description>SELU is equal to: scale * elu(x, alpha), where alpha and scale are pre-defined constants.
layer_activation_selu( object, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
input_shape Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.</description></item><item><title>Segment-wise linear approximation of sigmoid.</title><link>/reference/keras/k_hard_sigmoid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_hard_sigmoid/</guid><description>Faster than sigmoid. Returns 0. if x &amp;lt; -2.5, 1. if x &amp;gt; 2.5. In -2.5 &amp;lt;= x &amp;lt;= 2.5, returns 0.2 * x + 0.5.
k_hard_sigmoid(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Select a Keras implementation and backend</title><link>/reference/keras/use_implementation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/use_implementation/</guid><description>Select a Keras implementation and backend
use_implementation(implementation = c(&#34;keras&#34;, &#34;tensorflow&#34;)) use_backend(backend = c(&#34;tensorflow&#34;, &#34;cntk&#34;, &#34;theano&#34;, &#34;plaidml&#34;)) Arguments implementation One of &#34;keras&#34; or &#34;tensorflow&#34; (defaults to &#34;keras&#34;).
backend One of &#34;tensorflow&#34;, &#34;cntk&#34;, or &#34;theano&#34; (defaults to &#34;tensorflow&#34;)
Details Keras has multiple implementations (the original keras implementation and the implementation native to TensorFlow) and supports multiple backends (&#34;tensorflow&#34;, &#34;cntk&#34;, &#34;theano&#34;, and &#34;plaidml&#34;). These functions allow switching between the various implementations and backends.</description></item><item><title>Selects &lt;code&gt;x&lt;/code&gt; in test phase, and &lt;code&gt;alt&lt;/code&gt; otherwise.</title><link>/reference/keras/k_in_test_phase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_in_test_phase/</guid><description>Note that alt should have the same shape as x.
k_in_test_phase(x, alt, training = NULL) Arguments x What to return in test phase (tensor or function that returns a tensor).
alt What to return otherwise (tensor or function that returns a tensor).
training Optional scalar tensor (or R logical or integer) specifying the learning phase.
Value Either x or alt based on k_learning_phase().</description></item><item><title>Selects &lt;code&gt;x&lt;/code&gt; in train phase, and &lt;code&gt;alt&lt;/code&gt; otherwise.</title><link>/reference/keras/k_in_train_phase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_in_train_phase/</guid><description>Note that alt should have the same shape as x.
k_in_train_phase(x, alt, training = NULL) Arguments x What to return in train phase (tensor or function that returns a tensor).
alt What to return otherwise (tensor or function that returns a tensor).
training Optional scalar tensor (or R logical or integer) specifying the learning phase.
Value Either x or alt based on the training flag.</description></item><item><title>Separable 2D convolution.</title><link>/reference/keras/layer_separable_conv_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_separable_conv_2d/</guid><description>Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls how many output channels are generated per input channel in the depthwise step. Intuitively, separable convolutions can be understood as a way to factorize a convolution kernel into two smaller kernels, or as an extreme version of an Inception block.</description></item><item><title>Serialize a model to an R object</title><link>/reference/keras/serialize_model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/serialize_model/</guid><description>Model objects are external references to Keras objects which cannot be saved and restored across R sessions. The serialize_model() and unserialize_model() functions provide facilities to convert Keras models to R objects for persistence within R data files.
serialize_model(model, include_optimizer = TRUE) unserialize_model(model, custom_objects = NULL, compile = TRUE) Arguments model Keras model or R &#34;raw&#34; object containing serialized Keras model.
include_optimizer If TRUE, save optimizer&#39;s state.</description></item><item><title>Sets entries in &lt;code&gt;x&lt;/code&gt; to zero at random, while scaling the entire tensor.</title><link>/reference/keras/k_dropout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_dropout/</guid><description>Sets entries in x to zero at random, while scaling the entire tensor.
k_dropout(x, level, noise_shape = NULL, seed = NULL) Arguments x tensor
level fraction of the entries in the tensor that will be set to 0.
noise_shape shape for randomly generated keep/drop flags, must be broadcastable to the shape of x
seed random seed to ensure determinism.</description></item><item><title>Sets the learning phase to a fixed value.</title><link>/reference/keras/k_set_learning_phase/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_set_learning_phase/</guid><description>Sets the learning phase to a fixed value.
k_set_learning_phase(value) Arguments value Learning phase value, either 0 or 1 (integers).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Sets the manual variable initialization flag.</title><link>/reference/keras/k_manual_variable_initialization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_manual_variable_initialization/</guid><description>This boolean flag determines whether variables should be initialized as they are instantiated (default), or if the user should handle the initialization (e.g. via tf$initialize_all_variables()).
k_manual_variable_initialization(value) Arguments value Logical
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Sets the value of a variable, from an R array.</title><link>/reference/keras/k_set_value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_set_value/</guid><description>Sets the value of a variable, from an R array.
k_set_value(x, value) Arguments x Tensor to set to a new value.
value Value to set the tensor to, as an R array (of the same shape).
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Sets the values of many tensor variables at once.</title><link>/reference/keras/k_batch_set_value/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_batch_set_value/</guid><description>Sets the values of many tensor variables at once.
k_batch_set_value(lists) Arguments lists a list of lists (tensor, value). value should be an R array.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Sets vocabulary (and optionally document frequency) data for the layer</title><link>/reference/keras/set_vocabulary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/set_vocabulary/</guid><description>This method sets the vocabulary and DF data for this layer directly, instead of analyzing a dataset through adapt(). It should be used whenever the vocab (and optionally document frequency) information is already known. If vocabulary data is already present in the layer, this method will either replace it, if append is set to FALSE, or append to it (if &#39;append&#39; is set to TRUE)
set_vocabulary( object, vocab, df_data = NULL, oov_df_value = FALSE, append = FALSE ) Arguments object a text vectorization layer</description></item><item><title>Single gradient update or model evaluation over one batch of samples.</title><link>/reference/keras/train_on_batch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/train_on_batch/</guid><description>Single gradient update or model evaluation over one batch of samples.
train_on_batch(object, x, y, class_weight = NULL, sample_weight = NULL) test_on_batch(object, x, y, sample_weight = NULL) Arguments object Keras model object
x input data, as an array or list of arrays (if the model has multiple inputs).
y labels, as an array.
class_weight named list mapping classes to a weight value, used for scaling the loss function (during training only).</description></item><item><title>Softmax activation function.</title><link>/reference/keras/layer_activation_softmax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_softmax/</guid><description>It follows: f(x) = alpha * (exp(x) - 1.0) for x &amp;lt; 0, f(x) = x for x &amp;gt;= 0.
layer_activation_softmax( object, axis = -1, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
axis Integer, axis along which the softmax normalization is applied.</description></item><item><title>Softmax of a tensor.</title><link>/reference/keras/k_softmax/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_softmax/</guid><description>Softmax of a tensor.
k_softmax(x, axis = -1) Arguments x A tensor or variable.
axis The dimension softmax would be performed on. The default is -1 which indicates the last dimension.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Softplus of a tensor.</title><link>/reference/keras/k_softplus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_softplus/</guid><description>Softplus of a tensor.
k_softplus(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Softsign of a tensor.</title><link>/reference/keras/k_softsign/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_softsign/</guid><description>Softsign of a tensor.
k_softsign(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Spatial 1D version of Dropout.</title><link>/reference/keras/layer_spatial_dropout_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_spatial_dropout_1d/</guid><description>This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, layer_spatial_dropout_1d will help promote independence between feature maps and should be used instead.</description></item><item><title>Spatial 2D version of Dropout.</title><link>/reference/keras/layer_spatial_dropout_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_spatial_dropout_2d/</guid><description>This version performs the same function as Dropout, however it drops entire 2D feature maps instead of individual elements. If adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, layer_spatial_dropout_2d will help promote independence between feature maps and should be used instead.</description></item><item><title>Spatial 3D version of Dropout.</title><link>/reference/keras/layer_spatial_dropout_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_spatial_dropout_3d/</guid><description>This version performs the same function as Dropout, however it drops entire 3D feature maps instead of individual elements. If adjacent voxels within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, layer_spatial_dropout_3d will help promote independence between feature maps and should be used instead.</description></item><item><title>Stacks a list of rank &lt;code&gt;R&lt;/code&gt; tensors into a rank &lt;code&gt;R&#43;1&lt;/code&gt; tensor.</title><link>/reference/keras/k_stack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_stack/</guid><description>Stacks a list of rank R tensors into a rank R+1 tensor.
k_stack(x, axis = 1) Arguments x List of tensors.
axis Axis along which to perform stacking (axis indexes are 1-based).
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Standard deviation of a tensor, alongside the specified axis.</title><link>/reference/keras/k_std/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_std/</guid><description>Standard deviation of a tensor, alongside the specified axis.
k_std(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, the axis to compute the standard deviation over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>Stochastic gradient descent optimizer</title><link>/reference/keras/optimizer_sgd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/optimizer_sgd/</guid><description>Stochastic gradient descent optimizer with support for momentum, learning rate decay, and Nesterov momentum.
optimizer_sgd( lr = 0.01, momentum = 0, decay = 0, nesterov = FALSE, clipnorm = NULL, clipvalue = NULL ) Arguments lr float &amp;gt;= 0. Learning rate.
momentum float &amp;gt;= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations.
decay float &amp;gt;= 0. Learning rate decay over each update.</description></item><item><title>Stop training when a monitored quantity has stopped improving.</title><link>/reference/keras/callback_early_stopping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_early_stopping/</guid><description>Stop training when a monitored quantity has stopped improving.
callback_early_stopping( monitor = &#34;val_loss&#34;, min_delta = 0, patience = 0, verbose = 0, mode = c(&#34;auto&#34;, &#34;min&#34;, &#34;max&#34;), baseline = NULL, restore_best_weights = FALSE ) Arguments monitor quantity to be monitored.
min_delta minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.</description></item><item><title>Sum of the values in a tensor, alongside the specified axis.</title><link>/reference/keras/k_sum/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_sum/</guid><description>Sum of the values in a tensor, alongside the specified axis.
k_sum(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, the axis to sum over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>Switches between two operations depending on a scalar value.</title><link>/reference/keras/k_switch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_switch/</guid><description>Note that both then_expression and else_expression should be symbolic tensors of the same shape.
k_switch(condition, then_expression, else_expression) Arguments condition tensor (int or bool).
then_expression either a tensor, or a function that returns a tensor.
else_expression either a tensor, or a function that returns a tensor.
Value The selected tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Takes the dataframe and the path to a directory and generates batches of augmented/normalized data.</title><link>/reference/keras/flow_images_from_dataframe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/flow_images_from_dataframe/</guid><description>Takes the dataframe and the path to a directory and generates batches of augmented/normalized data.
flow_images_from_dataframe( dataframe, directory = NULL, x_col = &#34;filename&#34;, y_col = &#34;class&#34;, generator = image_data_generator(), target_size = c(256, 256), color_mode = &#34;rgb&#34;, classes = NULL, class_mode = &#34;categorical&#34;, batch_size = 32, shuffle = TRUE, seed = NULL, save_to_dir = NULL, save_prefix = &#34;&#34;, save_format = &#34;png&#34;, subset = NULL, interpolation = &#34;nearest&#34;, drop_duplicates = TRUE ) Arguments dataframe data.</description></item><item><title>TensorBoard basic visualizations</title><link>/reference/keras/callback_tensorboard/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/callback_tensorboard/</guid><description>This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.
callback_tensorboard( log_dir = NULL, histogram_freq = 0, batch_size = NULL, write_graph = TRUE, write_grads = FALSE, write_images = FALSE, embeddings_freq = 0, embeddings_layer_names = NULL, embeddings_metadata = NULL, embeddings_data = NULL, update_freq = &#34;epoch&#34;, profile_batch = 0 ) Arguments log_dir The path of the directory where to save the log files to be parsed by Tensorboard.</description></item><item><title>Text tokenization utility</title><link>/reference/keras/text_tokenizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/text_tokenizer/</guid><description>Vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...
text_tokenizer( num_words = NULL, filters = &#34;!\&#34;#$%&amp;amp;()*+,-./:;&amp;lt;=&amp;gt;?@[\\]^_`{|}~\t\n&#34;, lower = TRUE, split = &#34; &#34;, char_level = FALSE, oov_token = NULL ) Arguments num_words the maximum number of words to keep, based on word frequency.</description></item><item><title>Text vectorization layer</title><link>/reference/keras/layer_text_vectorization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_text_vectorization/</guid><description>This layer has basic options for managing text in a Keras model. It transforms a batch of strings (one sample = one string) into either a list of token indices (one sample = 1D tensor of integer token indices) or a dense representation (one sample = 1D tensor of float values representing data about the sample&#39;s tokens).
layer_text_vectorization( object, max_tokens = NULL, standardize = &#34;lower_and_strip_punctuation&#34;, split = &#34;whitespace&#34;, ngrams = NULL, output_mode = c(&#34;</description></item><item><title>TF session to be used by the backend.</title><link>/reference/keras/k_get_session/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_get_session/</guid><description>If a default TensorFlow session is available, we will return it. Else, we will return the global Keras session. If no global Keras session exists at this point: we will create a new global session. Note that you can manually set the global session via k_set_session().
k_get_session() k_set_session(session) Arguments session A TensorFlow Session.
Value A TensorFlow session
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.</description></item><item><title>Thresholded Rectified Linear Unit.</title><link>/reference/keras/layer_activation_thresholded_relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_activation_thresholded_relu/</guid><description>It follows: f(x) = x for x &amp;gt; theta, f(x) = 0 otherwise.
layer_activation_thresholded_relu( object, theta = 1, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
theta float &amp;gt;= 0. Threshold location of activation.
input_shape Input shape (list of integers, does not include the samples axis) which is required when using this layer as the first layer in a model.</description></item><item><title>Train a Keras model</title><link>/reference/keras/fit.keras.engine.training.model/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/fit.keras.engine.training.model/</guid><description>Trains the model for a fixed number of epochs (iterations on a dataset).
# S3 method for keras.engine.training.Model fit( object, x = NULL, y = NULL, batch_size = NULL, epochs = 10, verbose = getOption(&#34;keras.fit_verbose&#34;, default = 1), callbacks = NULL, view_metrics = getOption(&#34;keras.view_metrics&#34;, default = &#34;auto&#34;), validation_split = 0, validation_data = NULL, shuffle = TRUE, class_weight = NULL, sample_weight = NULL, initial_epoch = 0, steps_per_epoch = NULL, validation_steps = NULL, .</description></item><item><title>Transform each text in texts in a sequence of integers.</title><link>/reference/keras/texts_to_sequences/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/texts_to_sequences/</guid><description> Only top &#34;num_words&#34; most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.
texts_to_sequences(tokenizer, texts) Arguments tokenizer Tokenizer
texts Vector/list of texts (strings).
See also Other text tokenization: fit_text_tokenizer(), save_text_tokenizer(), sequences_to_matrix(), text_tokenizer(), texts_to_matrix(), texts_to_sequences_generator()</description></item><item><title>Transforms each text in texts in a sequence of integers.</title><link>/reference/keras/texts_to_sequences_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/texts_to_sequences_generator/</guid><description> Only top &#34;num_words&#34; most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.
texts_to_sequences_generator(tokenizer, texts) Arguments tokenizer Tokenizer
texts Vector/list of texts (strings).
Value Generator which yields individual sequences
See also Other text tokenization: fit_text_tokenizer(), save_text_tokenizer(), sequences_to_matrix(), text_tokenizer(), texts_to_matrix(), texts_to_sequences()</description></item><item><title>Transposed 2D convolution layer (sometimes called Deconvolution).</title><link>/reference/keras/layer_conv_2d_transpose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_2d_transpose/</guid><description>The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution. When using this layer as the first layer in a model, provide the keyword argument input_shape (list of integers, does not include the sample axis), e.</description></item><item><title>Transposed 3D convolution layer (sometimes called Deconvolution).</title><link>/reference/keras/layer_conv_3d_transpose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_conv_3d_transpose/</guid><description>The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution.
layer_conv_3d_transpose( object, filters, kernel_size, strides = c(1, 1, 1), padding = &#34;valid&#34;, output_padding = NULL, data_format = NULL, activation = NULL, use_bias = TRUE, kernel_initializer = &#34;</description></item><item><title>Transposes a tensor and returns it.</title><link>/reference/keras/k_transpose/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_transpose/</guid><description>Transposes a tensor and returns it.
k_transpose(x) Arguments x Tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.rstudio.com/articles/backend.html#backend-functions.</description></item><item><title>Turn a nD tensor into a 2D tensor with same 1st dimension.</title><link>/reference/keras/k_batch_flatten/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_batch_flatten/</guid><description>In other words, it flattens each data samples of a batch.
k_batch_flatten(x) Arguments x A tensor or variable.
Value A tensor.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Turns positive integers (indexes) into dense vectors of fixed size.</title><link>/reference/keras/layer_embedding/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_embedding/</guid><description>For example, list(4L, 20L) -&amp;gt; list(c(0.25, 0.1), c(0.6, -0.2)) This layer can only be used as the first layer in a model.
layer_embedding( object, input_dim, output_dim, embeddings_initializer = &#34;uniform&#34;, embeddings_regularizer = NULL, activity_regularizer = NULL, embeddings_constraint = NULL, mask_zero = FALSE, input_length = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
input_dim int &amp;gt; 0.</description></item><item><title>Update the value of &lt;code&gt;x&lt;/code&gt; by adding &lt;code&gt;increment&lt;/code&gt;.</title><link>/reference/keras/k_update_add/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_update_add/</guid><description>Update the value of x by adding increment.
k_update_add(x, increment) Arguments x A Variable.
increment A tensor of same shape as x.
Value The variable x updated.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Update the value of &lt;code&gt;x&lt;/code&gt; by subtracting &lt;code&gt;decrement&lt;/code&gt;.</title><link>/reference/keras/k_update_sub/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_update_sub/</guid><description>Update the value of x by subtracting decrement.
k_update_sub(x, decrement) Arguments x A Variable.
decrement A tensor of same shape as x.
Value The variable x updated.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Update the value of &lt;code&gt;x&lt;/code&gt; to &lt;code&gt;new_x&lt;/code&gt;.</title><link>/reference/keras/k_update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_update/</guid><description>Update the value of x to new_x.
k_update(x, new_x) Arguments x A Variable.
new_x A tensor of same shape as x.
Value The variable x updated.
Keras Backend This function is part of a set of Keras backend functions that enable lower level access to the core operations of the backend tensor engine (e.g. TensorFlow, CNTK, Theano, etc.).
You can see a list of all available backend functions here: https://keras.</description></item><item><title>Update tokenizer internal vocabulary based on a list of texts or list of sequences.</title><link>/reference/keras/fit_text_tokenizer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/fit_text_tokenizer/</guid><description>Update tokenizer internal vocabulary based on a list of texts or list of sequences.
fit_text_tokenizer(object, x) Arguments object Tokenizer returned by text_tokenizer()
x Vector/list of strings, or a generator of strings (for memory-efficiency); Alternatively a list of &#34;sequence&#34; (a sequence is a list of integer word indices).
Note Required before using texts_to_sequences(), texts_to_matrix(), or sequences_to_matrix().
See also Other text tokenization: save_text_tokenizer(), sequences_to_matrix(), text_tokenizer(), texts_to_matrix(), texts_to_sequences_generator(), texts_to_sequences()</description></item><item><title>Upsampling layer for 1D inputs.</title><link>/reference/keras/layer_upsampling_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_upsampling_1d/</guid><description>Repeats each temporal step size times along the time axis.
layer_upsampling_1d( object, size = 2L, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
size integer. Upsampling factor.
batch_size Fixed batch size for layer
name An optional name string for the layer. Should be unique in a model (do not reuse the same name twice).</description></item><item><title>Upsampling layer for 2D inputs.</title><link>/reference/keras/layer_upsampling_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_upsampling_2d/</guid><description>Repeats the rows and columns of the data by size[[0]] and size[[1]] respectively.
layer_upsampling_2d( object, size = c(2L, 2L), data_format = NULL, interpolation = &#34;nearest&#34;, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
size int, or list of 2 integers. The upsampling factors for rows and columns.
data_format A string, one of channels_last (default) or channels_first.</description></item><item><title>Upsampling layer for 3D inputs.</title><link>/reference/keras/layer_upsampling_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_upsampling_3d/</guid><description>Repeats the 1st, 2nd and 3rd dimensions of the data by size[[0]], size[[1]] and size[[2]] respectively.
layer_upsampling_3d( object, size = c(2L, 2L, 2L), data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
size int, or list of 3 integers. The upsampling factors for dim1, dim2 and dim3.
data_format A string, one of channels_last (default) or channels_first.</description></item><item><title>Utility function for generating batches of temporal data.</title><link>/reference/keras/timeseries_generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/timeseries_generator/</guid><description>Utility function for generating batches of temporal data.
timeseries_generator( data, targets, length, sampling_rate = 1, stride = 1, start_index = 0, end_index = NULL, shuffle = FALSE, reverse = FALSE, batch_size = 128 ) Arguments data Object containing consecutive data points (timesteps). The data should be 2D, and axis 1 is expected to be the time dimension.
targets Targets corresponding to timesteps in data.</description></item><item><title>Variance of a tensor, alongside the specified axis.</title><link>/reference/keras/k_var/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/k_var/</guid><description>Variance of a tensor, alongside the specified axis.
k_var(x, axis = NULL, keepdims = FALSE) Arguments x A tensor or variable.
axis An integer, the axis to compute the variance over (axis indexes are 1-based).
keepdims A boolean, whether to keep the dimensions or not. If keepdims is FALSE, the rank of the tensor is reduced by 1. If keepdims is TRUE, the reduced dimension is retained with length 1.</description></item><item><title>VGG16 and VGG19 models for Keras.</title><link>/reference/keras/application_vgg/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_vgg/</guid><description>VGG16 and VGG19 models for Keras.
application_vgg16( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) application_vgg19( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) Arguments include_top whether to include the 3 fully-connected layers at the top of the network.
weights NULL (random initialization), imagenet (ImageNet weights), or the path to the weights file to be loaded.</description></item><item><title>Weight constraints</title><link>/reference/keras/constraints/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/constraints/</guid><description>Functions that impose constraints on weight values.
constraint_maxnorm(max_value = 2, axis = 0) constraint_nonneg() constraint_unitnorm(axis = 0) constraint_minmaxnorm(min_value = 0, max_value = 1, rate = 1, axis = 0) Arguments max_value The maximum norm for the incoming weights.
axis The axis along which to calculate weight norms. For instance, in a dense layer the weight matrix has shape input_dim, output_dim, set axis to 0 to constrain each weight vector of length input_dim,.</description></item><item><title>Wraps arbitrary expression as a layer</title><link>/reference/keras/layer_lambda/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_lambda/</guid><description>Wraps arbitrary expression as a layer
layer_lambda( object, f, output_shape = NULL, mask = NULL, arguments = NULL, input_shape = NULL, batch_input_shape = NULL, batch_size = NULL, dtype = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
f The function to be evaluated. Takes input tensor as first argument.
output_shape Expected output shape from the function (not required when using TensorFlow back-end).</description></item><item><title>Xception V1 model for Keras.</title><link>/reference/keras/application_xception/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/application_xception/</guid><description>Xception V1 model for Keras.
application_xception( include_top = TRUE, weights = &#34;imagenet&#34;, input_tensor = NULL, input_shape = NULL, pooling = NULL, classes = 1000 ) xception_preprocess_input(x) Arguments include_top whether to include the fully-connected layer at the top of the network.
weights NULL (random initialization), imagenet (ImageNet weights), or the path to the weights file to be loaded.
input_tensor optional Keras tensor to use as image input for the model.</description></item><item><title>Zero-padding layer for 1D input (e.g. temporal sequence).</title><link>/reference/keras/layer_zero_padding_1d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_zero_padding_1d/</guid><description>Zero-padding layer for 1D input (e.g. temporal sequence).
layer_zero_padding_1d( object, padding = 1L, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
padding int, or list of int (length 2)
If int: How many zeros to add at the beginning and end of the padding dimension (axis 1).
If list of int (length 2): How many zeros to add at the beginning and at the end of the padding dimension ((left_pad, right_pad)).</description></item><item><title>Zero-padding layer for 2D input (e.g. picture).</title><link>/reference/keras/layer_zero_padding_2d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_zero_padding_2d/</guid><description>This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.
layer_zero_padding_2d( object, padding = c(1L, 1L), data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
padding int, or list of 2 ints, or list of 2 lists of 2 ints.</description></item><item><title>Zero-padding layer for 3D data (spatial or spatio-temporal).</title><link>/reference/keras/layer_zero_padding_3d/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/reference/keras/layer_zero_padding_3d/</guid><description>Zero-padding layer for 3D data (spatial or spatio-temporal).
layer_zero_padding_3d( object, padding = c(1L, 1L, 1L), data_format = NULL, batch_size = NULL, name = NULL, trainable = NULL, weights = NULL ) Arguments object Model or layer object
padding int, or list of 3 ints, or list of 3 lists of 2 ints.
If int: the same symmetric padding is applied to width and height.</description></item></channel></rss>