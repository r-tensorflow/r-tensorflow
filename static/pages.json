[{"path":"/deploy/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"deploy-overview\"","    parent":"   parent: \"deploy-top\"","    weight":"   weight: 1","content":"\nThere are multiple ways to deploy TensorFlow models. In this section we will \ndescribe some of the most used ways of deploying those models.\n\nPlumber API: Create a REST API using Plumber to deploy\nyour TensorFlow model. With Plumber you will still depend on having an R runtime\nwhich be useful when you want to make the data pre-processing in R.\n\nShiny: Create a Shiny app that uses a TensorFlow model \nto generate outputs. \n\nTensorFlow Serving: This is the most performant way of deploying TensorFlow models since it's based only inn the TensorFlow serving C++ server. With TF serving you don't depend\non an R runtime, so all pre-processing must be done in the TensorFlow graph.\n\nRStudio Connect: RStudio Connect makes it easy to deploy\nTensorFlow models and uses TensorFlow serving in the backend.\n\nThere are many other options to deploy TensorFlow models built with R that are not\ncovered in this section. For example:\n\nDeploy it using a Python runtime.\nDeploy using a JavaScript runtime.\nDeploy to a mobile phone app using TensorFlow Lite.\nDeploy to a iOS app using Apple's Core ML tool.\nUse plumber and Docker to deploy your TensorFlow model (by T-Mobile).\n\n","id":0},{"path":"/deploy/docker","title":"\"Deploying a TensorFlow model using TensorFlow serving\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"TensorFlow Serving\"","    identifier":"   identifier: \"deploy-tf-serving\"","    parent":"   parent: \"deploy-top\"","    weight":"   weight: 30","editor_options":"","  chunk_output_type":" chunk_output_type: console","content":"\nIn this tutorial you will learn how to deploy a TensorFlow model using TensorFlow serving.\n\nWe will use the Docker container provided by the TensorFlow organization to deploy a model that classifies images of handwritten digits.\n\nUsing the Docker container is a an easy way to test the API locally and\nthen deploy it to any cloud provider.\n\nBuilding the model\n\nThe first thing we are going to do is to build our model.\nWe will use the Keras API to build this model.\n\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x - (mnist$train$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x - (mnist$test$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural \nnetwork.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate \nthe results on the test dataset with:\n\nmodel %% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model.\nFirst, let's save the model in the SavedModel format using:\n\nsavemodeltf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file.\n\n Running locally\n\nYou can run the tensorflow/serving Docker image locally using the great stevedore\npackage. For example:\n\ndocker <- stevedore::docker_client()\ncontainer <- docker$container$run(\n  image = \"tensorflow/serving\", # name of the image\n  \n  # host port and docker port - if you set 4000:8501, the API \n  # will be accecible in localhost:4000\n  port = \"8501:8501\", \n  \n  # a string path/to/the/saved/model/locally:models/modelname/version\n  # you must put the model file in the /models/ folder.\n  volume = paste0(normalizePath(\"cnn-mnist\"), \":/models/model/1\"), \n  \n  # the name of the model - it's the name of the folder inside /models\n  # above.\n  env = c(\"MODEL_NAME\" = \"model\"),\n  \n  # to run the container detached\n  detach = TRUE\n)\n\nNow we have initialized the container serving the model. You can see the container\nlogs with:\n\ncontainer$logs()\n\nNow you can make POST requests no the following endpoint : http://localhost:8501/v1/models/model/versions/1:predict.\nThe input data must be passed in a special format 0 - see the format definition here, which may\nseem unnatural for R users. Here is an example:\n\ninstances - purrr::array_tree(mnist$test$x[1:5,,,,drop=FALSE]) %% \n  purrr::map(~list(input_1 = .x))\ninstances <- list(instances = instances)\n\nreq <- httr::POST(\n  \"http://localhost:8501/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nThis is how you can serve TensorFlow models with TF serving locally.\nAdditionaly, we can deploy this to multiple clouds. In the next section we will show how it can be deployed to Google Cloud.\n\nWhen done, you can stop the container with:\n\ncontainer$stop()\n\nDeploying to Google Cloud Run\n\nTHe first thing you need to do is to follow the section Before you begin in this page.\n\nNow let's create a Dockerfile that will copy the SavedModel to the container image. We assume in this section some experience with Docker.\n\nHere's an example - create a file called Dockerfile in the same root folder as your SavedModel and paste the following:\n\nFROM tensorflow/serving\nCOPY cnn-mnist /models/model/1\nENTRYPOINT [\"/usr/bin/tfservingentrypoint.sh\", \"--restapiport=8080\"]\n\nWe need to run the rest service in the 8080 port. The only that is open by Google Cloud Run. Now you can build this image and send it to gcr.io.\nRun the following in your terminal:\n\ndocker build -t gcr.io/PROJECT-ID/cnn-mnist .\ndocker push gcr.io/PROJECT-ID/cnn-mnist\n\nYou can get your PROJECT-ID by running:\n\ngcloud config get-value project\n\nNext, we can create the service in Google Cloud Run using:\n\ngcloud run deploy --image gcr.io/rstudio-162821/cnn-mnist --platform managed\n\nYou will be prompted to select a region, a name for the service and wether you allow unauthorized requests. If everything works correctly \nyou will get a url like https://cnn-mnist-ld4lzfalyq-ue.a.run.app \nwhich you can now use to make requests to your model. For example:\n\nreq <- httr::POST(\n  \"https://cnn-mnist-ld4lzfalyq-ue.a.run.app/v1/models/model/versions/1:predict\", \n  body = instances, \n  encode = \"json\"\n)\nhttr::content(req)\n\nNote that in this case, all pre-processing must be done in R before sending the data to the API. \n\n","id":1},{"path":"/deploy/plumber","title":"\"Deploying a TensorFlow API with Plumber\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Plumber\"","    identifier":"   identifier: \"deploy-plumber\"","    parent":"   parent: \"deploy-top\"","    weight":"   weight: 10","editor_options":"","  chunk_output_type":" chunk_output_type: console","content":"\nIn this tutorial you will learn how to deploy a TensorFlow model using a plumber API.\n\nIn this example we will build an endpoint that takes POST requests sending images containing handwritten digits and returning the predicted number.\n\nBuilding the model\n\nThe first thing we are going to do is to build our model. W\nWe will use the Keras API to build this model.\n\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x - (mnist$train$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x - (mnist$test$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural \nnetwork.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate \nthe results on the test dataset with:\n\nmodel %% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model.\nFirst, let's save the model in the SavedModel format using:\n\nsavemodeltf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file.\n\n Plumber API\n\nA plumber API is defined by a .R file with a few annotations. Here's is how we can write our api.R file:\n\nlibrary(keras)\n\nmodel <- loadmodeltf(\"cnn-mnist/\")\n\n* Predicts the number in an image\n* @param enc a base64  encoded 28x28 image\n* @post /cnn-mnist\nfunction(enc)  \n\nMake sure to have the your SavedModel in the same folder as api.R and call:\n\np <- plumber::plumb(\"api.R\")\np$run(port = 8000)\n\nYou can now make requests to the http://lcoalhost:8000/cnn-minist/ endpoint. For example, let's verify we can make a POST request to the API sending the first image from the test set:\n\nimg <- mnist$test$x[1,,,]\nmnist$test$y[1]\n\nFirst let's encode the image:\n\nencoded_img - img %% \n  jpeg::writeJPEG() %% \n  base64enc::base64encode()\nencoded_img\n\nreq <- httr::POST(\"http://localhost:8000/cnn-mnist\",\n           body = list(enc = encoded_img), \n           encode = \"json\")\nhttr::content(req)\n\n[[1]]\n[1] 7\n\nYou can also access the Swagger interface by accessing http://127.0.0.1:8000/swagger/ and paste the encoded string in the UI to visualize the result.\n\nMore advanced models\n\nWhen building more advanced models you may not be able to save the entire model using the savemodeltf function. In this case you can use the savemodelweights_tf function. \n\nFor example:\n\nsavemodelweights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use loadmodelweights_tf to load\nthe model weights.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nloadmodelweights_tf(model, \"cnn-model-weights\")\n\n Hosting the plumber API\n\nPlumber is very flexible and allows multiple hosting options. See the plumber Hostinng documentation for more information.\n\n","id":2},{"path":"/deploy/rsconnect","title":"\"Deploying a TensorFlow Model to RStudio Connect\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"RStudio Connect\"","    identifier":"   identifier: \"deploy-rsconnect\"","    parent":"   parent: \"deploy-top\"","    weight":"   weight: 50","editor_options":"","  chunk_output_type":" chunk_output_type: console","content":"\nIn this tutorial you will learn how to deploy a TensorFlow model to RStudio Connect. RStudio Connect uses [TensorFlow\nServing](https://github.com/tensorflow/serving) for performance but makes it \nmuch easier for R users to manage their deployment.\n\nBuilding the model\n\nThe first thing we are going to do is to build our model.\nWe will use the Keras API to build this model.\n\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x - (mnist$train$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x - (mnist$test$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural \nnetwork.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate \nthe results on the test dataset with:\n\nmodel %% evaluate(x = mnist$test$x, y = mnist$test$y, verbose = 0)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model.\nFirst, let's save the model in the SavedModel format using:\n\nsavemodeltf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file.\n\n Deployiong to RStudio Connect\n\nOnce the model is saved to the SavedModel format, the model can be deployed with\na single line of code:\n\nrsconnect::deployTFModel(\"cnn-mnist/\")\n\nWhen the deployment is complete you will be redirected to your browser with some\ninstructions on how to call the REST endpoint:\n\n \n\n","id":3},{"path":"/deploy/shiny","title":"\"Deploying a Shiny app with a TensorFlow model\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Shiny\"","    identifier":"   identifier: \"deploy-shiny\"","    parent":"   parent: \"deploy-top\"","    weight":"   weight: 10","editor_options":"","  chunk_output_type":" chunk_output_type: console","content":"\nIn this tutorial you will learn how to deploy a TensorFlow model inside a Shiny app.\nWe will build a model that can classify handwritten digits in images, then we will\nbuild a Shiny app that let's you upload an image and get predictions from this model.\n\nBuilding the model\n\nThe first thing we are going to do is to build our model.\nWe will use the Keras API to build this model.\n\nWe will use the MNIST dataset to build our model.\n\nlibrary(keras)\nlibrary(tensorflow)\nmnist <- dataset_mnist()\n\nmnist$train$x - (mnist$train$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nmnist$test$x - (mnist$test$x/255) %% \n  array_reshape(., dim = c(dim(.), 1))\n\nNow, we are going to define our Keras model, it will be a simple convolutional neural \nnetwork.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNext, we fit the model using the MNIST dataset:\n\nmodel %% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    batch_size = 32,\n    epochs = 5,\n    validation_sample = 0.2,\n    verbose = 2\n  )\n\nWhen we are happy with our model accuracy in the validation dataset we can evaluate \nthe results on the test dataset with:\n\nmodel %% evaluate(x = mnist$test$x, y = mnist$test$y)\n\nOK, we have 99% accuracy on the test dataset and we want to deploy that model.\nFirst, let's save the model in the SavedModel format using:\n\nsavemodeltf(model, \"cnn-mnist\")\n\nWith the model built and saved we can now start building our plumber API file.\n\n Shiny app\n\nA simple shiny app can be define in an app.R file with a few conventions. Here's\nhow we can structure our Shiny app.\n\nlibrary(shiny)\nlibrary(keras)\n\nLoad the model\nmodel <- loadmodeltf(\"cnn-mnist/\")\n\n Define the UI\nui <- fluidPage(\n  # App title -,  titlePanel(\"Hello TensorFlow!\"),\n  # Sidebar layout with input and output definitions -,  sidebarLayout(\n    # Sidebar panel for inputs -,    sidebarPanel(\n      # Input: File upload\n      fileInput(\"image_path\", label = \"Input a JPEG image\")\n    ),\n    # Main panel for displaying outputs -,    mainPanel(\n      # Output: Histogram -,      textOutput(outputId = \"prediction\"),\n      plotOutput(outputId = \"image\")\n    )\n  )\n)\n\nDefine server logic required to draw a histogram -,server <- function(input, output)  )\n  \n  output$prediction <- renderText( )\n  \n  output$image <- renderPlot( )\n  \n \n\nshinyApp(ui, server)\n\nThis app can be used locally or deployed using any Shiny deployment option.\nIf you are deploying to RStudio Connect or Shinnyapps.io, don't forget to set the RETICULATE_PYTHON environment variable so rsconnect can detect what python\npackages are required to reproduce your local environment. See the F.A.Q. for more information.\n\nYou can see a live version of this app here. Note that to keep the code simple, it will only accept JPEG images with 28x28 pixels. You can download this file if you want to try the app.\n\n More advanced models\n\nWhen building more advanced models you may not be able to save the entire model using the savemodeltf function. In this case you can use the savemodelweights_tf function. \n\nFor example:\n\nsavemodelweights_tf(model, \" cnn-model-weights\")\n\nThen, in the api.R file whenn loading the model you will need to rebuild the model using the exact same code that you used when training and saving and then use loadmodelweights_tf to load\nthe model weights.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nloadmodelweights_tf(model, \"cnn-model-weights\")\n\nHosting the shiny app\n\nThis Shiny app can be hosted in any server using the Shiny Server. If you are managing the complete infrastructure, make sure that you have Python and all required Python packages\ninstalled in the server. \n\nIf you are using Shinyapps.io or RStudio Connect the dependencies will be infered when deploying the app. In this case, don't forget to set the RETICULATE_PYTHON environment variable.\n\nYou can find more examples of using reticulate in RStudio products here and learn more about Python in \nRStudio Connect best practices here.\n\n","id":4},{"path":"/guide/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"guide-get-started\"","    parent":"   parent: \"guide-top\"","    weight":"   weight: 10","content":"\nIn the guide you will find documentation about specific functionality of the R packages in the TensorFlow ecosystem.\n\nEssential documentation\n\nIf you are just getting started started with TensorFlow for R, we recommend reading the following selected topics in the guide:\n\nInstalling TensorFlow: Install TensorFlow R packages and its dependencies.\nGet started with Keras: First time using Keras? Read ir!\nRead more about Keras: More information about customizing your Keras models.\nGet started with TensorFlow Datasets: Overview of the TensorFlow Datasets API.\nEager execution: Understand TensorFlow's eager execution.\n\n Customization\n\nRead the following guides for more information on how to customize your model with TensorFlow and Keras:\n\nCustom Layers: Create custom layers for your Keras models.\nCallbacks: Using callbacks to customize model training.\nRagged Tensors: Data structure useful for sequences of variable length.\nFeature Spec API: Use the feature spec interface to build models for tabular data.\n  \nSee also the deployment guide.\n\n","id":5},{"path":"/guide/keras/_","title":"\"Getting Started with Keras\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"keras-keras-overview\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 5 ","content":"\n\nlibrary(keras)\nknitr::opts_chunk$set(eval = FALSE)\n\nOverview\n\nKeras is a high-level neural networks API developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Keras has the following key features:\n\nAllows the same code to run on CPU or on GPU, seamlessly.\n\nUser-friendly API which makes it easy to quickly prototype deep learning models.\n\nBuilt-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n\nSupports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.\n\nThis website provides documentation for the R interface to Keras. See the main Keras website at https://keras.io for additional information on the project.\n\n Installation\n\nFirst, install the keras R package with:\n\ninstall.packages(\"keras\")\n\nor install the development version with:\n\ndevtools::install_github(\"rstudio/keras\")\n\nThe Keras R interface uses the TensorFlow backend engine by default.\n\ninstall.packages(\"keras\")\ninstall_keras()\n\nThis will provide you with default CPU-based installations of Keras and TensorFlow. If you want a more customized installation, e.g. if you want to take advantage of NVIDIA GPUs, see the documentation for install_keras() and the installation section.\n\nMNIST Example\n\nWe can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:\n\nimg style=\"width: 50%;\" src=\"images/MNIST.png\"\n\nThe dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\n\n Preparing the Data\n\nThe MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:\n\nlibrary(keras)\nmnist <- dataset_mnist()\nx_train <- mnist$train$x\ny_train <- mnist$train$y\nx_test <- mnist$test$x\ny_test <- mnist$test$y\n\nThe x data is a 3-d array (images,width,height) of grayscale values . To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:\n\nreshape\nxtrain <- arrayreshape(xtrain, c(nrow(xtrain), 784))\nxtest <- arrayreshape(xtest, c(nrow(xtest), 784))\n rescale\nxtrain <- xtrain / 255\nxtest <- xtest / 255\n\nNote that we use the array_reshape() function rather than the dim<-() function to reshape the array. This is so that the data is re-interpreted using row-major semantics (as opposed to R's default column-major semantics), which is in turn compatible with the way that the numerical libraries called by Keras interpret array dimensions.\n\nThe y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:\n\nytrain <- tocategorical(y_train, 10)\nytest <- tocategorical(y_test, 10)\n\nDefining the Model\n\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers.\n\nWe begin by creating a sequential model and then adding layers using the pipe (%%) operator:\n\nmodel <- kerasmodelsequential() \nmodel %% \n  layerdense(units = 256, activation = 'relu', inputshape = c(784)) %% \n  layer_dropout(rate = 0.4) %% \n  layer_dense(units = 128, activation = 'relu') %%\n  layer_dropout(rate = 0.3) %%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.\n\nUse the summary() function to print the details of the model:\n\nsummary(model)\n\npre style=\"box-shadow: none;\"codeModel\n____________________\nLayer (type)                        Output Shape                    Param      \n\ndense_1 (Dense)                     (None, 256)                     200960      \n____________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n____________________\ndense_2 (Dense)                     (None, 128)                     32896       \n____________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n____________________\ndense_3 (Dense)                     (None, 10)                      1290        \n\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n____________________/code/pre\n\nNext, compile the model with appropriate loss function, optimizer, and metrics:\n\nmodel %% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nTraining and Evaluation\n\nUse the fit() function to train the model for 30 epochs using batches of 128 images:\n\nhistory - model %% fit(\n  xtrain, ytrain, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nThe history object returned by fit() includes loss and accuracy metrics which we can plot:\n\nplot(history)\n\n \n\nEvaluate the model's performance on the test data:\n\nmodel %% evaluate(xtest, ytest)\n$loss\n[1] 0.1149\n\n$acc\n[1] 0.9807\n\nGenerate predictions on new data:\n\nmodel %% predictclasses(xtest)\n  [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2\n [40] 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2\n [79] 9 7 7 6 2 7 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9\n [ reached getOption(\"max.print\") -- omitted 9900 entries ]\n\nKeras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward. \n\n Deep Learning with R Book\n\nIf you want a more comprehensive introduction to both Keras and the concepts and practice of deep learning, we recommend the Deep Learning with R book from Manning. This book is a collaboration between François Chollet, the creator of Keras, and J.J. Allaire, who wrote the R interface to Keras.\n\nThe book presumes no significant knowledge of machine learning and deep learning, and goes all the way from basic theory to advanced practical applications, all using the R interface to Keras. \n\ndiv style=\"clear: both;\"/div\n\nWhy this name, Keras?\n\nKeras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n \"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\" Homer, Odyssey 19. 562 ff (Shewring translation).\n","id":6},{"path":"/guide/keras/applications","title":"\"Using Pre-Trained Models\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Pre-Trained Models\"","    identifier":"   identifier: \"keras-pre-trained-models\"","    parent":"   parent: \"keras-advanced-top\"","    weight":"   weight: 40","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nApplications\n\nKeras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n\nWeights are downloaded automatically when instantiating a model. They are stored at ~/.keras/models/.\n\nThe following image classification models (with weights trained on ImageNet) are available:\n\nXception\nVGG16\nVGG19\nResNet50\nInceptionV3\nInceptionResNetV2\nMobileNet\nMobileNetV2\nDenseNet\nNASNet\n\nAll of these architectures are compatible with all the backends (TensorFlow, Theano, and CNTK), and upon instantiation the models will be built according to the image data format set in your Keras configuration file at ~/.keras/keras.json. For instance, if you have set imagedataformat=channels_last, then any model loaded from this repository will get built according to the TensorFlow data format convention, \"Height-Width-Depth\".\n\nFor Keras < 2.2.0, The Xception model is only available for TensorFlow, due to its reliance on SeparableConvolution layers.\nFor Keras < 2.1.5, The MobileNet model is only available for TensorFlow, due to its reliance on DepthwiseConvolution layers.\n\n Usage Examples\n\nClassify ImageNet classes with ResNet50\n\n instantiate the model\nmodel <- application_resnet50(weights = 'imagenet')\n\nload the image\nimg_path <- \"elephant.jpg\"\nimg <- imageload(imgpath, target_size = c(224,224))\nx <- imagetoarray(img)\n\n ensure we have a 4d tensor with single element in the batch dimension,\nthe preprocess the input for prediction using resnet50\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenetpreprocessinput(x)\n\n make predictions then decode and print them\npreds - model %% predict(x)\nimagenetdecodepredictions(preds, top = 3)[[1]]\n\n  classname classdescription      score\n1  n02504013   Indian_elephant 0.90117526\n2  n01871265            tusker 0.08774310\n3  n02504458  African_elephant 0.01046011\n\nExtract features with VGG16\n\nmodel <- applicationvgg16(weights = 'imagenet', includetop = FALSE)\n\nimg_path <- \"elephant.jpg\"\nimg <- imageload(imgpath, target_size = c(224,224))\nx <- imagetoarray(img)\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenetpreprocessinput(x)\n\nfeatures - model %% predict(x)\n\n Extract features from an arbitrary intermediate layer with VGG19\n\nbasemodel <- applicationvgg19(weights = 'imagenet')\nmodel <- kerasmodel(inputs = basemodel$input, \n                     outputs = getlayer(basemodel, 'block4_pool')$output)\n\nimg_path <- \"elephant.jpg\"\nimg <- imageload(imgpath, target_size = c(224,224))\nx <- imagetoarray(img)\nx <- array_reshape(x, c(1, dim(x)))\nx <- imagenetpreprocessinput(x)\n\nblock4poolfeatures - model %% predict(x)\n\nFine-tune InceptionV3 on a new set of classes\n\n create the base pre-trained model\nbasemodel <- applicationinceptionv3(weights = 'imagenet', includetop = FALSE)\n\nadd our custom layers\npredictions - base_model$output %% \n  layerglobalaveragepooling2d() %% \n  layer_dense(units = 1024, activation = 'relu') %% \n  layer_dense(units = 200, activation = 'softmax')\n\n this is the model we will train\nmodel <- kerasmodel(inputs = basemodel$input, outputs = predictions)\n\nfirst: train only the top layers (which were randomly initialized)\n i.e. freeze all convolutional InceptionV3 layers\nfreezeweights(basemodel)\n\ncompile the model (should be done after setting layers to non-trainable)\nmodel %% compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')\n\n train the model on the new data for a few epochs\nmodel %% fit_generator(...)\n\nat this point, the top layers are well trained and we can start fine-tuning\n convolutional layers from inception V3. We will freeze the bottom N layers\nand train the remaining top layers.\n\n let's visualize layer names and layer indices to see how many layers\nwe should freeze:\nlayers <- base_model$layers\nfor (i in 1:length(layers))\n  cat(i, layers[[i]]$name, \"\\n\")\n\n we chose to train the top 2 inception blocks, i.e. we will freeze\nthe first 172 layers and unfreeze the rest:\nfreezeweights(basemodel, from = 1, to = 172)\nunfreezeweights(basemodel, from = 173)\n\n we need to recompile the model for these modifications to take effect\nwe use SGD with a low learning rate\nmodel %% compile(\n  optimizer = optimizer_sgd(lr = 0.0001, momentum = 0.9), \n  loss = 'categorical_crossentropy'\n)\n\n we train our model again (this time fine-tuning the top 2 inception blocks\nalongside the top Dense layers\nmodel %% fit_generator(...)\n\n Build InceptionV3 over a custom input tensor\n\nthis could also be the output a different Keras model or layer\ninputtensor <- layerinput(shape = c(224, 224, 3))\n\nmodel <- applicationinceptionV3(inputtensor = inputtensor, \n                                  weights='imagenet', \n                                  include_top = TRUE)\n\n Additional examples\n\nThe VGG16 model is the basis for the Deep dream Keras example script.\n\n","id":7},{"path":"/guide/keras/custom_layers","title":"\"Writing Custom Keras Layers\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Custom Layers\"","    identifier":"   identifier: \"keras-custom-layers\"","    parent":"   parent: \"keras-advanced-top\"","    weight":"   weight: 10","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nIf the existing Keras layers don't meet your requirements you can create a custom layer. For simple, stateless custom operations, you are probably better off using layer_lambda() layers. But for any custom operation that has trainable weights, you should implement your own layer. \n\nThe example below illustrates the skeleton of a Keras custom layer. The mnist_antirectifier example includes another demonstration of creating a custom layer.\n\nKerasLayer R6 Class\n\nTo create a custom Keras layer, you create an R6 class derived from KerasLayer. There are three methods to implement (only one of which, call(), is required for all types of layer):\n\nbuild(input_shape): This is where you will define your weights. Note that if your layer doesn't define trainable weights then you need not implemented this method.\ncall(x): This is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call: the input tensor.\ncomputeoutputshape(input_shape): In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don't modify the shape of the input then you need not implement this method.\n\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    output_dim = NULL,\n    \n    kernel = NULL,\n    \n    initialize = function(output_dim)  ,\n    \n    build = function(input_shape)  ,\n    \n    call = function(x, mask = NULL)  ,\n    \n    computeoutputshape = function(input_shape)  \n  )\n)\n\nNote that tensor operations are executed using the Keras backend(). See the Keras Backend article for details on the various functions available from Keras backends.\n\n Layer Wrapper Function\n\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\ndefine layer wrapper function\nlayercustom <- function(object, outputdim, name = NULL, trainable = TRUE)  \n\n use it in a model\nmodel <- kerasmodelsequential()\nmodel %% \n  layerdense(units = 32, inputshape = c(32,32)) %% \n  layercustom(outputdim = 32)\n\nSome important things to note about the layer wrapper function:\n\n1) It accepts object as its first parameter (the object will either be a Keras sequential model or another Keras layer). The object parameter enables the layer to be composed with other layers using the magrittr pipe (%%) operator.\n\n2) It converts it's outputdim to integer using the as.integer() function. This is done as convenience to the user because Keras variables are strongly typed (you can't pass a float if an integer is expected). This enables users of the function to write outputdim = 32 rather than output_dim = 32L.\n\n3) Some additional parameters not used by the layer (name and trainable) are in the function signature. Custom layer functions can include any of the core layer function arguments (input_shape,\nbatchinputshape, batch_size, dtype, name, trainable, and weights) and they will be automatically forwarded to the Layer base class.\n\nSee the mnist_antirectifier example for another demonstration of creating a custom layer.\n\n","id":8},{"path":"/guide/keras/custom_models","title":"\"Writing Custom Keras Models\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Custom Models\"","    identifier":"   identifier: \"keras-custom-models\"","    parent":"   parent: \"keras-advanced-top\"","    weight":"   weight: 20","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nOverview\n\nIn addition to sequential models and models created with the functional API, you may also define models by defining a custom call() (forward pass) operation. \n\nTo create a custom Keras model, you call the kerasmodelcustom() function, passing it an R function which in turn returns another R function that implements the custom call() (forward pass) operation. The R function you pass takes a model argument, which provides access to the underlying Keras model object should you need it. \n\nTypically, you'll wrap your call to kerasmodelcustom() in yet another function that enables callers to easily instantiate your custom model.\n\n Creating a Custom Model\n\nThis example demonstrates the implementation of a simple custom model that implements a multi-layer-perceptron with optional dropout and batch normalization:\n\nlibrary(keras)\n\nkerasmodelsimplemlp <- function(numclasses, \n                                   usebn = FALSE, usedp = FALSE, \n                                   name = NULL)  \n   )\n \n\nNote that we include a name parameter so that users can optionally provide a human readable name for the model.\n\nNote also that when we create layers to be used in our forward pass we set them onto the self object so they are tracked appropriately by Keras.\n\nIn call(), you may specify custom losses by calling self$add_loss(). You can also access any other members of the Keras model you need (or even add fields to the model) by using self$.\n\nUsing a Custom Model\n\nTo use a custom model, just call your model's high-level wrapper function. For example:\n\nlibrary(keras)\n\n create the model \nmodel <- kerasmodelsimplemlp(numclasses = 10, use_dp = TRUE)\n\ncompile graph\nmodel %% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\n Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)\n\nConvert labels to categorical one-hot encoding\nonehotlabels <- tocategorical(labels, numclasses = 10)\n\n Train the model\nmodel %% fit(data, onehotlabels, epochs=10, batch_size=32)\n\n","id":9},{"path":"/guide/keras/examples/_","title":"\"Keras Examples\"","menu":"menu:","  main":" main:","    name":"   name: \"Examples\"","    parent":"   parent: \"guide-top\"","    weight":"   weight: 25","content":"\n| Example  | Description   |\n|----------------------|------------------------------------|\n| addition_rnn | Implementation of sequence to sequence learning for performing addition of two numbers (as strings). |\n| babi_memnn | Trains a memory network on the bAbI dataset for reading comprehension. |\n| babi_rnn | Trains a two-branch recurrent network on the bAbI dataset for reading comprehension. |\n| cifar10_cnn | Trains a simple deep CNN on the CIFAR10 small images dataset. |\n| cifar10_densenet | Trains a DenseNet-40-12 on the CIFAR10 small images dataset. |\n| conv_lstm | Demonstrates the use of a convolutional LSTM network. |\n| deep_dream | Deep Dreams in Keras. |\n| eager_dcgan | Generating digits with generative adversarial networks and eager execution. |\n| eagerimagecaptioning | Generating image captions with Keras and eager execution. |\n| eager_pix2pix | Image-to-image translation with Pix2Pix, using eager execution. |\n| eager_styletransfer | Neural style transfer with eager execution. |\n| fine_tuning | Fine tuning of a image classification model. | \n| imdbbidirectionallstm | Trains a Bidirectional LSTM on the IMDB sentiment classification task. |\n| imdb_cnn | Demonstrates the use of Convolution1D for text classification. |\n| imdbcnnlstm | Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task. |\n| imdb_fasttext | Trains a FastText model on the IMDB sentiment classification task. |\n| imdb_lstm | Trains a LSTM on the IMDB sentiment classification task. |\n| lstmtextgeneration | Generates text from Nietzsche's writings. |\n| lstm_seq2seq | This script demonstrates how to implement a basic character-level sequence-to-sequence model. |\n| mnist_acgan | Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset |\n| mnist_antirectifier | Demonstrates how to write custom layers for Keras |\n| mnist_cnn | Trains a simple convnet on the MNIST dataset. |\n| mnistcnnembeddings | Demonstrates how to visualize embeddings in TensorBoard. |\n| mnist_irnn | Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" by Le et al. |\n| mnist_mlp | Trains a simple deep multi-layer perceptron on the MNIST dataset. |\n| mnisthierarchicalrnn | Trains a Hierarchical RNN (HRNN) to classify MNIST digits. |\n| mnist_tfrecord | MNIST dataset with TFRecords, the standard TensorFlow data format. |\n| mnisttransfercnn | Transfer learning toy example. |\n| neuralstyletransfer | Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture). |\n| nmt_attention | Neural machine translation with an attention mechanism. |\n| quorasiameselstm | Classifying duplicate quesitons from Quora using Siamese Recurrent Architecture. | \n| reuters_mlp | Trains and evaluatea a simple MLP on the Reuters newswire topic classification task. |\n| stateful_lstm | Demonstrates how to use stateful RNNs to model long sequences efficiently. |\n| textexplanationlime | How to use lime to explain text data. |\n| variational_autoencoder | Demonstrates how to build a variational autoencoder. |\n| variationalautoencoderdeconv | Demonstrates how to build a variational autoencoder with Keras using deconvolution layers. |\n| tfprob_vae | A variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST. |\n| vq_vae | Discrete Representation Learning with VQ-VAE and TensorFlow Probability. |\n\n<!--\n| cifar10_resnet | Trains a ResNet on the CIFAR10 dataset. |\n| convfiltervisualization | Visualization of the filters of VGG16, via gradient ascent in input space. |\n| image_ocr | Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). |\n| lstm_benchmark | Compares different LSTM implementations on the IMDB sentiment classification task. |\n| mnistdatasetapi | MNIST classification with TensorFlow's Dataset API. |\n| mnist_net2net | Reproduction of the Net2Net experiment with MNIST in \"Net2Net: Accelerating Learning via Knowledge Transfer\". |\n| mnistsiamesegraph | Trains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. |\n| mnist_swwae | Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. |\n| neural_doodle | Neural doodle. |\n| reutersmlpreluvsselu | Compares self-normalizing MLPs with regular MLPs.  |\n| lstm_stateful | Example script showing how to use a stateful LSTM model\n+and how its stateless counterpart performs. |\n--\n\n<!--\n\nThese examples are complete however don't yet work properly (see inline comments in \nscripts for details) so we aren't listing them.\n\n| pretrainedwordembeddings | Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. |\n\n--\n\n","id":10},{"path":"/guide/keras/examples/addition_rnn","title":"addition_rnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":11},{"path":"/guide/keras/examples/babi_memnn","title":"babi_memnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":12},{"path":"/guide/keras/examples/babi_rnn","title":"babi_rnn","type":"docs","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":13},{"path":"/guide/keras/examples/cifar10_cnn","title":"cifar10_cnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":14},{"path":"/guide/keras/examples/cifar10_densenet","title":"cifar10_densenet","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":15},{"path":"/guide/keras/examples/cifar10_resnet","title":"cifar10_resnet","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":16},{"path":"/guide/keras/examples/conv_filter_visualization","title":"conv_filter_visualization","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":17},{"path":"/guide/keras/examples/conv_lstm","title":"conv_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":18},{"path":"/guide/keras/examples/deep_dream","title":"deep_dream","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":19},{"path":"/guide/keras/examples/eager_cvae","title":"eager_cvae","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":20},{"path":"/guide/keras/examples/eager_dcgan","title":"eager_dcgan","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":21},{"path":"/guide/keras/examples/eager_image_captioning","title":"eager_image_captioning","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":22},{"path":"/guide/keras/examples/eager_pix2pix","title":"eager_pix2pix","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":23},{"path":"/guide/keras/examples/eager_styletransfer","title":"eager_styletransfer","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":24},{"path":"/guide/keras/examples/fine_tuning","title":"fine_tuning","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":25},{"path":"/guide/keras/examples/image_ocr","title":"image_ocr","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":26},{"path":"/guide/keras/examples/imdb_bidirectional_lstm","title":"imdb_bidirectional_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":27},{"path":"/guide/keras/examples/imdb_cnn","title":"imdb_cnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":28},{"path":"/guide/keras/examples/imdb_cnn_lstm","title":"imdb_cnn_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":29},{"path":"/guide/keras/examples/imdb_fasttext","title":"imdb_fasttext","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":30},{"path":"/guide/keras/examples/imdb_lstm","title":"imdb_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":31},{"path":"/guide/keras/examples/lstm_benchmark","title":"lstm_benchmark","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":32},{"path":"/guide/keras/examples/lstm_seq2seq","title":"lstm_seq2seq","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":33},{"path":"/guide/keras/examples/lstm_stateful","title":"lstm_stateful","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":34},{"path":"/guide/keras/examples/lstm_text_generation","title":"lstm_text_generation","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":35},{"path":"/guide/keras/examples/mmd_cvae","title":"mmd_cvae","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":36},{"path":"/guide/keras/examples/mnist_acgan","title":"mnist_acgan","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":37},{"path":"/guide/keras/examples/mnist_antirectifier","title":"mnist_antirectifier","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":38},{"path":"/guide/keras/examples/mnist_cnn","title":"mnist_cnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":39},{"path":"/guide/keras/examples/mnist_cnn_embeddings","title":"mnist_cnn_embeddings","type":"docs","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":40},{"path":"/guide/keras/examples/mnist_dataset_api","title":"mnist_dataset_api","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":41},{"path":"/guide/keras/examples/mnist_hierarchical_rnn","title":"mnist_hierarchical_rnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":42},{"path":"/guide/keras/examples/mnist_irnn","title":"mnist_irnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":43},{"path":"/guide/keras/examples/mnist_mlp","title":"mnist_mlp","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":44},{"path":"/guide/keras/examples/mnist_net2net","title":"mnist_net2net","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":45},{"path":"/guide/keras/examples/mnist_siamese_graph","title":"mnist_siamese_graph","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":46},{"path":"/guide/keras/examples/mnist_swwae","title":"mnist_swwae","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":47},{"path":"/guide/keras/examples/mnist_tfrecord","title":"mnist_tfrecord","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n\n","id":48},{"path":"/guide/keras/examples/mnist_transfer_cnn","title":"mnist_transfer_cnn","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":49},{"path":"/guide/keras/examples/neural_style_transfer","title":"neural_style_transfer","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":50},{"path":"/guide/keras/examples/nmt_attention","title":"nmt_attention","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":51},{"path":"/guide/keras/examples/nueral_doodle","title":"nueral_doodle","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":52},{"path":"/guide/keras/examples/pretrained_word_embeddings","title":"pretrained_word_embeddings","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":53},{"path":"/guide/keras/examples/quora_siamese_lstm","title":"quora_siamese_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":54},{"path":"/guide/keras/examples/reuters_mlp","title":"reuters_mlp","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":55},{"path":"/guide/keras/examples/reuters_mlp_relu_vs_selu","title":"reuters_mlp_relu_vs_selu","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":56},{"path":"/guide/keras/examples/stateful_lstm","title":"stateful_lstm","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":57},{"path":"/guide/keras/examples/text_explanation_lime","title":"text_explanation_lime","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n`","id":58},{"path":"/guide/keras/examples/tfprob_vae","title":"tfprob_vae","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":59},{"path":"/guide/keras/examples/unet","title":"unet","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n`","id":60},{"path":"/guide/keras/examples/unet_linux","title":"unet_linux","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":61},{"path":"/guide/keras/examples/variational_autoencoder","title":"variational_autoencoder","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":62},{"path":"/guide/keras/examples/variational_autoencoder_deconv","title":"variational_autoencoder_deconv","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":63},{"path":"/guide/keras/examples/vq_vae","title":"vq_vae","type":"docs","repo":"https://github.com/rstudio/keras","content":"\ndiv class=\"source-ref\"\nspan class=\"caption\"Source: /spanr sprintf(\"https://github.com/rstudio/keras/blob/master/vignettes/examples/%s.R\", rmarkdown::metadata$title)\n/div\n\nknitr::opts_chunk$set(eval = FALSE)\nknitr::spin_child(paste0(rmarkdown::metadata$title, \".R\"))\n","id":64},{"path":"/guide/keras/faq","title":"\"Frequently Asked Questions\"","output":"","  rmarkdown":" rmarkdown::html_vignette: default","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Frequently Asked Questions\"","    identifier":"   identifier: \"keras-faq\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 40","content":"\nlibrary(keras)\nknitr::opts_chunk$set(eval = FALSE)\n\nHow should I cite Keras?\n\nPlease cite Keras in your publications if it helps your research. Here is an example BibTeX entry:\n\n@misc ,\n  author= ois and Allaire, JJ and others ,\n  year= ,\n  publisher= ,\n  howpublished=  ,\n \n\n How can I run Keras on a GPU?\n\nNote that installation and configuration of the GPU-based backends can take considerably more time and effort. So if you are just getting started with Keras you may want to stick with the CPU version initially, then install the appropriate GPU version once your training becomes more computationally demanding.\n\nBelow are instructions for installing and enabling GPU support for the various supported backends.\n\nTensorFlow\n\nIf your system has an NVIDIA® GPU and you have the GPU version of TensorFlow installed then your Keras code will automatically run on the GPU.\n\nAdditional details on GPU installation can be found here: https://tensorflow.rstudio.com/installation_gpu.html.\n\n Theano\n\nIf you are running on the Theano backend, you can set the THEANO_FLAGS environment variable to indicate you'd like to execute tensor operations on the GPU. For example:\n\nSys.setenv(KERAS_BACKEND = \"keras\")\nSys.setenv(THEANO_FLAGS = \"device=gpu,floatX=float32\")\nlibrary(keras)\n\nThe name 'gpu' might have to be changed depending on your device's identifier (e.g. gpu0, gpu1, etc).\n\nCNTK\n\nIf you have the GPU version of CNTK installed then your Keras code will automatically run on the GPU.\n\nAdditional information on installing the GPU version of CNTK can be found here: https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python\n\n How can I run a Keras model on multiple GPUs?\n\nWe recommend doing so using the TensorFlow backend. There are two ways to run a single model on multiple GPUs: data parallelism and device parallelism.\n\nIn most cases, what you need is most likely data parallelism.\n\nData parallelism\n\nData parallelism consists in replicating the target model once on each device, and using each replica to process a different fraction of the input data.\nKeras has a built-in utility, multigpumodel(), which can produce a data-parallel version of any model, and achieves quasi-linear speedup on up to 8 GPUs.\n\nFor more information, see the documentation for multigpumodel. Here is a quick example:\n\n Replicates model on 8 GPUs.\nThis assumes that your machine has 8 available GPUs.\nparallelmodel <- multigpu_model(model, gpus=8)\nparallel_model %% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = \"rmsprop\"\n)\n\n This fit call will be distributed on 8 GPUs.\nSince the batch size is 256, each GPU will process 32 samples.\nparallelmodel %% fit(x, y, epochs = 20, batchsize = 256)\n\n Device parallelism\n\nDevice parallelism consists in running different parts of a same model on different devices. It works best for models that have a parallel architecture, e.g. a model with two branches.\n\nThis can be achieved by using TensorFlow device scopes. Here is a quick example:\n\nModel where a shared LSTM is used to encode two different sequences in parallel\ninputa <- layerinput(shape = c(140, 256))\ninputb <- layerinput(shape = c(140, 256))\n\nsharedlstm <- layerlstm(units = 64)\n\n Process the first sequence on one GPU\nlibrary(tensorflow)\nwith(tf$device_scope(\"/gpu:0\",  ):\n    \nProcess the next sequence on another GPU\nwith(tf$device_scope(\"/gpu:1\",  ):\n\n Concatenate results on CPU\nwith(tf$device_scope(\"/cpu:0\",  ):\n\nWhat does \"sample\", \"batch\", \"epoch\" mean?\n\nBelow are some common definitions that are necessary to know and understand to correctly utilize Keras:\n\nSample: one element of a dataset.\n  Example: one image is a sample in a convolutional network\n  Example: one audio file is a sample for a speech recognition model\nBatch: a set of N* samples. The samples in a *batch** are processed independently, in parallel. If training, a batch results in only one update to the model.\n  A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\nEpoch: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n  When using evaluationdata or evaluationsplit with the fit method of Keras models, evaluation will be run at the end of every epoch.\n  Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n\n Why are Keras objects modified in place?\n\nUnlike most R objects, Keras objects are \"mutable\". That means that when you modify an object you're modifying it \"in place\", and you don't need to assign the updated object back to the original name. For example, to add layers to a Keras model you might use this code:\n\nmodel %% \n  layerdense(units = 32, activation = 'relu', inputshape = c(784)) %% \n  layer_dense(units = 10, activation = 'softmax')\n\nRather than this code:\n\nmodel - model %% \n  layerdense(units = 32, activation = 'relu', inputshape = c(784)) %% \n  layer_dense(units = 10, activation = 'softmax')\n\nYou need to be aware of this because it makes the Keras API a little different than most other pipelines you may have used, but it's necessary to match the data structures and behavior of the underlying Keras library.\n\nHow can I save a Keras model?\n\n Saving/loading whole models (architecture + weights + optimizer state)\n\nYou can use savemodelhdf5() to save a Keras model into a single HDF5 file which will contain:\n\nthe architecture of the model, allowing to re-create the model\nthe weights of the model\nthe training configuration (loss, optimizer)\nthe state of the optimizer, allowing to resume training exactly where you left off.\n\nYou can then use loadmodelhdf5() to reinstantiate your model.\nloadmodelhdf5() will also take care of compiling the model using the saved training configuration\n(unless the model was never compiled in the first place).\n\nExample:\n\nsavemodelhdf5(model, 'my_model.h5')\nmodel <- loadmodelhdf5('my_model.h5')\n\nSaving/loading only a model's architecture\n\nIf you only need to save the architecture of a model, and not its weights or its training configuration, you can do:\n\njsonstring <- modelto_json(model)\nyamlstring <- modelto_yaml(model)\n\nThe generated JSON / YAML files are human-readable and can be manually edited if needed.\n\nYou can then build a fresh model from this data:\n\nmodel <- modelfromjson(json_string)\nmodel <- modelfromyaml(yaml_string)\n\n Saving/loading only a model's weights\n\nIf you need to save the weights of a model, you can do so in HDF5 with the code below.\n\nsavemodelweightshdf5('mymodel_weights.h5')\n\nAssuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture:\n\nmodel %% loadmodelweightshdf5('mymodel_weights.h5')\n\nIf you need to load weights into a different architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by layer name:\n\nmodel %% loadmodelweightshdf5('mymodelweights.h5', byname = TRUE)\n\nFor example:\n\nassuming the original model looks like this:\n   model <- kerasmodelsequential()\nmodel %% \n     layerdense(units = 2, inputdim = 3, name = \"dense 1\") %% \nlayerdense(units = 3, name = \"dense3\") %% \n     ...\nsavemodelweights(model, fname)\n\n new model\nmodel <- kerasmodelsequential()\nmodel %% \n  layerdense(units = 2, inputdim = 3, name = \"dense 1\") %%  # will be loaded\n  layerdense(units = 3, name = \"dense3\")                     # will not be loaded\n\nload weights from first model; will only affect the first layer, dense_1.\nloadmodelweights(fname, by_name = TRUE)\n\n Why is the training loss much higher than the testing loss?\n\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\n\nBesides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n\nHow can I obtain the output of an intermediate layer?\n\nOne simple way is to create a new Model that will output the layers that you are interested in:\n\nmodel <- ...   create the original model\n\nlayername <- 'mylayer'\nintermediatelayermodel <- keras_model(inputs = model$input,\n                                        outputs = getlayer(model, layername)$output)\nintermediateoutput <- predict(intermediatelayer_model, data)\n\nHow can I use Keras with datasets that don't fit in memory?\n\n Generator Functions\n\nTo provide training or evaluation data incrementally you can write an R generator function that yields batches of training data then pass the function to the fitgenerator() function (or related functions evaluategenerator() and predict_generator().\n\nThe output of generator functions must be a list of one of these forms:\n\n   (inputs, targets)\n   (inputs, targets, sample_weights)\n     \nAll arrays should contain the same number of samples. The generator is expected to loop over its data indefinitely. For example, here's simple generator function that yields randomly sampled batches of data:\n\nsamplinggenerator <- function(Xdata, Ydata, batchsize)  \n \n\nmodel %% \n  fitgenerator(samplinggenerator(Xtrain, Ytrain, batch_size = 128), \n                stepsperepoch = nrow(X_train) / 128, epochs = 10)\n  \n\nThe stepsperepoch parameter indicates the number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of unique samples if your dataset divided by the batch size.\n\nExternal Data Generators\n\nThe above example doesn't however address the use case of datasets that don't fit in memory. Typically to do that you'll write a generator that reads from another source (e.g. a sparse matrix or file(s) on disk) and maintains an offset into that data as it's called repeatedly. For example, imagine you have a set of text files in a directory you want to read from:\n\ndatafilesgenerator <- function(dir)  \n \n\nThe above function is an example of a stateful generator---the function maintains information across calls to keep track of which data to provide next. This is accomplished by defining shared state outside the generator function body and using the <<- operator to assign to it from within the generator.\n\nImage Generators\n\nYou can also use the flowimagesfromdirectory() and flowimagesfromdata() functions along with fitgenerator() for training on sets of images stored on disk (with optional image augmentation/normalization via imagedata_generator()).\n\nYou can see batch image training in action in our CIFAR10 example.\n\n Batch Functions\n\nYou can also do batch training using the trainonbatch() and testonbatch() functions. These functions enable you to write a training loop that reads into memory only the data required for each batch.\n\nHow can I interrupt training when the validation loss isn't decreasing anymore?\n\nYou can use an early stopping callback:\n\nearlystopping <- callbackearlystopping(monitor = 'valloss', patience = 2)\nmodel %% fit(X, y, validationsplit = 0.2, callbacks = c(earlystopping))\n\nFind out more in the callbacks documentation.\n\n How is the validation split computed?\n\nIf you set the validation_split argument in fit to e.g. 0.1, then the validation data used will be the last 10% of the data. If you set it to 0.25, it will be the last 25% of the data, etc. Note that the data isn't shuffled before extracting the validation split, so the validation is literally just the last x% of samples in the input you passed.\n\nThe same validation set is used for all epochs (within a same call to fit).\n\nIs the data shuffled during training?\n\nYes, if the shuffle argument in fit is set to TRUE (which is the default), the training data will be randomly shuffled at each epoch.\n\nValidation data is never shuffled.\n\n How can I record the training / validation loss / accuracy at each epoch?\n\nThe model.fit method returns an History callback, which has a history attribute containing the lists of successive losses and other metrics.\n\nhist - model %% fit(X, y, validation_split=0.2)\nhist$history\n\nHow can I \"freeze\" Keras layers?\n\nTo \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.\n\nYou can pass a trainable argument (boolean) to a layer constructor to set a layer to be non-trainable:\n\nfrozenlayer <- layerdense(units = 32, trainable = FALSE)\n\nAdditionally, you can set the trainable property of a layer to TRUE or FALSE after instantiation. For this to take effect, you will need to call compile() on your model after modifying the trainable property. Here's an example:\n\nx <- layer_input(shape = c(32))\nlayer <- layer_dense(units = 32)\nlayer$trainable <- FALSE\ny - x %% layer\n\nfrozenmodel <- kerasmodel(x, y)\n in the model below, the weights of layer will not be updated during training\nfrozen_model %% compile(optimizer = 'rmsprop', loss = 'mse')\n\nlayer$trainable <- TRUE\ntrainablemodel <- kerasmodel(x, y)\nwith this model the weights of the layer will be updated during training\n (which will also affect the above model since it uses the same layer instance)\ntrainable_model %% compile(optimizer = 'rmsprop', loss = 'mse')\n\nfrozen_model %% fit(data, labels)  # this does NOT update the weights of layer\ntrainable_model %% fit(data, labels)  # this updates the weights of layer\n\nFinally, you can freeze or unfreeze the weights for an entire model (or a range of layers within the model) using the freezeweights() and unfreezeweights() functions. For example:\n\ninstantiate a VGG16 model\nconvbase <- applicationvgg16(\n  weights = \"imagenet\",\n  include_top = FALSE,\n  input_shape = c(150, 150, 3)\n)\n\n freeze it's weights\nfreezeweights(convbase)\n\ncreate a composite model that includes the base + more layers\nmodel - kerasmodelsequential() %% \n  conv_base %% \n  layer_flatten() %% \n  layer_dense(units = 256, activation = \"relu\") %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\n compile\nmodel %% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nunfreeze weights from \"block5_conv1\" on\nunfreezeweights(convbase, from = \"block5_conv1\")\n\n compile again since we froze or unfroze layers\nmodel %% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizer_rmsprop(lr = 2e-5),\n  metrics = c(\"accuracy\")\n)\n\nHow can I use stateful RNNs?\n\nMaking a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n\nWhen using stateful RNNs, it is therefore assumed that:\n\nall batches have the same number of samples\nIf X1 and X2 are successive batches of samples, then X2[[i]] is the follow-up sequence to X1[[i], for every i.\n\nTo use statefulness in RNNs, you need to:\n\nexplicitly specify the batch size you are using, by passing a batchsize argument to the first layer in your model. E.g. batchsize=32 for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.\nset stateful=TRUE in your RNN layer(s).\nspecify shuffle=FALSE when calling fit().\n\nTo reset the states accumulated in either a single layer or an entire model use the reset_states() function.\n\nNotes that the methods predict(), fit(), trainonbatch(), predict_classes(), etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.\n\n How can I remove a layer from a Sequential model?\n\nYou can remove the last added layer in a Sequential model by calling pop_layer():\n\nmodel <- kerasmodelsequential()\nmodel %% \n  layerdense(units = 32, activation = 'relu', inputshape = c(784)) %% \n  layer_dense(units = 32, activation = 'relu') %% \n  layer_dense(units = 32, activation = 'relu')\n\nlength(model$layers)     # \"3\"\nmodel %% pop_layer()\nlength(model$layers)     # \"2\"\n\nHow can I use pre-trained models in Keras?\n\nCode and pre-trained weights are available for the following image classification models:\n\nXception\nVGG16\nVGG19\nResNet50\nInceptionV3\nInceptionResNetV2\nMobileNet\nMobileNetV2\nDenseNet\nNASNet\n\nFor example:\n\nmodel <- applicationvgg16(weights = 'imagenet', includetop = TRUE)\n\nFor a few simple usage examples, see the documentation for the Applications module.\n\nThe VGG16 model is also the basis for the Deep dream Keras example script.\n\n How can I use other Keras backends?\n\nBy default the Keras Python and R packages use the TensorFlow backend. Other available backends include Theano or CNTK. To learn more about using alternatate backends (e.g. Theano or CNTK) see the article on Keras backends.\n\nHow can I use the PlaidML backend?\n\nPlaidML is an open source portable deep learning engine that runs on most existing PC hardware with OpenCL-capable GPUs from NVIDIA, AMD, or Intel. PlaidML includes a Keras backend which you can use as described below.\n\nFirst, build and install PlaidML as described on the project website. You must be sure that PlaidML is correctly installed, setup, and working before proceeding further!\n\nThen, to use Keras with the PlaidML backend you do the following:\n\nlibrary(keras)\nuse_backend(\"plaidml\")\n\nThis should automatically discover and use the Python environment where plaidml and plaidml-keras were installed. If this doesn't work as expected you can also force the selection of a particular Python environment. For example, if you installed PlaidML in conda environment named \"plaidml\" you would do this:\n\nlibrary(keras)\nuse_condaenv(\"plaidml\") \nuse_backend(\"plaidml\")\n\n How can I use Keras in another R package?\n\nTesting on CRAN\n\nThe main consideration in using Keras within another R package is to ensure that your package can be tested in an environment where Keras is not available (e.g. the CRAN test servers). To do this, arrange for your tests to be skipped when Keras isn't available using the iskerasavailable() function. \n\nFor example, here's a testthat utility function that can be used to skip a test when Keras isn't available:\n\n testthat utilty for skipping tests when Keras isn't available\nskipifno_keras <- function(version = NULL)  \n\nuse the function within a test\ntest_that(\"keras function works correctly\",  )\n\nYou can pass the version argument to check for a specific version of Keras.\n\nKeras Module\n\nAnother consideration is gaining access to the underlying Keras python module. You might need to do this if you require lower level access to Keras than is provided for by the Keras R package. \n\nSince the Keras R package can bind to multiple different implementations of Keras (either the original Keras or the TensorFlow implementation of Keras), you should use the keras::implementation() function to obtain access to the correct python module. You can use this function within the .onLoad function of a package to provide global access to the module within your package. For example:\n\n Keras python module\nkeras <- NULL\n\nObtain a reference to the module from the keras R package\n.onLoad <- function(libname, pkgname)  \n\n Custom Layers\n\nIf you create custom layers in R or import other Python packages which include custom Keras layers, be sure to wrap them using the create_layer() function so that they are composable using the magrittr pipe operator. See the documentation on layer wrapper functions for additional details.\n\nHow can I obtain reproducible results using Keras during development?\n\nDuring development of a model, sometimes it is useful to be able to obtain reproducible results from run to run in order to determine if a change in performance is due to an actual model or data modification, or merely a result of a new random sample.  \n\nThe usesessionwith_seed() function establishes a common random seed for R, Python, NumPy, and TensorFlow. It furthermore disables hash randomization, GPU computations, and CPU parallelization, which can be additional sources of non-reproducibility. \n\nTo use the function, call it immediately after you load the keras package:\n\nlibrary(keras)\nusesessionwith_seed(42)\n\n ...rest of code follows...\n\nThis function takes all measures known to promote reproducible results from Keras sessions, however it's possible that various individual features or libraries used by the backend escape its effects. If you encounter non-reproducible results please investigate the possible sources of the problem. The source code for usesessionwith_seed() is here: https://github.com/rstudio/tensorflow/blob/master/R/seed.R. Contributions via pull request are very welcome!\n\nPlease note again that usesessionwithseed() disables GPU computations and CPU parallelization by default (as both can lead to non-deterministic computations) so should generally not be used when model training time is paramount. You can re-enable GPU computations and/or CPU parallelism using the disablegpu and disableparallelcpu arguments. For example:\n\nlibrary(keras)\nusesessionwithseed(42, disablegpu = FALSE, disableparallelcpu = FALSE)\n\nWhere is the Keras configuration filed stored?\n\nThe default directory where all Keras data is stored is:\n\n~/.keras/\n\nIn case Keras cannot create the above directory (e.g. due to permission issues), /tmp/.keras/ is used as a backup.\n\nThe Keras configuration file is a JSON file stored at $HOME/.keras/keras.json. The default configuration file looks like this:\n\n \n\nIt contains the following fields:\n\nThe image data format to be used as default by image processing layers and utilities (either channelslast or channelsfirst).\nThe epsilon numerical fuzz factor to be used to prevent division by zero in some operations.\nThe default float data type.\nThe default backend (this will always be \"tensorflow\" in the R interface to Keras)\n\nLikewise, cached dataset files, such as those downloaded with get_file(), are stored by default in $HOME/.keras/datasets/.\n","id":65},{"path":"/guide/keras/functional_api","title":"\"Guide to the Functional API\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Functional API\"","    identifier":"   identifier: \"keras-functional-api\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 20","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nThe Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n\nThis guide assumes that you are already familiar with the Sequential model.\n\nLet's start with something simple.\n\nFirst example: a densely-connected network\n\nThe Sequential model is probably a better choice to implement such a network, but it helps to start with something really simple.\n\nTo use the functional API, build your input and output layers and then pass them to the model() function. This model can be trained just like Keras sequential models.\n\nlibrary(keras)\n\n input layer\ninputs <- layer_input(shape = c(784))\n \noutputs compose input + dense layers\npredictions - inputs %%\n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dense(units = 10, activation = 'softmax')\n\n create and compile model\nmodel <- keras_model(inputs = inputs, outputs = predictions)\nmodel %% compile(\n  optimizer = 'rmsprop',\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\nNote that Keras objects are modified in place which is why it's not necessary for model to be assigned back to after it is compiled.\n\nAll models are callable, just like layers\n\nWith the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer. Note that you aren't just reusing the architecture of the model, you are also reusing its weights.\n\nx <- layer_input(shape = c(784))\n This works, and returns the 10-way softmax we defined above.\ny - x %% model\n\nThis can allow, for instance, to quickly create models that can process sequences of inputs. You could turn an image classification model into a video classification model, in just one line:\n\nInput tensor for sequences of 20 timesteps,\n each containing a 784-dimensional vector\ninputsequences <- layerinput(shape = c(20, 784))\n\nThis applies our previous model to the input sequence\nprocessedsequences - inputsequences %%\n  time_distributed(model)\n\n Multi-input and multi-output models\n\nHere's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n\nLet's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\n\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n\nHere's what our model looks like:\n\nimg src=\"../images/multi-input-multi-output-graph.png\" alt=\"multi-input-multi-output-graph\" style=\"width: 400px;\"/\n\nLet's implement it with the functional API.\n\nThe main input will receive the headline, as a sequence of integers (each integer encodes a word).\nThe integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.\n\nWe'll include an \n\nlibrary(keras)\n\nmaininput <- layerinput(shape = c(100), dtype = 'int32', name = 'main_input')\n\nlstmout - maininput %% \n  layerembedding(inputdim = 10000, outputdim = 512, inputlength = 100) %% \n  layer_lstm(units = 32)\n\nHere we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model:\n\nauxiliaryoutput - lstmout %% \n  layerdense(units = 1, activation = 'sigmoid', name = 'auxoutput')\n\nAt this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output, stacking a deep densely-connected network on top and adding the main logistic regression layer\n\nauxiliaryinput <- layerinput(shape = c(5), name = 'aux_input')\n\nmainoutput - layerconcatenate(c(lstmout, auxiliaryinput)) %%  \n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dense(units = 64, activation = 'relu') %% \n  layerdense(units = 1, activation = 'sigmoid', name = 'mainoutput')\n\nThis defines a model with two inputs and two outputs:\n\nmodel <- keras_model(\n  inputs = c(maininput, auxiliaryinput), \n  outputs = c(mainoutput, auxiliaryoutput)\n)\n\nsummary(model)\n\nModel\n________________________\nLayer (type)                 Output Shape        Param #    Connected to                  \n\nmain_input (InputLayer)      (None, 100)         0                                        \n________________________\nembedding_1 (Embedding)      (None, 100, 512)    5120000                                  \n________________________\nlstm_1 (LSTM)                (None, 32)          69760                                    \n________________________\naux_input (InputLayer)       (None, 5)           0                                        \n________________________\nconcatenate_1 (Concatenate)  (None, 37)          0                                        \n________________________\ndense_1 (Dense)              (None, 64)          2432                                     \n________________________\ndense_2 (Dense)              (None, 64)          4160                                     \n________________________\ndense_3 (Dense)              (None, 64)          4160                                     \n________________________\nmain_output (Dense)          (None, 1)           65                                       \n________________________\naux_output (Dense)           (None, 1)           33                                       \n\nTotal params: 5,200,610\nTrainable params: 5,200,610\nNon-trainable params: 0\n________________________\n\nWe compile the model and assign a weight of 0.2 to the auxiliary loss.\nTo specify different loss_weights or loss for each different output, you can use a list or a dictionary.\nHere we pass a single loss as the loss argument, so the same loss will be used on all outputs.\n\nmodel %% compile(\n  optimizer = 'rmsprop',\n  loss = 'binary_crossentropy',\n  loss_weights = c(1.0, 0.2)\n)\n\nWe can train the model by passing it lists of input arrays and target arrays:\n\nmodel %% fit(\n  x = list(headlinedata, additionaldata),\n  y = list(labels, labels),\n  epochs = 50,\n  batch_size = 32\n)\n\nSince our inputs and outputs are named (we passed them a \"name\" argument),\nWe could also have compiled the model via:\n\nmodel %% compile(\n  optimizer = 'rmsprop',\n  loss = list(mainoutput = 'binarycrossentropy', auxoutput = 'binarycrossentropy'),\n  lossweights = list(mainoutput = 1.0, aux_output = 0.2)\n)\n\nAnd trained it via:\nmodel %% fit(\n  x = list(maininput = headlinedata, auxinput = additionaldata),\n  y = list(mainoutput = labels, auxoutput = labels),\n  epochs = 50,\n  batch_size = 32\n)\n\n Shared layers\n\nAnother good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n\nLet's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\n\nOne way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and then adds a logistic regression; this outputs a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\n\nBecause the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\n\nLet's build this with the functional API. We will take as input for a tweet a binary matrix of shape (280, 256), i.e. a sequence of 280 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).\n\nlibrary(keras)\n\ntweeta <- layerinput(shape = c(280, 256))\ntweetb <- layerinput(shape = c(280, 256))\n\nTo share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:\n\nThis layer can take as input a matrix and will return a vector of size 64\nsharedlstm <- layerlstm(units = 64)\n\n When we reuse the same layer instance multiple times, the weights of the layer are also\nbeing reused (it is effectively the same layer)\nencodeda - tweeta %% shared_lstm\nencodedb - tweetb %% shared_lstm\n\n We can then concatenate the two vectors and add a logistic regression on top\npredictions - layerconcatenate(c(encodeda, encoded_b), axis=-1) %% \n  layer_dense(units = 1, activation = 'sigmoid')\n\nWe define a trainable model linking the tweet inputs to the predictions\nmodel <- kerasmodel(inputs = c(tweeta, tweet_b), outputs = predictions)\n\nmodel %% compile(\n  optimizer = 'rmsprop',\n  loss = 'binary_crossentropy',\n  metrics = c('accuracy')\n)\n\nmodel %% fit(list(dataa, datab), labels, epochs = 10)\n\n The concept of layer \"node\"\n\nWhenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 1, 2, 2...\n\nYou can obtain the output tensor of a layer via layer$output, or its output shape via layer$output_shape. But what if a layer is connected to multiple inputs?\n\nAs long as a layer is only connected to one input, there is no confusion, and $output will return the one output of the layer:\n\na <- layer_input(shape = c(280, 256))\n\nlstm <- layer_lstm(units = 32)\n\nencoded_a - a %% lstm\n\nlstm$output\n\nNot so if the layer has multiple inputs:\n\na <- layer_input(shape = c(280, 256))\nb <- layer_input(shape = c(280, 256))\n\nlstm <- layer_lstm(units = 32)\n\nencoded_a - a %% lstm\nencoded_b - b %% lstm\n\nlstm$output\n\nAttributeError: Layer lstm4 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use getoutputat(nodeindex) instead.\n\nOkay then. The following works:\n\ngetoutputat(lstm, 1)\ngetoutputat(lstm, 2)\n\nSimple enough, right?\n\nThe same is true for the properties inputshape and outputshape: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by layer$outputshape/layer$inputshape. But if, for instance, you apply the same layerconv2d() layer to an input of shape (32, 32, 3), and then to an input of shape (64, 64, 3), the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:\n\na <- layer_input(shape = c(32, 32, 3))\nb <- layer_input(shape = c(64, 64, 3))\n\nconv <- layerconv2d(filters = 16, kernel_size = c(3,3), padding = 'same')\n\nconved_a - a %% conv\n\nonly one input so far, the following will work\nconv$input_shape\n\nconved_b - b %% conv\n now the $input_shape property wouldn't work, but this does:\ngetinputshape_at(conv, 1)\ngetinputshape_at(conv, 2) \n\nMore examples\n\nCode examples are still the best way to get started, so here are a few more.\n\n Inception module\n\nFor more information about the Inception architecture, see Going Deeper with Convolutions.\n\nlibrary(keras)\n\ninputimg <- layerinput(shape = c(256, 256, 3))\n\ntower1 - inputimg %% \n  layerconv2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu') %% \n  layerconv2d(filters = 64, kernel_size = c(3, 3), padding='same', activation='relu')\n\ntower2 - inputimg %% \n  layerconv2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu') %% \n  layerconv2d(filters = 64, kernel_size = c(5, 5), padding='same', activation='relu')\n\ntower3 - inputimg %% \n  layermaxpooling2d(poolsize = c(3, 3), strides = c(1, 1), padding = 'same') %% \n  layerconv2d(filters = 64, kernel_size = c(1, 1), padding='same', activation='relu')\n\noutput <- layerconcatenate(c(tower1, tower2, tower3), axis = 1)\n\nResidual connection on a convolution layer\n\nFor more information about residual networks, see Deep Residual Learning for Image Recognition.\n\n input tensor for a 3-channel 256x256 image\nx <- layer_input(shape = c(256, 256, 3))\n3x3 conv with 3 output channels (same as input channels)\ny - x %% layerconv2d(filters = 3, kernel_size =c(3, 3), padding = 'same')\n this returns x + y.\nz <- layer_add(c(x, y))\n\nShared vision model\n\nThis model reuses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.\n\n First, define the vision model\ndigitinput <- layerinput(shape = c(27, 27, 1))\nout - digit_input %% \n  layerconv2d(filters = 64, kernel_size = c(3, 3)) %% \n  layerconv2d(filters = 64, kernel_size = c(3, 3)) %% \n  layermaxpooling2d(poolsize = c(2, 2)) %% \n  layer_flatten()\n\nvisionmodel <- kerasmodel(digit_input, out)\n\nThen define the tell-digits-apart model\ndigita <- layerinput(shape = c(27, 27, 1))\ndigitb <- layerinput(shape = c(27, 27, 1))\n\n The vision model will be shared, weights and all\nouta - digita %% vision_model\noutb - digitb %% vision_model\n\nout - layerconcatenate(c(outa, out_b)) %% \n  layer_dense(units = 1, activation = 'sigmoid')\n\nclassificationmodel <- kerasmodel(inputs = c(digita, digitb), out)\n\nVisual question answering model\n\nThis model can select the correct one-word answer when asked a natural-language question about a picture.\n\nIt works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.\n\n First, let's define a vision model using a Sequential model.\nThis model will encode an image into a vector.\nvisionmodel <- kerasmodel_sequential() \nvision_model %% \n  layerconv2d(filters = 64, kernel_size = c(3, 3), activation = 'relu', padding = 'same',\n                input_shape = c(224, 224, 3)) %% \n  layerconv2d(filters = 64, kernel_size = c(3, 3), activation = 'relu') %% \n  layermaxpooling2d(poolsize = c(2, 2)) %% \n  layerconv2d(filters = 128, kernel_size = c(3, 3), activation = 'relu', padding = 'same') %% \n  layerconv2d(filters = 128, kernel_size = c(3, 3), activation = 'relu') %% \n  layermaxpooling2d(poolsize = c(2, 2)) %% \n  layerconv2d(filters = 256, kernel_size = c(3, 3), activation = 'relu', padding = 'same') %% \n  layerconv2d(filters = 256, kernel_size = c(3, 3), activation = 'relu') %% \n  layerconv2d(filters = 256, kernel_size = c(3, 3), activation = 'relu') %% \n  layermaxpooling2d(poolsize = c(2, 2)) %% \n  layer_flatten()\n\n Now let's get a tensor with the output of our vision model:\nimageinput <- layerinput(shape = c(224, 224, 3))\nencodedimage - imageinput %% vision_model\n\nNext, let's define a language model to encode the question into a vector.\n Each question will be at most 100 word long,\nand we will index words as integers from 1 to 9999.\nquestioninput <- layerinput(shape = c(100), dtype = 'int32')\nencodedquestion - questioninput %% \n  layerembedding(inputdim = 10000, outputdim = 256, inputlength = 100) %% \n  layer_lstm(units = 256)\n\n Let's concatenate the question vector and the image vector then\ntrain a logistic regression over 1000 words on top\noutput - layerconcatenate(c(encodedquestion, encoded_image)) %% \n  layer_dense(units = 1000, activation='softmax')\n\n This is our final model:\nvqamodel <- kerasmodel(inputs = c(imageinput, questioninput), outputs = output)\n\nVideo question answering model\n\nNow that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" - \"football\").\n\nvideoinput <- layerinput(shape = c(100, 224, 224, 3))\n\n This is our video encoded via the previously trained vision_model (weights are reused)\nencodedvideo - videoinput %% \n  timedistributed(visionmodel) %% \n  layer_lstm(units = 256)\n\nThis is a model-level representation of the question encoder, reusing the same weights as before:\nquestionencoder <- kerasmodel(inputs = questioninput, outputs = encodedquestion)\n\n Let's use it to encode the question:\nvideoquestioninput <- layer_input(shape = c(100), dtype = 'int32')\nencodedvideoquestion - videoquestioninput %% question_encoder\n\nAnd this is our video question answering model:\noutput - layerconcatenate(c(encodedvideo, encodedvideoquestion)) %% \n  layer_dense(units = 1000, activation = 'softmax')\n\nvideoqamodel <- kerasmodel(inputs= c(videoinput, videoquestioninput), outputs = output)\n\n","id":66},{"path":"/guide/keras/guide_keras","title":"\"Guide to Keras Basics\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Keras basics\"","    identifier":"   identifier: \"keras-keras-basics\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\n\nKeras is a high-level API to build and train deep learning models. It’s\nused for fast prototyping, advanced research, and production, with three\nkey advantages:\n\n  User friendlybr Keras has a simple, consistent interface\n    optimized for common use cases. It provides clear and actionable\n    feedback for user errors.\n  Modular and composablebr Keras models are made by connecting\n    configurable building blocks together, with few restrictions.\n  Easy to extendbr Write custom building blocks to express new\n    ideas for research. Create new layers, loss functions, and develop\n    state-of-the-art models.\n\nImport keras\n\nTo get started, load the keras library:\n\nlibrary(keras)\n\n Build a simple model\n\nSequential model\n\nIn Keras, you assemble layers to build models. A model is (usually)\na graph of layers. The most common type of model is a stack of layers:\nthe sequential model.\n\nTo build a simple, fully-connected network (i.e., a multi-layer\nperceptron):\n\nmodel <- kerasmodelsequential()\n\nmodel %% \n  \n   Adds a densely-connected layer with 64 units to the model:\n  layer_dense(units = 64, activation = 'relu') %%\n  \n  # Add another:\n  layer_dense(units = 64, activation = 'relu') %%\n  \n  # Add a softmax layer with 10 output units:\n  layer_dense(units = 10, activation = 'softmax')\n\nConfigure the layers\n\nThere are many layers available with some common constructor\nparameters:\n\n  activation: Set the activation function for the layer. By default, no activation is applied.\n  kernelinitializer and biasinitializer: The initialization\n    schemes that create the layer’s weights (kernel and bias). This defaults to the\n    Glorot uniform initializer.\n  kernelregularizer and biasregularizer: The regularization\n    schemes that apply to the layer’s weights (kernel and bias), such as L1\n    or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates dense layers using\nconstructor arguments:\n\n Create a sigmoid layer:\nlayer_dense(units = 64, activation ='sigmoid')\n\nA linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayerdense(units = 64, kernelregularizer = regularizer_l1(0.01))\n\n A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayerdense(units = 64, biasregularizer = regularizer_l2(0.01))\n\nA linear layer with a kernel initialized to a random orthogonal matrix:\nlayerdense(units = 64, kernelinitializer = 'orthogonal')\n\n A linear layer with a bias vector initialized to 2.0:\nlayerdense(units = 64, biasinitializer = initializer_constant(2.0))\n\nTrain and evaluate\n\n Set up training\n\nAfter the model is constructed, configure its learning process by\ncalling the compile method:\n\nmodel %% compile(\n  optimizer = 'adam',\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\ncompile takes three important arguments:\n\n  optimizer: This object specifies the training procedure. Commonly used optimizers are e.g.  \n    adam, \n    rmsprop, or\n    sgd.\n  loss: The function to minimize during optimization. Common choices\n    include mean square error (mse), categorical_crossentropy, and\n    binary_crossentropy.\n  metrics: Used to monitor training. In classification, this usually is accuracy.\n\nThe following shows a few examples of configuring a model for training:\n\nConfigure a model for mean-squared error regression.\nmodel %% compile(\n  optimizer = 'adam',\n  loss = 'mse',            mean squared error\n  metrics = list('mae')   # mean absolute error\n)\n\nConfigure a model for categorical classification.\nmodel %% compile(\n  optimizer = optimizer_rmsprop(lr = 0.01),\n  loss = \"categorical_crossentropy\",\n  metrics = list(\"categorical_accuracy\")\n)\n\n Input data\n\nYou can train keras models directly on R matrices and arrays (possibly created from R data.frames).\nA model is fit to the training data using the fit method:\n\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nmodel %% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32\n)\n\nfit takes three important arguments:\n\n  epochs: Training is structured into epochs. An epoch is one\n    iteration over the entire input data (this is done in smaller\n    batches).\n  batch_size: When passed matrix or array data, the model slices the data into\n    smaller batches and iterates over these batches during training.\n    This integer specifies the size of each batch. Be aware that the\n    last batch may be smaller if the total number of samples is not\n    divisible by the batch size.\n  validation_data: When prototyping a model, you want to easily\n    monitor its performance on some validation data. Passing this\n    argument — a list of inputs and labels — allows the model to display\n    the loss and metrics in inference mode for the passed data, at the\n    end of each epoch.\n\nHere’s an example using validation_data:\n\ndata <- matrix(rnorm(1000 * 32), nrow = 1000, ncol = 32)\nlabels <- matrix(rnorm(1000 * 10), nrow = 1000, ncol = 10)\n\nval_data <- matrix(rnorm(1000 * 32), nrow = 100, ncol = 32)\nval_labels <- matrix(rnorm(100 * 10), nrow = 100, ncol = 10)\n\nmodel %% fit(\n  data,\n  labels,\n  epochs = 10,\n  batch_size = 32,\n  validationdata = list(valdata, val_labels)\n)\n\nEvaluate and predict\n\nSame as fit, the evaluate and predict methods can\nuse raw R data as well as a dataset.\n\nTo evaluate the inference-mode loss and metrics for the data provided:\n\nmodel %% evaluate(testdata, testlabels, batch_size = 32)\n\nmodel %% evaluate(test_dataset, steps = 30)\n\nAnd to predict the output of the last layer in inference for the data\nprovided, again as R data as well as a dataset:\n\nmodel %% predict(testdata, batchsize = 32)\n    \nmodel %% predict(test_dataset, steps = 30)\n\n    \n\n Build advanced models\n\nFunctional API\n\nThe sequential model is a simple stack of layers that cannot\nrepresent arbitrary models. Use the [Keras functional\nAPI](functional_api.html)\nto build complex model topologies such as:\n\n  multi-input models,\n  multi-output models,\n  models with shared layers (the same layer called several times),\n  models with non-sequential data flows (e.g., residual connections).\n\nBuilding a model with the functional API works like this:\n\nA layer instance is callable and returns a tensor.\nInput tensors and output tensors are used to define a\n    keras_model instance.\nThis model is trained just like the sequential model.\n\nThe following example uses the functional API to build a simple,\nfully-connected network:\n\ninputs <- layer_input(shape = (32))   Returns a placeholder tensor\n\npredictions - inputs %% \n  layer_dense(units = 64, activation = 'relu') %%\n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dense(units = 10, activation = 'softmax')\n\nInstantiate the model given inputs and outputs.\nmodel <- keras_model(inputs = inputs, outputs = predictions)\n\n The compile step specifies the training configuration.\nmodel %% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\nTrains for 5 epochs\nmodel %% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n\n Custom layers\n\nTo create a custom Keras layer, you create an R6 class derived from KerasLayer. There are three methods to implement (only one of which, call(), is required for all types of layer):\n\n build(input_shape): This is where you will define your weights. Note that if your layer doesn’t define trainable weights then you need not implement this method.\n call(x): This is where the layer’s logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to call: the input tensor.\n computeoutputshape(input_shape): In case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference. If you don’t modify the shape of the input then you need not implement this method.\n    \nHere is an example custom layer that performs a matrix multiplication:\n\nlibrary(keras)\n\nCustomLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    output_dim = NULL,\n    \n    kernel = NULL,\n    \n    initialize = function(output_dim)  ,\n    \n    build = function(input_shape)  ,\n    \n    call = function(x, mask = NULL)  ,\n    \n    computeoutputshape = function(input_shape)  \n  )\n)\n\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\ndefine layer wrapper function\nlayercustom <- function(object, outputdim, name = NULL, trainable = TRUE)  \n\nYou can now use the layer in a model as usual:\n\nmodel <- kerasmodelsequential()\nmodel %% \n  layerdense(units = 32, inputshape = c(32,32)) %% \n  layercustom(outputdim = 32)\n\n Custom models\n\nIn addition to creating custom layers, you can also create a custom model.\nThis might be necessary if you wanted to use TensorFlow eager execution in combination with an imperatively written forward pass.\n\nIn cases where this is not needed, but flexibility in building the architecture is required, it is recommended to just stick with the functional API.\n\nA custom model is defined by calling kerasmodelcustom() passing a function that specifies the layers to be created and the operations to be executed on forward pass.\n\nmymodel <- function(inputdim, output_dim, name = NULL)  \n   )\n \n\nmodel <- mymodel(inputdim = 32, output_dim = 10)\n\nmodel %% compile(\n  optimizer = optimizer_rmsprop(lr = 0.001),\n  loss = 'categorical_crossentropy',\n  metrics = list('accuracy')\n)\n\nTrains for 5 epochs\nmodel %% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5\n)\n\n Callbacks\n\nA callback is an object passed to a model to customize and extend its\nbehavior during training. You can write your own custom callback, or use\nthe built-in callbacks that include:\n\n  callbackmodelcheckpoint: Save checkpoints of your model\n    at regular intervals.\n  callbacklearningrate_scheduler: Dynamically change the\n    learning rate.\n  callbackearlystopping: Interrupt training when\n    validation performance has stopped improving.\n  callbacks_tensorboard: Monitor the model’s behavior using\n    TensorBoard.\n\nTo use a callback, pass it to the model’s fit\nmethod:\n\ncallbacks <- list(\n  callbackearlystopping(patience = 2, monitor = 'val_loss'),\n  callbacktensorboard(logdir = './logs')\n)\n\nmodel %% fit(\n  data,\n  labels,\n  batch_size = 32,\n  epochs = 5,\n  callbacks = callbacks,\n  validationdata = list(valdata, val_labels)\n)\n\nSave and restore\n\n Weights only\n\nSave and load the weights of a model using savemodelweightshdf5 and loadmodelweightshdf5, respectively:\n\nsave in SavedModel format\nmodel %% savemodelweightstf('mymodel/')\n\n Restore the model's state,\nthis requires a model with the same architecture.\nmodel %% loadmodelweightstf('mymodel/')\n\n Configuration only\n\nA model’s configuration can be saved - this serializes the model\narchitecture without any weights. A saved configuration can recreate and\ninitialize the same model, even without the code that defined the\noriginal model. Keras supports JSON and YAML serialization formats:\n\nSerialize a model to JSON format\njsonstring - model %% modelto_json()\n\n Recreate the model (freshly initialized)\nfreshmodel <- modelfromjson(jsonstring)\n\nSerializes a model to YAML format\nyamlstring - model %% modelto_yaml()\n\n Recreate the model\nfreshmodel <- modelfromyaml(yamlstring)\n\nCaution: Custom models are not serializable because their\narchitecture is defined by the R code in the function passed to kerasmodelcustom.\n\nEntire model\n\nThe entire model can be saved to a file that contains the weight values,\nthe model’s configuration, and even the optimizer’s configuration. This\nallows you to checkpoint a model and resume training later —from the\nexact same state —without access to the original code.\n\n Save entire model to the SavedModel format\nmodel %% savemodeltf('my_model/')\n\nRecreate the exact same model, including weights and optimizer.\nmodel <- loadmodeltf('my_model/')\n\n","id":67},{"path":"/guide/keras/saving_serializing","title":"\"Saving and serializing models\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Saving and serializing models\"","    identifier":"   identifier: \"keras-saving-serializing\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 30","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\n\n This tutorial is an R translation of this page\navailable in the official TensorFlow documentation.\n\nThe first part of this guide covers saving and serialization for Sequential models and models built using the Functional API. The saving and serialization \nAPIs are the exact same for both of these types of models.\n\nSaving for custom subclasses of Model is covered in the section \"Saving Subclassed Models\". \nThe APIs in this case are slightly different than for Sequential or Functional models.\n\nOverview\n\nFor Sequential Models and models built using the Functional API use:\n\nsavemodelhdf5()/loadmodelhdf5() to save the entire model to disk, including the optimizer state. You can also use savemodeltf/loadmodeltf to save the entire model to the SavedModel format.\n\ngetconfig()/fromconfig() to load only the model architecture into an R object.\n\nmodeltojson()/modelfromjson() to save only the architecture of the model to a single string - useful for saving the architecture to disk. You can also use modeltoyaml()/modelfromyaml() to save the architecture.\n\nsavemodelweightshdf5()/loadmodelweightshdf5() if you want to save only the model weights to disk in the hdf5 format. You can also use savemodelweightstf()/loadmodelweightstf() to save the weights in the SavedModel format.\n\nNote you can use a combination of modeltojson() and savemodelweights_hdf5() to save both the architecture and the weights. In this case the optimizer state is not saved.\n\nFor custom models use:\n\nsavemodelweightstf() or savemodelweightshdf5() to save the model weights. Usually for custom models, the architecture must be recreated using code.\n\n Setup\n\nlibrary(keras)\n\nSaving Sequential Models or Functional models\n\ninputs <- layer_input(shape = 784, name = \"digits\")\noutputs - inputs %% \n  layerdense(units = 64, activation = \"relu\", name = \"dense1\") %% \n  layerdense(units = 64, activation = \"relu\", name = \"dense2\") %% \n  layer_dense(units = 10, activation = \"softmax\", name = \"predictions\")\nmodel <- keras_model(inputs, outputs) \nsummary(model)\n\nOptionally, let's train this model, just so it has weight values to save, \nas well as an an optimizer state. Of course, you can save models you've never \ntrained, too, but obviously that's less interesting.\n\nc(c(xtrain, ytrain), c(xtest, ytest)) %<-% dataset_mnist()\nxtrain - xtrain %% array_reshape(dim = c(60000, 784))/255\nxtest - xtest %% array_reshape(dim = c(10000, 784))/255\n\nmodel %% compile(loss = \"sparsecategoricalcrossentropy\",\n                  optimizer = optimizer_rmsprop())\n\nhistory - model %% fit(xtrain, ytrain, batch_size = 64, epochs = 1)\n\n Save predictions for future checks\npredictions <- predict(model, x_test)\n\nWhole-model saving\n\nYou can save a model built with the Functional API into a single file. You can \nlater recreate the same model from this file, even if you no longer have access \nto the code that created the model.\n\nThis file includes:\n\nThe model's architecture\nThe model's weight values (which were learned during training)\nThe model's training config (what you passed to compile), if any\nThe optimizer and its state, if any (this enables you to restart training where you left off)\n\n Save the model\nsavemodelhdf5(model, \"model.h5\")\n\nRecreate the exact same model purely from the file\nnewmodel <- loadmodel_hdf5(\"model.h5\")\n\n Check that the state is preserved\nnewpredictions <- predict(newmodel, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer state is preserved as well so you can resume \ntraining where you left off.\n\nExport to SavedModel\n\nYou can also export a whole model to the TensorFlow SavedModel format. SavedModel is \na standalone serialization format for Tensorflow objects, supported by TensorFlow \nserving as well as TensorFlow implementations other than Python. Note that\nsavemodeltf is only available for TensorFlow version greater than 1.14.\n\n Export the model to a SavedModel\nsavemodeltf(model, \"model/\")\n\nRecreate the exact same model\nnewmodel <- loadmodel_tf(\"model/\")\n\n Check that the state is preserved\nnewpredictions <- predict(newmodel, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer state is preserved as well so you can resume \ntraining where you left off.\n\nThe SavedModel files that were created contain:\n\nA TensorFlow checkpoint containing the model weights.\nA SavedModel proto containing the underlying Tensorflow graph. Separate graphs are saved for prediction (serving), train, and evaluation. If the model wasn't compiled before, then only the inference graph gets exported.\nThe model's architecture config, if available.\n\nYou can also use the export_savedmodel function to export models but those\nmodels can not be loaded as Keras models again. Models exported using \nexported_savedmodels can be used for prediction though. \n\nexport_savedmodel(model, \"savedmodel/\")\nnewpredictions <- tfdeploy::predictsavedmodel(x_test, \"savedmodel/\")\n\nNote Exporting with export_savedmodel sets learning phase to 0 so you need to restart R and re-build the model before doing additional training.\n\nArchitecture-only saving\n\nSometimes, you are only interested in the architecture of the model, and you \ndon't need to save the weight values or the optimizer. In this case, you can \nretrieve the \"config\" of the model via the get_config() method. The config is \na named list that enables you to recreate the same model -- initialized from \nscratch, without any of the information learned previously during training.\n\nconfig <- get_config(model)\nreinitializedmodel <- fromconfig(config)\n\n Note that the model state is not preserved! We only saved the architecture.\nnewpredictions <- predict(reinitializedmodel, x_test)\nall.equal(predictions, new_predictions)\n\nYou can alternatively use modeltojson() and modelfromjson(), which uses a \nJSON string to store the config instead of a named list. This is useful to save \nthe config to disk.\n\njsonconfig <- modelto_json(model)\nreinitializedmodel <- modelfromjson(jsonconfig)\n\nWeights-only saving\n\nSometimes, you are only interested in the state of the model -- its weights values -- and \nnot in the architecture. In this case, you can retrieve the weights values as a list of arrays \nvia getweights(), and set the state of the model via setweights:\n\nweights <- get_weights(model)\nsetweights(reinitializedmodel, weights)\n\nnewpredictions <- predict(reinitializedmodel, x_test)\nall.equal(predictions, new_predictions)\n\nYou can combine getconfig()/fromconfig() and getweights()/setweights() to \nrecreate your model in the same state. However, unlike savemodelhdf5, this will not \ninclude the training config and the optimizer. You would have to call compile() again \nbefore using the model for training.\n\nconfig <- get_config(model)\nweights <- get_weights(model)\n\nnewmodel <- fromconfig(config)\nsetweights(newmodel, weights)\n\n Check that the state is preserved\nnewpredictions <- predict(newmodel, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer was not preserved, so the model should be compiled \nanew before training (and the optimizer will start from a blank state).\n\nThe save-to-disk alternative to getweights() and setweights(weights) is saveweights(fpath) and loadweights(fpath).\n\nSave JSON config to disk\njsonconfig <- modelto_json(model)\nwriteLines(jsonconfig, \"modelconfig.json\")\n\n Save weights to disk\nsavemodelweightshdf5(model, \"modelweights.h5\")\n\nReload the model from the 2 files we saved\njsonconfig <- readLines(\"modelconfig.json\")\nnewmodel <- modelfromjson(jsonconfig)\nloadmodelweightshdf5(newmodel, \"model_weights.h5\")\n\n Check that the state is preserved\nnewpredictions <- predict(newmodel, x_test)\nall.equal(predictions, new_predictions)\n\nNote that the optimizer was not preserved. But remember that the simplest, \nrecommended way is just this:\n\nsavemodelhdf5(model, \"model.h5\")\nnewmodel <- loadmodel_hdf5(\"model.h5\")\n\nWeights-only saving in SavedModel format\n\nNote that save_weights can create files either in the Keras HDF5 format, or in \nthe TensorFlow SavedModel format.\n\nsavemodelweightstf(model, \"modelweights\")\n\n Saving Subclassed Models\n\nSequential models and Functional models are data structures that represent a \nDAG of layers. As such, they can be safely serialized and deserialized.\n\nA subclassed model differs in that it's not a data structure, it's \na piece of code. The architecture of the model is defined via the body of the \ncall method. This means that the architecture of the model cannot be safely \nserialized. To load a model, you'll need to have access to the code that \ncreated it (the code of the model subclass). Alternatively, you could be \nserializing this code as bytecode (e.g. via pickling), but that's unsafe \nand generally not portable.\n\nFor more information about these differences, see the article \n\"What are Symbolic and Imperative APIs in TensorFlow 2.0?\".\n\nLet's consider the following subclassed model, which follows the same structure as the model from the first section:\n\nkerasmodelsimplemlp <- function(numclasses, \n                                   usebn = FALSE, usedp = FALSE, \n                                   name = NULL)  \n   )\n \n\nmodel <- kerasmodelsimplemlp(numclasses = 10)\n\nFirst of all, a subclassed model that has never been used cannot be saved.\n\nThat's because a subclassed model needs to be called on some data in order to create its weights.\n\nUntil the model has been called, it does not know the shape and dtype of the input \ndata it should be expecting, and thus cannot create its weight variables. You \nmay remember that in the Functional model from the first section, the shape and \ndtype of the inputs was specified in advance (via layer_input) -- that's \nwhy Functional models have a state as soon as they're instantiated.\n\nLet's train the model, so as to give it a state:\n\nmodel %% compile(loss = \"sparsecategoricalcrossentropy\",\n                  optimizer = optimizer_rmsprop())\n\nhistory - model %% fit(xtrain, ytrain, batch_size = 64, epochs = 1)\n\nThe recommended way to save a subclassed model is to use savemodelweights_tf to \ncreate a TensorFlow SavedModel checkpoint, which will contain the value of all variables \nassociated with the model: - The layers' weights - The optimizer's state - Any variables \nassociated with stateful model metrics (if any).\n\nsavemodelweightstf(model, \"myweights\")\n\nSave predictions for future checks\npredictions <- predict(model, x_test)\n Also save the loss on the first batch\nto later assert that the optimizer state was preserved\nfirstbatchloss <- trainonbatch(model, xtrain[1:64,], ytrain[1:64])\n\nTo restore your model, you will need access to the code that created the model object.\n\nNote that in order to restore the optimizer state and the state of any stateful \nmetric, you should compile the model (with the exact same arguments as before) and \ncall it on some data before calling load_weights:\n\nnewmodel <- kerasmodelsimplemlp(num_classes = 10)\nnewmodel %% compile(loss = \"sparsecategorical_crossentropy\",\n                  optimizer = optimizer_rmsprop())\n\n This initializes the variables used by the optimizers,\nas well as any stateful metric variables\ntrainonbatch(newmodel, xtrain[1:5,], y_train[1:5])\n\n Load the state of the old model\nloadmodelweightstf(newmodel, \"my_weights\")\n\nCheck that the model state has been preserved\nnewpredictions <- predict(newmodel, x_test)\nall.equal(predictions, new_predictions)\n\n The optimizer state is preserved as well,\nso you can resume training where you left off\nnewfirstbatchloss <- trainonbatch(newmodel, xtrain[1:64,], ytrain[1:64])\nfirstbatchloss == newfirstbatch_loss\n\nYou've reached the end of this guide! This covers everything you need to know about saving and serializing models with Keras in TensorFlow 2.0.\n\n","id":68},{"path":"/guide/keras/sequential_model","title":"\"Guide to the Sequential Model\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Sequential API\"","    identifier":"   identifier: \"keras-sequential-model\"","    parent":"   parent: \"keras-getting-started-top\"","    weight":"   weight: 15","content":"\nlibrary(keras)\nknitr::opts_chunk$set(eval = FALSE)\n\nDefining a Model\n\nThe sequential model is a linear stack of layers.\n\nYou create a sequential model by calling the kerasmodelsequential() function then a series of layer functions:\n\nlibrary(keras)\n\nmodel <- kerasmodelsequential() \nmodel %% \n  layerdense(units = 32, inputshape = c(784)) %% \n  layer_activation('relu') %% \n  layer_dense(units = 10) %% \n  layer_activation('softmax')\n\nNote that Keras objects are modified in place which is why it's not necessary for model to be assigned back to after the layers are added.\n\nPrint a summary of the model's structure using the summary() function:\n\nsummary(model)\n\nModel\n____________________\nLayer (type)                        Output Shape                    Param      \n\ndense_1 (Dense)                     (None, 256)                     200960      \n____________________\ndropout_1 (Dropout)                 (None, 256)                     0           \n____________________\ndense_2 (Dense)                     (None, 128)                     32896       \n____________________\ndropout_2 (Dropout)                 (None, 128)                     0           \n____________________\ndense_3 (Dense)                     (None, 10)                      1290        \n\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\n____________________\n\nInput Shapes\n\nThe model needs to know what input shape it should expect. For this reason, the first layer in a sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. \n\nAs illustrated in the example above, this is done by passing an inputshape argument to the first layer. This is a list of integers or NULL entries, where NULL indicates that any positive integer may be expected. In inputshape, the batch dimension is not included.\n\nIf you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a batchsize argument to a layer. If you pass both batchsize=32 and input_shape=c(6, 8) to a layer, it will then expect every batch of inputs to have the batch shape (32, 6, 8).\n\n Compilation\n\nBefore training a model, you need to configure the learning process, which is done via the compile() function. It receives three arguments:\n\nAn optimizer. This could be the string identifier of an existing optimizer (e.g. as \"rmsprop\" or \"adagrad\") or a call to an optimizer function (e.g. optimizer_sgd()).\n\nA loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (e.g. \"categoricalcrossentropy\" or \"mse\") or a call to a loss function (e.g. lossmeansquarederror()).\n\nA list of metrics. For any classification problem you will want to set this to metrics = c('accuracy'). A metric could be the string identifier of an existing metric or a call to metric function (e.g. metricbinarycrossentropy()).\n\nHere's the definition of a model along with the compilation step (the compile() function has arguments appropriate for a multi-class classification problem):\n\nFor a multi-class classification problem\nmodel <- kerasmodelsequential() \nmodel %% \n  layerdense(units = 32, inputshape = c(784)) %% \n  layer_activation('relu') %% \n  layer_dense(units = 10) %% \n  layer_activation('softmax')\n\nmodel %% compile(\n  optimizer = 'rmsprop',\n  loss = 'categorical_crossentropy',\n  metrics = c('accuracy')\n)\n\nHere's what compilation might look like for a mean squared error regression problem:\n\nmodel %% compile(\n  optimizer = optimizer_rmsprop(lr = 0.002),\n  loss = 'mse'\n)\n\nHere's compilation for a binary classification problem:\n\nmodel %% compile( \n  optimizer = optimizer_rmsprop(),\n  loss = lossbinarycrossentropy,\n  metrics = metricbinaryaccuracy\n)\n\nHere's compilation with a custom metric:\n\n create metric using backend tensor functions\nmetricmeanpred <- custommetric(\"meanpred\", function(ytrue, ypred)  )\n\nmodel %% compile( \n  optimizer = optimizer_rmsprop(),\n  loss = lossbinarycrossentropy,\n  metrics = c('accuracy', metricmeanpred)\n)\n\nTraining\n\nKeras models are trained on R matrices or higher dimensional arrays of input data and labels. For training a model, you will typically use the fit() function.\n\nHere's a single-input model with 2 classes (binary classification):\n\n create model\nmodel <- kerasmodelsequential()\n\nadd layers and compile the model\nmodel %% \n  layerdense(units = 32, activation = 'relu', inputshape = c(100)) %% \n  layer_dense(units = 1, activation = 'sigmoid') %% \n  compile(\n    optimizer = 'rmsprop',\n    loss = 'binary_crossentropy',\n    metrics = c('accuracy')\n  )\n\n Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)\n\nTrain the model, iterating on the data in batches of 32 samples\nmodel %% fit(data, labels, epochs=10, batch_size=32)\n\nHere's a single-input model with 10 classes (categorical classification):\n\n create model\nmodel <- kerasmodelsequential()\n\ndefine and compile the model\nmodel %% \n  layerdense(units = 32, activation = 'relu', inputshape = c(100)) %% \n  layer_dense(units = 10, activation = 'softmax') %% \n  compile(\n    optimizer = 'rmsprop',\n    loss = 'categorical_crossentropy',\n    metrics = c('accuracy')\n  )\n\n Generate dummy data\ndata <- matrix(runif(1000*100), nrow = 1000, ncol = 100)\nlabels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)\n\nConvert labels to categorical one-hot encoding\nonehotlabels <- tocategorical(labels, numclasses = 10)\n\n Train the model, iterating on the data in batches of 32 samples\nmodel %% fit(data, onehotlabels, epochs=10, batch_size=32)\n\nExamples\n\nHere are a few examples to get you started!\n\nOn the examples page you will also find example models for real datasets:\n\nCIFAR10 small images classification \n\nIMDB movie review sentiment classification \n\nReuters newswires topic classification \n\nMNIST handwritten digits classification\n\nCharacter-level text generation with LSTM\n\nSome additional examples are provided below.\n\n Multilayer Perceptron (MLP) for multi-class softmax classification\n\nlibrary(keras)\n\ngenerate dummy data\nx_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)\n\ny_train - runif(1000, min = 0, max = 9) %% \n  round() %%\n  matrix(nrow = 1000, ncol = 1) %% \n  tocategorical(numclasses = 10)\n\nx_test  <- matrix(runif(100*20), nrow = 100, ncol = 20)\n\ny_test - runif(100, min = 0, max = 9) %% \n  round() %%\n  matrix(nrow = 100, ncol = 1) %% \n  tocategorical(numclasses = 10)\n\n create model\nmodel <- kerasmodelsequential()\n\ndefine and compile the model\nmodel %% \n  layerdense(units = 64, activation = 'relu', inputshape = c(20)) %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 10, activation = 'softmax') %% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_sgd(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = TRUE),\n    metrics = c('accuracy')     \n  )\n\n train\nmodel %% fit(xtrain, ytrain, epochs = 20, batch_size = 128)\n\nevaluate\nscore - model %% evaluate(xtest, ytest, batch_size = 128)\n\n MLP for binary classification\n\nlibrary(keras)\n\ngenerate dummy data\nx_train <- matrix(runif(1000*20), nrow = 1000, ncol = 20)\ny_train <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)\nx_test <- matrix(runif(100*20), nrow = 100, ncol = 20)\ny_test <- matrix(round(runif(100, min = 0, max = 1)), nrow = 100, ncol = 1)\n\n create model\nmodel <- kerasmodelsequential()\n\ndefine and compile the model\nmodel %% \n  layerdense(units = 64, activation = 'relu', inputshape = c(20)) %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 64, activation = 'relu') %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 1, activation = 'sigmoid') %% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\n train \nmodel %% fit(xtrain, ytrain, epochs = 20, batch_size = 128)\n\nevaluate\nscore = model %% evaluate(xtest, ytest, batch_size=128)\n\n VGG-like convnet\n\nlibrary(keras)\n\ngenerate dummy data\nx_train <- array(runif(100 * 100 * 100 * 3), dim = c(100, 100, 100, 3))\n\ny_train - runif(100, min = 0, max = 9) %% \n  round() %%\n  matrix(nrow = 100, ncol = 1) %% \n  tocategorical(numclasses = 10)\n\nx_test <- array(runif(20 * 100 * 100 * 3), dim = c(20, 100, 100, 3))\n\ny_test - runif(20, min = 0, max = 9) %% \n  round() %%\n  matrix(nrow = 20, ncol = 1) %% \n  tocategorical(numclasses = 10)\n\n create model\nmodel <- kerasmodelsequential()\n\ndefine and compile model\n input: 100x100 images with 3 channels - (100, 100, 3) tensors.\nthis applies 32 convolution filters of size 3x3 each.\nmodel %% \n  layerconv2d(filters = 32, kernel_size = c(3,3), activation = 'relu', \n                input_shape = c(100,100,3)) %% \n  layerconv2d(filters = 32, kernel_size = c(3,3), activation = 'relu') %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_dropout(rate = 0.25) %% \n  layerconv2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %% \n  layerconv2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layer_dropout(rate = 0.25) %% \n  layer_flatten() %% \n  layer_dense(units = 256, activation = 'relu') %% \n  layer_dropout(rate = 0.25) %% \n  layer_dense(units = 10, activation = 'softmax') %% \n  compile(\n    loss = 'categorical_crossentropy', \n    optimizer = optimizer_sgd(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = TRUE)\n  )\n  \n train\nmodel %% fit(xtrain, ytrain, batch_size = 32, epochs = 10)\n\nevaluate\nscore - model %% evaluate(xtest, ytest, batch_size = 32)\n\n Sequence classification with LSTM\n\nmodel <- kerasmodelsequential() \nmodel %% \n  layerembedding(inputdim = maxfeatures, outputdim - 256) %% \n  layer_lstm(units = 128) %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 1, activation = 'sigmoid') %% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\nmodel %% fit(xtrain, ytrain, batch_size = 16, epochs = 10)\nscore - model %% evaluate(xtest, ytest, batch_size = 16)\n\nSequence classification with 1D convolutions:\n\nmodel <- kerasmodelsequential()\nmodel %% \n  layerconv1d(filters = 64, kernel_size = 3, activation = 'relu',\n                inputshape = c(seqlength, 100)) %% \n  layerconv1d(filters = 64, kernel_size = 3, activation = 'relu') %% \n  layermaxpooling1d(poolsize = 3) %% \n  layerconv1d(filters = 128, kernel_size = 3, activation = 'relu') %% \n  layerconv1d(filters = 128, kernel_size = 3, activation = 'relu') %% \n  layerglobalaveragepooling1d() %% \n  layer_dropout(rate = 0.5) %% \n  layer_dense(units = 1, activation = 'sigmoid') %% \n  compile(\n    loss = 'binary_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n\nmodel %% fit(xtrain, ytrain, batch_size = 16, epochs = 10)\nscore - model %% evaluate(xtest, ytest, batch_size = 16)\n\n Stacked LSTM for sequence classification\n\nIn this model, we stack 3 LSTM layers on top of each other,\nmaking the model capable of learning higher-level temporal representations.\n\nThe first two LSTMs return their full output sequences, but the last one only returns\nthe last step in its output sequence, thus dropping the temporal dimension\n(i.e. converting the input sequence into a single vector).\n\nimg src=\"../images/regularstackedlstm.png\" alt=\"stacked LSTM\" style=\"width: 300px;\"/\n\nlibrary(keras)\n\nconstants\ndata_dim <- 16\ntimesteps <- 8\nnum_classes <- 10\n\n define and compile model\nexpected input data shape: (batchsize, timesteps, datadim)\nmodel <- kerasmodelsequential() \nmodel %% \n  layerlstm(units = 32, returnsequences = TRUE, inputshape = c(timesteps, datadim)) %% \n  layerlstm(units = 32, returnsequences = TRUE) %% \n  layer_lstm(units = 32) %%  return a single vector dimension 32\n  layer_dense(units = 10, activation = 'softmax') %% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n  \ngenerate dummy training data\nxtrain <- array(runif(1000 * timesteps * datadim), dim = c(1000, timesteps, data_dim))\nytrain <- matrix(runif(1000 * numclasses), nrow = 1000, ncol = num_classes)\n\n generate dummy validation data\nxval <- array(runif(100 * timesteps * datadim), dim = c(100, timesteps, data_dim))\nyval <- matrix(runif(100 * numclasses), nrow = 100, ncol = num_classes)\n\ntrain\nmodel %% fit( \n  xtrain, ytrain, batchsize = 64, epochs = 5, validationdata = list(xval, yval)\n)\n\n Same stacked LSTM model, rendered \"stateful\"\n\nA stateful recurrent model is one for which the internal states (memories) obtained after processing a batch of samples are reused as initial states for the samples of the next batch. This allows to process longer sequences while keeping computational complexity manageable.\n\nYou can read more about stateful RNNs in the FAQ.\n\nlibrary(keras)\n\nconstants\ndata_dim <- 16\ntimesteps <- 8\nnum_classes <- 10\nbatch_size <- 32\n\n define and compile model\nExpected input batch shape: (batchsize, timesteps, datadim)\n Note that we have to provide the full batchinputshape since the network is stateful.\nthe sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel <- kerasmodelsequential()\nmodel %% \n  layerlstm(units = 32, returnsequences = TRUE, stateful = TRUE,\n             batchinputshape = c(batchsize, timesteps, datadim)) %% \n  layerlstm(units = 32, returnsequences = TRUE, stateful = TRUE) %% \n  layer_lstm(units = 32, stateful = TRUE) %% \n  layer_dense(units = 10, activation = 'softmax') %% \n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = 'rmsprop',\n    metrics = c('accuracy')\n  )\n  \n generate dummy training data\nxtrain <- array(runif( (batchsize * 10) * timesteps * data_dim), \n                 dim = c(batchsize * 10, timesteps, datadim))\nytrain <- matrix(runif( (batchsize * 10) * num_classes), \n                  nrow = batchsize * 10, ncol = numclasses)\n\ngenerate dummy validation data\nxval <- array(runif( (batchsize * 3) * timesteps * data_dim), \n               dim = c(batchsize * 3, timesteps, datadim))\nyval <- matrix(runif( (batchsize * 3) * num_classes), \n                nrow = batchsize * 3, ncol = numclasses)\n\n train\nmodel %% fit( \n  x_train, \n  y_train, \n  batchsize = batchsize, \n  epochs = 5, \n  shuffle = FALSE,\n  validationdata = list(xval, y_val)\n)\n\n","id":69},{"path":"/guide/keras/tools","title":"\"Tools\"","content":"\nlibrary(keras)\nknitr::opts_chunk$set(eval = FALSE)\n\nTools which provide various enhancements to Keras.\n\nstyle type=\"text/css\"\n.page-header  \n/style\n\nbr/\n\ndiv class=\"link-table\"\n\n|  |  |\n|---------------|---------------------------------------------------------------|\n| a href=\"https://tensorflow.rstudio.com/tools/gpu.html\"GPUsbr/img class=\"nav-image\" src=\"images/tools-gpu.png\" width=64//a  | It's highly recommended, although not strictly necessary, that you run deep-learning code on a modern NVIDIA GPU. Some applications -- in particular, image processing with convolutional networks and sequence processing with recurrent neural networks -- will be excruciatingly slow on CPU, even a fast multicore CPU. |\n| a href=\"https://tensorflow.rstudio.com/tools/cloudml/articles/getting_started.html\"CloudMLbr/img class=\"nav-image\" src=\"images/tools-cloud-ml.png\" width=64//a | The cloudml package provides an R interface to Google Cloud Machine Learning Engine, a managed service that provides on-demand access to training on GPUs, hyperparameter tuning to optmize key attributes of model architectures, and deployment of trained models to the Google global prediction platform. |\n| a href=\"https://tensorflow.rstudio.com/tools/training_flags.html\"Training Flagsbr/img class=\"nav-image\" src=\"images/tools-training-flags.png\" width=64//a | Tuning a model often requires exploring the impact of changes to many hyperparameters. The best way to approach this is generally not to progressively change your source code, but rather to define external flags for key parameters which you may want to vary.  |\n| a href=\"https://tensorflow.rstudio.com/tools/tfruns/articles/overview.html\"Training Runsbr/img class=\"nav-image\" src=\"images/tools-training-runs.png\" width=64//a  | The tfruns package provides a suite of tools for tracking and managing TensorFlow training runs and experiments from R. Track the hyperparameters, metrics, output, and source code of every training run, visualize the results of individual runs and comparisons between runs. |\n\n/div\n\n","id":70},{"path":"/guide/keras/training_callbacks","title":"\"Training Callbacks\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Training Callbacks\"","    identifier":"   identifier: \"keras-training-callbacks\"","    parent":"   parent: \"keras-advanced-top\"","    weight":"   weight: 30","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nOverview\n\nA callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the fit() function. The relevant methods of the callbacks will then be called at each stage of the training. \n\nFor example:\n\nlibrary(keras)\n\n generate dummy training data\ndata <- matrix(rexp(1000*784), nrow = 1000, ncol = 784)\nlabels <- matrix(round(runif(1000*10, min = 0, max = 9)), nrow = 1000, ncol = 10)\n\ncreate model\nmodel <- kerasmodelsequential() \n\n add layers and compile\nmodel %%\n  layerdense(32, inputshape = c(784)) %%\n  layer_activation('relu') %%\n  layer_dense(10) %%\n  layer_activation('softmax') %% \n  compile(\n    loss='binary_crossentropy',\n    optimizer = optimizer_sgd(),\n    metrics='accuracy'\n  )\n  \nfit with callbacks\nmodel %% fit(data, labels, callbacks = list(\n  callbackmodelcheckpoint(\"checkpoints.h5\"),\n  callbackreducelronplateau(monitor = \"val_loss\", factor = 0.1)\n))\n\n Built in Callbacks\n\nThe following built-in callbacks are available as part of Keras:\n\ntable class=\"ref-index\"\n\ntbody\ntr\n!--  --\ntd\ncallbackprogbarlogger()\n/td\ntdpCallback that prints metrics to stdout./p/td\n/trtr\n!--  --\ntd\ncallbackmodelcheckpoint()\n/td\ntdpSave the model after every epoch./p/td\n/trtr\n!--  --\ntd\ncallbackearlystopping()\n/td\ntdpStop training when a monitored quantity has stopped improving./p/td\n/trtr\n!--  --\ntd\ncallbackremotemonitor()\n/td\ntdpCallback used to stream events to a server./p/td\n/trtr\n!--  --\ntd\ncallbacklearningrate_scheduler()\n/td\ntdpLearning rate scheduler./p/td\n/trtr\n!--  --\ntd\ncallback_tensorboard()\n/td\ntdpTensorBoard basic visualizations/p/td\n/trtr\n!--  --\ntd\ncallbackreducelronplateau()\n/td\ntdpReduce learning rate when a metric has stopped improving./p/td\n/trtr\n!--  --\ntd\ncallbackcsvlogger()\n/td\ntdpCallback that streams epoch results to a csv file/p/td\n/trtr\n!--  --\ntd\ncallback_lambda()\n/td\ntdpCreate a custom callback/p/td\n/tr\n/tbody\n\n/table\n\nCustom Callbacks\n\nYou can create a custom callback by creating a new R6 class that inherits from the KerasCallback class. \n\nHere's a simple example saving a list of losses over each batch during training:\n\nlibrary(keras)\n\n define custom callback class\nLossHistory <- R6::R6Class(\"LossHistory\",\n  inherit = KerasCallback,\n  \n  public = list(\n    \n    losses = NULL,\n     \n    onbatchend = function(batch, logs = list())  \n))\n\ndefine model\nmodel <- kerasmodelsequential() \n\n add layers and compile\nmodel %% \n  layerdense(units = 10, inputshape = c(784)) %% \n  layer_activation(activation = 'softmax') %% \n  compile(\n    loss = 'categorical_crossentropy', \n    optimizer = 'rmsprop'\n  )\n\ncreate history callback object and use it during training\nhistory <- LossHistory$new()\nmodel %% fit(\n  Xtrain, Ytrain,\n  batch_size=128, epochs=20, verbose=0,\n  callbacks= list(history)\n)\n\n print the accumulated losses\nhistory$losses\n\n[1] 0.6604760 0.3547246 0.2595316 0.2590170 ...\n\nFields\n\nCustom callback objects have access to the current model and it's training parameters via the following fields:\n\nself$params\n\n:    Named list with training parameters (eg. verbosity, batch size, number of epochs...).\n\nself$model\n\n:    Reference to the Keras model being trained.\n\n Methods\n\nCustom callback objects can implement one or more of the following methods:\n\nonepochbegin(epoch, logs)\n\n:    Called at the beginning of each epoch.\n\nonepochend(epoch, logs)\n\n:    Called at the end of each epoch.\n\nonbatchbegin(batch, logs)\n\n:    Called at the beginning of each batch.\n\nonbatchend(batch, logs)\n\n:    Called at the end of each batch.\n\nontrainbegin(logs)\n\n:    Called at the beginning of training.\n\nontrainend(logs)\n\n:    Called at the end of training.\n\nontrainbatch_begin \n\n:    Called at the beginning of every batch.\n\nontrainbatch_end \n\n:    Called at the end of every batch.`\n\nonpredictbatch_begin \n\n:    Called at the beginning of a batch in predict methods.\n\nonpredictbatch_end \n\n:    Called at the end of a batch in predict methods.\n\nonpredictbegin \n\n:    Called at the beginning of prediction.\n\nonpredictend \n\n:    Called at the end of prediction.\n\nontestbatch_begin \n\n:    Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided.\n     \nontestbatch_end \n\n:    Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided.\n\nontestbegin \n\n:    Called at the beginning of evaluation or validation.\n\nontestend \n\n:    Called at the end of evaluation or validation.\n\n","id":71},{"path":"/guide/keras/training_visualization","title":"\"Training Visualization\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Training Visualization\"","    identifier":"   identifier: \"keras-training-visualization\"","    parent":"   parent: \"keras-advanced-top\"","    weight":"   weight: 50","content":"\nlibrary(keras)\nknitr::opts_chunk$set(comment = NA, eval = FALSE)\n\nOverview\n\nThere are a number of tools available for visualizing the training of Keras models, including:\n\n1) A plot method for the Keras training history returned from fit().\n\n2) Real time visualization of training metrics within the RStudio IDE.\n\n3) Integration with the TensorBoard visualization tool included with TensorFlow. Beyond just training metrics, TensorBoard has a wide variety of other visualizations available including the underlying TensorFlow graph, gradient histograms, model weights, and more. TensorBoard also enables you to compare metrics across multiple training runs.\n\nEach of these tools is described in more detail below.\n\n Plotting History\n\nThe Keras fit() method returns an R object containing the training history, including the value of metrics at the end of each epoch . You can plot the training metrics by epoch using the plot() method. \n\nFor example, here we compile and fit a model with the \"accuracy\" metric:\n\nmodel %% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory - model %% fit(\n  xtrain, ytrain, \n  epochs = 30, batch_size = 128, \n  validation_split = 0.2\n)\n\nWe can then plot the training history as follows:\n\nplot(history)\n\n \n\nThe history will be plotted using ggplot2 if available (if not then base graphics will be used), include all specified metrics as well as the loss, and draw a smoothing line if there are 10 or more epochs. You can customize all of this behavior via various options of the plot method.\n\nIf you want to create a custom visualization you can call the as.data.frame() method on the history to obtain a data frame with factors for each metric as well as training vs. validation:\n\nhistory_df <- as.data.frame(history)\nstr(history_df)\n\n'data.frame':\t120 obs. of  4 variables:\n $ epoch : int  1 2 3 4 5 6 7 8 9 10 ...\n $ value : num  0.87 0.941 0.954 0.962 0.965 ...\n $ metric: Factor w/ 2 levels \"acc\",\"loss\": 1 1 1 1 1 1 1 1 1 1 ...\n $ data  : Factor w/ 2 levels \"training\",\"validation\": 1 1 1 1 1 1 1 1 1 1 ...\n\nRStudio IDE \n\nIf you are training your model within the RStudio IDE then real time metrics are available within the Viewer pane:\n\nrstudiohistory  \n\na id=\"rstudiohistory\" href=\"https://rstudioblog.files.wordpress.com/2017/08/rstudio-training-metrics-full.gif\"/a\nscript type=\"application/javascript\"\nfunction isMobilePhone()  \n  catch(e)  \n \nvar a = document.getElementById(\"rstudiohistory\");\nvar img = document.createElement(\"IMG\");\nimg.className = \"r-plot\";\nimg.width = 640;\nimg.height = 502;\nif (!isMobilePhone())   else  \na.appendChild(img);\n/script\n\nThe view_metrics argument of the fit() method controls whether real time metrics are displayed. By default metrics are automatically displayed if one or more metrics are specified in the call to compile() and there is more than one training epoch. \n\nYou can explicitly control whether metrics are displayed by specifying the viewmetrics argument. You can also set a global session default using the keras.viewmetrics option:\n\ndon't show metrics during this run\nhistory - model %% fit(\n  xtrain, ytrain, \n  epochs = 30, batch_size = 128, \n  view_metrics = FALSE,\n  validation_split = 0.2\n)\n\n set global default to never show metrics\noptions(keras.view_metrics = FALSE)\n\nNote that when view_metrics is TRUE metrics will be displayed even when not running within RStudio (in that case metrics will be displayed in an external web browser).\n\nTensorBoard\n\nTensorBoard is a visualization tool included with TensorFlow that enables you to visualize dynamic graphs of your Keras training and test metrics, as well as activation histograms for the different layers in your model. \n\nFor example, here's a TensorBoard display for Keras accuracy and loss metrics:\n\n \n\n Recording Data\n\nTo record data that can be visualized with TensorBoard, you add a TensorBoard callback to the fit() function. For example:\n\nhistory - model %% fit(\n  xtrain, ytrain,\n  batchsize = batchsize,\n  epochs = epochs,\n  verbose = 1,\n  callbacks = callbacktensorboard(\"logs/runa\"),\n  validation_split = 0.2\n)\n\nSee the documentation on the callbacktensorboard() function for the various available options. The most important option is the logdir, which determines which directory logs are written to for a given training run. \n\nYou should either use a distinct log directory for each training run or remove the log directory between runs.\n\nViewing Data\n\nTo view TensorBoard data for a given set of runs you use the tensorboard() function, pointing it to the previously specified log_dir:\n\ntensorboard(\"logs/run_a\")\n\nIt's often useful to run TensorBoard while you are training a model. To do this, simply launch tensorboard within the training directory right before you begin training:\n\n launch TensorBoard (data won't show up until after the first epoch)\ntensorboard(\"logs/run_a\")\n\nfit the model with the TensorBoard callback\nhistory - model %% fit(\n  xtrain, ytrain,\n  batchsize = batchsize,\n  epochs = epochs,\n  verbose = 1,\n  callbacks = callbacktensorboard(\"logs/runa\"),\n  validation_split = 0.2\n)\n\nKeras writes TensorBoard data at the end of each epoch so you won't see any data in TensorBoard until 10-20 seconds after the end of the first epoch (TensorBoard automatically refreshes it's display every 30 seconds during training). \n\n Comparing Runs\n\nTensorBoard will automatically include all runs logged within the sub-directories of the specified log_dir, for example, if you logged another run using:\n\ncallbacktensorboard(logdir = \"logs/run_b\")\n\nThen called tensorboard as follows:\n\ntensorboard(\"logs\")\n\nThe TensorBoard visualization would look like this:\n\n \n\nYou can also pass multiple log directories. For example:\n\ntensorboard(c(\"logs/runa\", \"logs/runb\"))\n\nCustomization\n\n Metrics\n\nIn the above examples TensorBoard metrics are logged for loss and accuracy. The TensorBoard callback will log data for any metrics which are specified in the metrics parameter of the compile() function. For example, in the following code:\n\nmodel %% compile(\n  loss = 'meansquarederror',\n  optimizer = 'sgd',\n  metrics= c('mae', 'acc')\n)\n\nTensorBoard data series will be created for the loss (mean squared error) as well as for the mean absolute error and accuracy metrics.\n\nOptions\n\nThe callback_tensorboard() function includes a number of other options that control logging during training:\n\ncallbacktensorboard(logdir = \"logs\", histogram_freq = 0,\n  writegraph = TRUE, writeimages = FALSE, embeddings_freq = 0,\n  embeddingslayernames = NULL, embeddings_metadata = NULL)\n\n| Name  | Description |\n|-----------------|-------------------------------------------------------------------|\n| log_dir  | Path of the directory to save the log files to be parsed by Tensorboard. |\n| histogram_freq  | Frequency (in epochs) at which to compute activation histograms for the layers of the model. If set to 0 (the default), histograms won't be computed.  |\n| writegraph | Whether to visualize the graph in Tensorboard. The log file can become quite large when writegraph is set to TRUE  |\n| write_images  | Whether to write model weights to visualize as image in Tensorboard.  |\n| embeddings_freq | Frequency (in epochs) at which selected embedding layers will be saved.  |\n| embeddingslayernames | A list of names of layers to keep eye on. If NULL or empty list all the embedding layers will be watched.  |\n| embeddings_metadata  | A named list which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about the metadata file format. In case if the same metadata file is used for all embedding layers, string can be passed.  |\n\n","id":72},{"path":"/guide/saving/checkpoints","title":"\"Checkpoints\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfhub","menu":"menu:","  main":" main:","    name":"   name: \"Checkpoints\"","    identifier":"   identifier: \"checkpoints\"","    parent":"   parent: \"saving-checkpoint-top\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#\", \n  eval = TRUE\n)\n\nThe phrase \"Saving a TensorFlow model\" typically means one of two things:\n\nCheckpoints, OR\nSavedModel.\n\nCheckpoints capture the exact value of all parameters (tf$Variable objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\n\nThe SavedModel format on the other hand includes a serialized description of the computation defined by the model in addition to the parameter values (checkpoint). Models in this format are independent of the source code that created the model. They are thus suitable for deployment via TensorFlow Serving, TensorFlow Lite, TensorFlow.js, or programs in other programming languages (the C, C++, Java, Go, Rust, C# etc. TensorFlow APIs).\n\nThis guide covers APIs for writing and reading checkpoints.\n\nSetup\n\nlibrary(keras)\nlibrary(tensorflow)\n\n A simple linear model\ncreate_model <- function(size)  \n   )\n \n\nmodel <- create_model(5)\n\nSaving models through the Keras API\n\nThe respective Keras guide explains how to use the Keras API to save and restore complete models as well as model weights.\n\nCalling save_weights effectively results in saving a TensorFlow checkpoint:\n\nmodel %% savemodelweightstf(\"weightsonly\")\n\nMore customization is available through lower-level TensorFlow methods.\n\n Writing checkpoints\n\nThe persistent state of a TensorFlow model is stored in tf$Variable objects. These can be constructed directly, but are often created through high-level APIs like Keras layers or models.\n\nThe easiest way to manage variables is by attaching them to Python objects, then referencing those objects.\n\nSubclasses of tf$train$Checkpoint, tf$keras$layers$Layer, and tf$keras$Model automatically track variables assigned to their attributes. The following example constructs a simple linear model, then writes checkpoints which contain values for all of the model's variables.\n\nManual checkpointing\n\n Setup\n\nTo help demonstrate all the features of  tf$train$Checkpoint, define a toy dataset and an optimization step:\n\nlibrary(tfdatasets)\ntoy_dataset <- function()  \n\ntrain_step <- function(model, example, optimizer)  )\n  variables <- model$trainable_variables\n  gradients <- tape$gradient(loss, variables)\n  optimizer$apply_gradients(purrr::transpose(list(gradients, variables)))\n  loss\n \n\nCreate the checkpoint objects\n\nTo manually make a checkpoint you will need a tf$train$Checkpoint object. Where the objects you want to checkpoint are set as attributes on the object.\n\nA tf$train$CheckpointManager can also be helpful for managing multiple checkpoints.\n\nopt <- optimizer_adam(0.1)\nckpt <- tf$train$Checkpoint(step = tf$Variable(1), optimizer = opt, net = model)\nmanager <- tf$train$CheckpointManager(ckpt, './tfckpts', maxto_keep = 3)\n\n Train and checkpoint the model\n\nThe following training loop creates an instance of the model and of an optimizer, then gathers them into a tf$train$Checkpoint object. It calls the training step in a loop on each batch of data, and periodically writes checkpoints to disk.\n\nlibrary(glue)\nlibrary(tfautograph)\n\ntrainandcheckpoint <- autograph(function(model, manager, dataset)  \\n\"))\n    else  \n\n  for (example in dataset())  :  \"))\n      tf$print(glue(\"loss:  \"))\n     \n   \n )\n\ntrainandcheckpoint(model, manager, toy_dataset)\n\nRestore and continue training\n\nAfter the first you can pass a new model and manager, but pickup training exactly where you left off:\n\nopt <- optimizer_adam(0.1)\nnet <- create_model(5)\nckpt <- tf$train$Checkpoint(step = tf$Variable(1), optimizer = opt, net = net)\nmanager <- tf$train$CheckpointManager(ckpt, './tfckpts', maxto_keep = 3)\n\ntrainandcheckpoint(net, manager, toy_dataset)\n\nThe tf$train$CheckpointManager object deletes old checkpoints. Above it's configured to keep only the three most recent checkpoints.\n\nmanager$checkpoints   List the three remaining checkpoints\n\nThese paths, e.g. './tfckpts/ckpt-10', are not files on disk. Instead they are prefixes for an index file and one or more data files which contain the variable values. These prefixes are grouped together in a single checkpoint file ('./tfckpts/checkpoint') where the CheckpointManager saves its state.\n\nlist.files(\"./tf_ckpts/\")\n\nTensorFlow matches variables to checkpointed values by traversing a directed graph with named edges, starting from the object being loaded. Edge names typically come from attribute names in objects, for example the \"l1\" in self$l1 <- layer_dense(units = size). tf$train$Checkpoint uses its keyword argument names, as in the \"step\" in tf$train$CheckpointManager(step = ...).\n\nThe dependency graph from the example above looks like this:\n\nknitr::include_graphics(\"graph.svg\")\n\nWith the optimizer in red, regular variables in blue, and optimizer slot variables in orange. The other nodes, for example representing the tf$train$Checkpoint, are black.\n\nSlot variables are part of the optimizer's state, but are created for a specific variable. For example the m edges above correspond to momentum, which the Adam optimizer tracks for each variable. Slot variables are only saved in a checkpoint if the variable and the optimizer would both be saved, thus the dashed edges.\n\nCalling restore() on a tf$train$Checkpoint object queues the requested restorations, restoring variable values as soon as there's a matching path from the Checkpoint object. For example we can load just the kernel from the model we defined above by reconstructing one path to it through the network and the layer.\n\nto_restore <- tf$Variable(tf$zeros(list(5L)))\nas.numeric(to_restore)  # All zeros\nfakelayer <- tf$train$Checkpoint(bias = torestore)\nfakenet <- tf$train$Checkpoint(l1 = fakelayer)\nnewroot <- tf$train$Checkpoint(net = fakenet)\nstatus <- newroot$restore(tf$train$latestcheckpoint('./tf_ckpts/'))\nas.numeric(to_restore)  # We get the restored value now\n\nThe dependency graph for these new objects is a much smaller subgraph of the larger checkpoint we wrote above. It includes only the bias and a save counter that tf$train$Checkpoint uses to number checkpoints.\n\nknitr::includegraphics(\"smallgraph.svg\")\n\nrestore() returns a status object, which has optional assertions. All of the objects we've created in our new Checkpoint have been restored, so status.assertexistingobjects_matched() passes.\n\nstatus$assertexistingobjects_matched()\n\nThere are many objects in the checkpoint which haven't matched, including the layer's kernel and the optimizer's variables. status$assert_consumed() only passes if the checkpoint and the program match exactly, and would throw an exception here.\n\nDelayed restorations\n\nLayer objects in TensorFlow may delay the creation of variables to their first call, when input shapes are available. For example the shape of a Dense layer's kernel depends on both the layer's input and output shapes, and so the output shape required as a constructor argument is not enough information to create the variable on its own. Since calling a Layer also reads the variable's value, a restore must happen between the variable's creation and its first use.\n\nTo support this idiom, tf$train$Checkpoint queues restores which don't yet have a matching variable.\n\ndelayed_restore <- tf$Variable(tf$zeros(list(1L, 5L)))\nas.numeric(delayed_restore)   Not restored; still zeros\nfakelayer$kernel <- delayedrestore\nas.numeric(delayed_restore)  # Restored\n\nManually inspecting checkpoints\n\ntf$train$list_variables lists the checkpoint keys and shapes of variables in a checkpoint. Checkpoint keys are paths in the graph displayed above.\n\ntf$train$listvariables(tf$train$latestcheckpoint('./tf_ckpts/'))\n\n List and dictionary tracking\n\nAs with direct attribute assignments like self$l1 <- layer_dense(units = size), assigning lists and dictionaries to attributes will track their contents.\n\nsave <- tf$train$Checkpoint()\nsave$listed <- list(tf$Variable(1))\nsave$listed$append(tf$Variable(2))\nsave$mapped <- dict(one = save$listed[0])\nreticulate::pysetitem(save$mapped, \"two\", save$listed[1])\nsave$mapped['two'] <- save$listed[1]\nsavepath <- save$save('./tflist_example')\n\nrestore <- tf$train$Checkpoint()\nv2 <- tf$Variable(0)\n0 == as.numeric(v2)   Not restored yet\nrestore$mapped <- dict(two = v2)\nrestore$restore(save_path)\n2 == as.numeric(v2)\n\nYou may notice wrapper objects for lists and dictionaries. These wrappers are checkpointable versions of the underlying data-structures. Just like the attribute based loading, these wrappers restore a variable's value as soon as it's added to the container.\n\nrestore$listed <- list()\nprint(restore$listed)  # ListWrapper([])\nv1 <- tf$Variable(0.)\nrestore$listed$append(v1)  # Restores v1, from restore() in the previous cell\n as.numeric(v1)\n\nThe same tracking is automatically applied to subclasses of tf.keras.Model, and may be used for example to track lists of layers.\n\n","id":73},{"path":"/guide/saving/saved_model","title":"\"Saved Model\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfhub","menu":"menu:","  main":" main:","    name":"   name: \"Saved Model\"","    identifier":"   identifier: \"saved_model\"","    parent":"   parent: \"saving-savedmodel-top\"","    weight":"   weight: 20","content":"\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#\", \n  eval = TRUE\n)\n\nA SavedModel contains a complete TensorFlow program, including weights and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying (with TFLite, TensorFlow.js, TensorFlow Serving, or TensorFlow Hub).\n\nIf you have code for a model in R and merely want to load weights into it, see the guide to training checkpoints.\n\nCreating a SavedModel from Keras\n\nFor a quick introduction, this section exports a pre-trained Keras model and serves image classification requests with it. The rest of the guide will fill in details and discuss other ways to create SavedModels.\n\nlibrary(keras)\nlibrary(tensorflow)\n\nphysicaldevices <- tf$config$experimental$listphysical_devices('GPU')\nif (length(physical_devices) != 0)\n  tf$config$experimental$setmemorygrowth(physical_devices[[1]], TRUE)\nWe'll use an image of Grace Hopper as a running example, and a Keras pre-trained image classification model since it's easy to use. Custom models work too, and are covered in detail later.\n\nfile <- get_file(\n    \"grace_hopper.jpg\",\n    \"https://storage.googleapis.com/download.tensorflow.org/exampleimages/gracehopper.jpg\")\nlibrary(magick)\nimg <- image_read(file)\nimg <- image_draw(img)\nimg\n\nx - imageload(file, targetsize = list(224, 224)) %%\n  imagetoarray() %% \n  mobilenetpreprocessinput\ndim(x) <- c(1, dim(x))\n\npretrained_model <- tf$keras$applications$MobileNet()\nresultbeforesave <- pretrained_model(x)\npreds <- imagenetdecodepredictions(as.matrix(resultbeforesave))\n\ntf$savedmodel$save(pretrainedmodel, \"/tmp/mobilenet/1/\")\n\nThe save-path follows a convention used by TensorFlow Serving where the last path component (1/ here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.\n\nSavedModels have named functions called signatures. Keras models export their forward pass under the serving_default signature key. The SavedModel command line interface (see below) is useful for inspecting SavedModels on disk:\n\nsavedmodelcli show --dir /tmp/mobilenet/1 --tagset serve --signaturedef serving_default\n\nWe can load the SavedModel back into Python with tf.saved_model.load and see how Admiral Hopper's image is classified.\n\nloaded <- tf$saved_model$load(\"/tmp/mobilenet/1/\")\nloaded$signatures$keys()   [\"serving_default\"]\nImported signatures always return dictionaries.\n\ninfer <- loaded$signatures[\"serving_default\"]\ninfer$structured_outputs\n\nRunning inference from the SavedModel gives the same result as the original model.\n\nlabeling <- infer(tf$constant(x, dtype = tf$float32))[pretrainedmodel$outputnames[1]]\nimagenetdecodepredictions(as.matrix(labeling[[1]]))\n\nThe SavedModel format on disk\n\nA SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n\nls /tmp/mobilenet/1\n\nThe saved_model.pb file stores the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs.\n\nSavedModels may contain multiple variants of the model (multiple v1$MetaGraphDefs, identified with the --tagset flag to savedmodel_cli), but this is rare. \n\nsavedmodelcli show --dir /tmp/mobilenet/1 --tag_set serve\n\nThe variables directory contains a standard training checkpoint (see the guide to training checkpoints).\n\nls /tmp/mobilenet/1/variables\n\nThe assets directory contains files used by the TensorFlow graph, for example text files used to initialize vocabulary tables. It is unused in this example.\n\nSavedModels may have an assets$extra directory for any files not used by the TensorFlow graph, for example information for consumers about what to do with the SavedModel. TensorFlow itself does not use this directory.\n\n Load a SavedModel in C++\n\nThe C++ version of the SavedModel loader provides an API to load a SavedModel from a path, while allowing SessionOptions and RunOptions. You have to specify the tags associated with the graph to be loaded. The loaded version of SavedModel is referred to as SavedModelBundle and contains the MetaGraphDef and the session within which it is loaded.\n\nconst string export_dir = ...\nSavedModelBundle bundle;\n...\nLoadSavedModel(sessionoptions, runoptions, export_dir,  ,\n               &bundle);\n\nDetails of the SavedModel command line interface\n\nYou can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. For example, you can use the CLI to inspect the model's SignatureDefs. The CLI enables you to quickly confirm that the input Tensor dtype and shape match the model. Moreover, if you want to test your model, you can use the CLI to do a sanity check by passing in sample inputs in various formats (for example, Python expressions) and then fetching the output.\n\n Install the SavedModel CLI\n\nBroadly speaking, you can install TensorFlow in either of the following two ways:\n\nBy installing a pre-built TensorFlow binary.\nBy building TensorFlow from source code.\n\nIf you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system at pathname bin/savedmodelcli.\n\nIf you built TensorFlow from source code, you must run the following additional command to build savedmodelcli:\n\n$ bazel build tensorflow/python/tools:savedmodelcli\n\nOverview of commands\n\nThe SavedModel CLI supports the following two commands on a SavedModel:\n\nshow, which shows the computations available from a SavedModel.\nrun, which runs a computation from a SavedModel.\n\n show command\n\nA SavedModel contains one or more model variants (technically, v1.MetaGraphDefs), identified by their tag-sets. To serve a model, you might wonder what kind of SignatureDefs are in each model variant, and what are their inputs and outputs. The show command let you examine the contents of the SavedModel in hierarchical order. Here's the syntax:\n\nusage: savedmodelcli show [-h] --dir DIR [--all]\n[--tagset TAGSET] [--signaturedef SIGNATUREDEF_KEY]\n\nFor example, the following command shows all available tag-sets in the SavedModel:\n\nsavedmodelcli show --dir /tmp/savedmodeldir\n\nThe following command shows all available SignatureDef keys for a tag set:\n\nsavedmodelcli show --dir /tmp/savedmodeldir --tag_set serve\n\nIf there are multiple tags in the tag-set, you must specify all tags, each tag separated by a comma. For example:\n\n$ savedmodelcli show --dir /tmp/savedmodeldir --tag_set serve,gpu\n\nTo show all inputs and outputs TensorInfo for a specific SignatureDef, pass in the SignatureDef key to signature_def option. This is very useful when you want to know the tensor key value, dtype and shape of the input tensors for executing the computation graph later. For example:\n\nsavedmodelcli show --dir \\\n/tmp/savedmodeldir --tagset serve --signaturedef serving_default\n\nTo show all available information in the SavedModel, use the --all option. For example:\n\nsavedmodelcli show --dir /tmp/savedmodeldir --all\n\nrun command\n\nInvoke the run command to run a graph computation, passing inputs and then displaying (and optionally saving) the outputs. Here's the syntax:\n\nusage: savedmodelcli run [-h] --dir DIR --tagset TAGSET --signature_def\n                           SIGNATUREDEFKEY [--inputs INPUTS]\n                           [--inputexprs INPUTEXPRS]\n                           [--inputexamples INPUTEXAMPLES] [--outdir OUTDIR]\n                           [--overwrite] [--tf_debug]\n\nThe run command provides the following three ways to pass inputs to the model:\n\n--inputs option enables you to pass numpy ndarray in files.\n--input_exprs option enables you to pass Python expressions.\n--input_examples option enables you to pass tf$train$Example.\n\n --inputs\n\nTo pass input data in files, specify the --inputs option, which takes the following general format:\n\n--inputs INPUTS\n\nwhere INPUTS is either of the following formats:\n\ninput_key=filename\ninputkey=filename[variablename]\n\nYou may pass multiple INPUTS. If you do pass multiple inputs, use a semicolon to separate each of the INPUTS.\n\nsavedmodelcli uses numpy.load to load the filename. The filename may be in any of the following formats:\n\n.npy\n.npz\npickle format\n\nA .npy file always contains a numpy ndarray. Therefore, when loading from a .npy file, the content will be directly assigned to the specified input tensor. If you specify a variablename with that .npy file, the variablename will be ignored and a warning will be issued.\n\nWhen loading from a .npz (zip) file, you may optionally specify a variablename to identify the variable within the zip file to load for the input tensor key. If you don't specify a variablename, the SavedModel CLI will check that only one file is included in the zip file and load it for the specified input tensor key.\n\nWhen loading from a pickle file, if no variablename is specified in the square brackets, whatever that is inside the pickle file will be passed to the specified input tensor key. Otherwise, the SavedModel CLI will assume a dictionary is stored in the pickle file and the value corresponding to the variablename will be used.\n\n--input_exprs\n\nTo pass inputs through Python expressions, specify the --input_exprs option. This can be useful for when you don't have data files lying around, but still want to sanity check the model with some simple inputs that match the dtype and shape of the model's SignatureDefs. For example:\n\ninput_key=[[1],[2],[3]]\n\nIn addition to Python expressions, you may also pass numpy functions. For example:\n\ninput_key=np.ones((32,32,3))\n\n(Note that the numpy module is already available to you as np.)\n\n --input_examples\n\nTo pass tf$train$Example as inputs, specify the --input_examples option. For each input key, it takes a list of dictionary, where each dictionary is an instance of tf$train$Example. The dictionary keys are the features and the values are the value lists for each feature. For example:\n\ninput_key=[ ]\n\nSave output\n\nBy default, the SavedModel CLI writes output to stdout. If a directory is passed to --outdir option, the outputs will be saved as .npy files named after output tensor keys under the given directory.\n\nUse --overwrite to overwrite existing output files.\n\n","id":74},{"path":"/guide/tensorflow/eager_execution","title":"\"Eager execution\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"\"docs\"","menu":"menu:","  main":" main:","    name":"   name: \"Eager execution\"","    identifier":"   identifier: \"custom-basic-eager-execution\"","    parent":"   parent: \"custom-basic-top\"","    weight":"   weight: 10 ","content":"\n\nknitr::opts_chunk$set(eval = TRUE)\n\nTensorFlow's eager execution is an imperative programming environment that\nevaluates operations immediately, without building graphs: operations return\nconcrete values instead of constructing a computational graph to run later. This\nmakes it easy to get started with TensorFlow and debug models, and it\nreduces boilerplate as well. To follow along with this guide, run the code\nsamples below in an interactive R interpreter.\n\nEager execution is a flexible machine learning platform for research and\nexperimentation, providing:\n\nAn intuitive interface—Structure your code naturally and use R data\n  structures. Quickly iterate on small models and small data.\nEasier debugging—Call ops directly to inspect running models and test\n  changes. Use standard R debugging tools for immediate error reporting.\nNatural control flow—Use R control flow instead of graph control\n  flow, simplifying the specification of dynamic models.\n\nEager execution supports most TensorFlow operations and GPU acceleration.\n\nNote: Some models may experience increased overhead with eager execution\nenabled. Performance improvements are ongoing, but please\nfile a bug if you find a\nproblem and share your benchmarks.\n\nSetup and basic usage\n\nlibrary(tensorflow)\nlibrary(tfautograph)\nlibrary(keras)\nlibrary(tfdatasets)\n\nIn Tensorflow 2.0, eager execution is enabled by default.\n\ntf$executing_eagerly()\n\nNow you can run TensorFlow operations and the results will return immediately:\n\nx <- matrix(2, ncol = 1, nrow = 1)\nm <- tf$matmul(x, x)\nm\n\nEnabling eager execution changes how TensorFlow operations behave—now they\nimmediately evaluate and return their values to R tf$Tensor objects\nreference concrete values instead of symbolic handles to nodes in a computational\ngraph. Since there isn't a computational graph to build and run later in a\nsession, it's easy to inspect results using print() or a debugger. Evaluating,\nprinting, and checking tensor values does not break the flow for computing\ngradients.\n\nEager execution works nicely with R. TensorFlow\nmath operations convert\nR objects and R arrays to tf$Tensor objects. The\nas.array method returns the object's value as an R array.\n\na <- tf$constant(matrix(c(1,2,3,4), ncol = 2))\na\n\n Broadcasting support\nb <- tf$add(a, 1)\nb\n\nOperator overloading is supported\na * b\n\n Obtain an R value from a Tensor\nas.array(a)\n\nDynamic control flow\n\nA major benefit of eager execution is that all the functionality of the host\nlanguage is available while your model is executing. So, for example,\nit is easy to write fizzbuzz:\n\nfizzbuzz <- autograph(function(max_num)   else if (num %% 3 == 0)   else if (num %% 5 == 0)   else  \n    counter <- counter + 1\n   \n )\nfizzbuzz(15)\n\nThis has conditionals that depend on tensor values and it prints these values\nat runtime.\n\n Eager training\n\nComputing gradients\n\nAutomatic differentiation\nis useful for implementing machine learning algorithms such as\nbackpropagation for training\nneural networks. During eager execution, use tf$GradientTape to trace\noperations for computing gradients later.\n\nYou can use tf$GradientTape to train and/or compute gradients in eager. It is especially useful for complicated training loops.  \n\nSince different operations can occur during each call, all\nforward-pass operations get recorded to a \"tape\". To compute the gradient, play\nthe tape backwards and then discard. A particular tf$GradientTape can only\ncompute one gradient; subsequent calls throw a runtime error.\n\nw <- tf$Variable(1)\nwith(tf$GradientTape() %as% tape,  )\ngrad <- tape$gradient(loss, w)\ngrad\n\n Train a model\n\nThe following example creates a multi-layer model that classifies the standard\nMNIST handwritten digits. It demonstrates the optimizer and layer APIs to build\ntrainable graphs in an eager execution environment.\n\nFetch and format the mnist data\nmnist <- dataset_mnist()\ndataset - tensorslicesdataset(mnist$train) %% \n  dataset_map(function(x)  ) %% \n  dataset_shuffle(1000) %% \n  dataset_batch(32)\n\ndataset\n\nmnistmodel - kerasmodel_sequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation= \"relu\",\n                input_shape = shape(NULL, NULL, 1)) %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layerglobalaveragepooling2d() %% \n  layer_dense(units = 10)\n\nEven without training, call the model and inspect the output in eager execution:\n\nel - dataset %% \n  dataset_take(1) %% \n  dataset_collect()\nmnist_model(el[[1]])\n\nWhile keras models have a builtin training loop (using the fit method), sometimes you need more customization. Here's an example, of a training loop implemented with eager:\n\noptimizer <- optimizer_adam()\nlossobject <- tf$keras$losses$SparseCategoricalCrossentropy(fromlogits = TRUE)\n\nloss_history <- list()\n\nNote: Use the assert functions in tf$debugging to check if a condition holds up. This works in eager and graph execution.\n\ntrain_step <- function(images, labels)  )\n  losshistory <<- append(losshistory, loss_value)\n  grads <- tape$gradient(lossvalue, mnistmodel$trainable_variables)\n  optimizer$apply_gradients(\n    purrr::transpose(list(grads, mnistmodel$trainablevariables))\n  )\n \n\ntrain <- autograph(function()  \n    tf$print(\"Epoch\", epoch, \"finished.\")\n   \n )\n\ntrain()\n\nhistory - loss_history %% \n  purrr::map(as.numeric) %% \n  purrr::flatten_dbl()\nggplot2::qplot(x = seq_along(history), y = history, geom = \"line\")\n\n Variables and optimizers\n\ntf$Variable objects store mutable tf$Tensor-like values accessed during\ntraining to make automatic differentiation easier. \n\nThe collections of variables can be encapsulated into layers or models, along with methods that operate on them. See Custom Keras layers and models for details. The main difference between layers and models is that models add methods like  fit, evaluate, and save.\n\nFor example, the automatic differentiation example above\ncan be rewritten:\n\nLinear <- function()  \n   ) \n \n\nA toy dataset of points around 3 * x + 2\nNUM_EXAMPLES <- 2000\ntraininginputs <- tf$random$normal(shape = shape(NUMEXAMPLES))\nnoise <- tf$random$normal(shape = shape(NUM_EXAMPLES))\ntrainingoutputs <- traininginputs * 3 + 2 + noise\n\n The loss function to be optimized\nloss <- function(model, inputs, targets)  \n\ngrad <- function(model, inputs, targets)  )\n  tape$gradient(loss_value, list(model$w, model$b))\n \n\nNext:\n\nCreate the model.\nThe Derivatives of a loss function with respect to model parameters.\nA strategy for updating the variables based on the derivatives.\n\nmodel <- Linear()\noptimizer <- optimizer_sgd(lr = 0.01)\n\ncat(\"Initial loss: \", as.numeric(loss(model, traininginputs, trainingoutputs), \"\\n\"))\n\nfor (i in seq_len(300))  \n\nmodel$w\nmodel$b\n\nNote: Variables persist until the last reference to the object\nis removed, and is the variable is deleted.\n\nObject-based saving\n\nA Keras model includes a convinient save_weights method allowing you to easily create a checkpoint: \n\nsavemodelweights_tf(model, \"weights\")\nloadmodelweights_tf(model, filepath = \"weights\")\n\nUsing tf$train$Checkpoint you can take full control over this process.\n\nThis section is an abbreviated version of the guide to training checkpoints.\n\nx <- tf$Variable(10)\ncheckpoint <- tf$train$Checkpoint(x = x)\n\nx$assign(2)  Assign a new value to the variables and save.\ncheckpoint_path <- \"ckpt/\"\ncheckpoint$save(checkpoint_path)\n\nx$assign(11) # Change the variable after saving.\ncheckpoint$restore(tf$train$latestcheckpoint(checkpointpath))\nx\n\nTo save and load models, tf$train$Checkpoint stores the internal state of objects,\nwithout requiring hidden variables. To record the state of a model,\nan optimizer, and a global step, pass them to a tf$train$Checkpoint:\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 16, kernel_size = c(3,3), activation = \"relu\") %% \n  layerglobalaveragepooling2d() %% \n  layer_dense(units = 10)\n\noptimizer <- optimizer_adam(lr = 0.001)\n\ncheckpointdir <- 'path/to/modeldir'\nif (!dir.exists(checkpoint_dir))\n  dir.create(checkpoint_dir, recursive = TRUE)\n\ncheckpointprefix <- file.path(checkpointdir, \"ckpt\")\n\nroot <- tf$train$Checkpoint(optimizer = optimizer, model = model)\n\nroot$save(checkpoint_prefix)\nroot$restore(tf$train$latestcheckpoint(checkpointdir))\n\nNote: In many training loops, variables are created after tf$train$Checkpoint.restore is called. These variables will be restored as soon as they are created, and assertions are available to ensure that a checkpoint has been fully loaded. See the guide to training checkpoints for details.\n\nObject-oriented metrics\n\ntf$keras$metrics are stored as objects. Update a metric by passing the new data to\nthe callable, and retrieve the result using the tf$keras$metrics$result method,\nfor example:\n\nm <- tf$keras$metrics$Mean(\"loss\")\nm(0)\nm(5)\nm$result()\nm(c(8, 9))\nm$result()\n\n Summaries and TensorBoard\n\nTensorBoard is a visualization tool for\nunderstanding, debugging and optimizing the model training process. It uses\nsummary events that are written while executing the program.\n\nYou can use tf$summary to record summaries of variable in eager execution.\nFor example, to record summaries of loss once every 100 training steps:\n\nlogdir <- \"./tb/\"\nwriter = tf$summary$createfilewriter(logdir)\ntensorboard(log_dir = logdir) # This will open a browser window pointing to Tensorboard\n\nwith(writer$as_default(),  \n )\n\nAdvanced automatic differentiation topics\n\n Dynamic models\n\ntf$GradientTape can also be used in dynamic models. This example for a\nbacktracking line search\nalgorithm looks like normal R code, except there are gradients and is\ndifferentiable, despite the complex control flow:\n\nlinesearchstep <- tf$customgradient(autograph(function(fn, initx, rate = 1)  )\n  grad <- tape$gradient(value, init_x)\n  gradnorm <- tf$reducesum(grad * grad)\n  init_value <- value\n  while(value  (initvalue - rate * gradnorm))  \n  list(x, value)\n ))\n\nCustom gradients\n\nCustom gradients are an easy way to override gradients. Within the forward function, define the gradient with respect to the\ninputs, outputs, or intermediate results. For example, here's an easy way to clip\nthe norm of the gradients in the backward pass:\n\nclipgradientby_norm <- function(x, norm)  \n  list(y, grad_fn)\n \n\nCustom gradients are commonly used to provide a numerically stable gradient for a\nsequence of operations:\n\nlog1pexp <- function(x)  \n\ngrad_log1pexp <- function(x)  )\n  tape$gradient(value, x)\n \n\n The gradient computation works fine at x = 0.\ngrad_log1pexp(tf$constant(0))\nHowever, x = 100 fails because of numerical instability.\ngrad_log1pexp(tf$constant(100))\n\nHere, the log1pexp function can be analytically simplified with a custom\ngradient. The implementation below reuses the value for tf$exp(x) that is\ncomputed during the forward pass—making it more efficient by eliminating\nredundant calculations:\n\nlog1pexp <- tf$custom_gradient(f = function(x)  \n  list(tf$math$log(1 + e), grad_fn)\n )\n\ngrad_log1pexp <- function(x)  )\n  tape$gradient(value, x)\n \n\n As before, the gradient computation works fine at x = 0.\ngrad_log1pexp(tf$constant(0))\n\nAnd the gradient computation also works at x = 100.\ngrad_log1pexp(tf$constant(100))\n\n Performance\n\nComputation is automatically offloaded to GPUs during eager execution. If you\nwant control over where a computation runs you can enclose it in a\ntf$device('/gpu:0') block (or the CPU equivalent):\n\nfun <- function(device, steps = 200)    \n   )\n \nmicrobenchmark::microbenchmark(\n  fun(\"/cpu:0\"),\n  fun(\"/gpu:0\")\n)\n\nUnit: milliseconds\n           expr      min        lq      mean    median        uq       max neval\nfun(\"/cpu:0\") 1117.596 1135.5450 1165.6269 1157.2208 1195.1529 1300.2236   100\n  fun(\"/gpu:0\")  112.888  121.7164  127.8525  126.6708  132.0415  228.1009   100\n\nA tf$Tensor object can be copied to a different device to execute its\noperations:\n\nx <- tf$random$normal(shape = shape(10,10))\n\nx_gpu0 <- x$gpu()\nx_cpu <- x$cpu()\n\ntf$matmul(xcpu, xcpu)    # Runs on CPU\ntf$matmul(xgpu0, xgpu0)  # Runs on GPU:0\n\nBenchmarks\n\nFor compute-heavy models, such as\nResNet50\ntraining on a GPU, eager execution performance is comparable to tf_function execution.\nBut this gap grows larger for models with less computation and there is work to\nbe done for optimizing hot code paths for models with lots of small operations.\n\n Work with functions\n\nWhile eager execution makes development and debugging more interactive,\nTensorFlow 1.x style graph execution has advantages for distributed training, performance\noptimizations, and production deployment. To bridge this gap, TensorFlow 2.0 introduces functions via the tffunction API. For more information, see the tffunction guide.\n\n","id":75},{"path":"/guide/tensorflow/ragged_tensors","title":"\"Ragged tensors\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"\"docs\"","menu":"menu:","  main":" main:","    name":"   name: \"Ragged tensors\"","    identifier":"   identifier: \"custom-advanced-ragged\"","    parent":"   parent: \"custom-advanced-top\"","    weight":"   weight: 10","content":"\n\nknitr::opts_chunk$set(eval = TRUE)\n\nOverview\n\nYour data comes in many shapes; your tensors should too.\nRagged tensors are the TensorFlow equivalent of nested variable-length\nlists. They make it easy to store and process data with non-uniform shapes,\nincluding:\n\nVariable-length features, such as the set of actors in a movie.\nBatches of variable-length sequential inputs, such as sentences or video\n    clips.\nHierarchical inputs, such as text documents that are subdivided into\n    sections, paragraphs, sentences, and words.\nIndividual fields in structured inputs, such as protocol buffers.\n\n What you can do with a ragged tensor\n\nRagged tensors are supported by more than a hundred TensorFlow operations,\nincluding math operations (such as tf$add and tf$reduce_mean), array operations\n(such as tf$concat and tf$tile), string manipulation ops (such as\ntf$substr), and many others:\n\nlibrary(tensorflow)\ndigits <- tf$ragged$constant(\n  list(list(3, 1, 4, 1), list(), list(5, 9, 2), list(6), list())\n)\nwords = tf$ragged$constant(\n  list(list(\"So\", \"long\"), list(\"thanks\", \"for\", \"all\", \"the\", \"fish\"))\n)\ntf$add(digits, 3)\ntf$reduce_mean(digits, axis=1L)\ntf$concat(list(digits, list(list(5, 3))), axis=0L)\ntf$tile(digits, list(1L, 2L))\ntf$strings$substr(words, 0L, 2L)\n\nThere are also a number of methods and operations that are\nspecific to ragged tensors, including factory methods, conversion methods,\nand value-mapping operations.\n\nAs with normal tensors, you can use R-style indexing to access specific\nslices of a ragged tensor. For more information, see the section on\nIndexing below.\n\ndigits[1,] # First row\n\ndigits[, 1:2]   # First two values in each row.\n\ndigits[, -2:]  # Last two values in each row.\n\nAnd just like normal tensors, you can use Python arithmetic and comparison operators to perform \nelementwise operations. For more information, see the section on Overloaded Operators below.\n\ndigits + 3\n\ndigits + tf$ragged$constant(list(list(1, 2, 3, 4), list(), list(5, 6, 7), list(8), list()))\n\nIf you need to perform an elementwise transformation to the values of a RaggedTensor, you can use tf$ragged$mapflatvalues, which takes a function plus one or more arguments, and applies the function to transform the RaggedTensor's values.\n\ntimestwoplus_one <- function(x)  \ntf$ragged$mapflatvalues(timestwoplus_one, digits)\n\nConstructing a ragged tensor\n\nThe simplest way to construct a ragged tensor is using\ntf$ragged$constant, which builds the\nRaggedTensor corresponding to a given nested list:\n\nsentences <- tf$ragged$constant(list(\n    list(\"Let's\", \"build\", \"some\", \"ragged\", \"tensors\", \"!\"),\n    list(\"We\", \"can\", \"use\", \"tf.ragged.constant\", \".\")))\n\nparagraphs <- tf$ragged$constant(list(\n    list(list('I', 'have', 'a', 'cat'), list('His', 'name', 'is', 'Mat')),\n    list(list('Do', 'you', 'want', 'to', 'come', 'visit'), list(\"I'm\", 'free', 'tomorrow'))\n))\nparagraphs\n\nRagged tensors can also be constructed by pairing flat values tensors with\nrow-partitioning tensors indicating how those values should be divided into\nrows, using factory classmethods such as tf$RaggedTensor$fromvaluerowids,\ntf$RaggedTensor$fromrowlengths, and\ntf$RaggedTensor$fromrowsplits.\n\n tf$RaggedTensor$fromvaluerowids\n\nIf you know which row each value belongs in, then you can build a RaggedTensor using a value_rowids row-partitioning tensor:\n\ntf$RaggedTensor$fromvaluerowids(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    value_rowids=as.integer(c(0, 0, 0, 0, 2, 2, 2, 3)))\n\ntf.RaggedTensor.fromrowlengths\n\nIf you know how long each row is, then you can use a row_lengths row-partitioning tensor:\n\ntf$RaggedTensor$fromrowlengths(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    row_lengths=as.integer(c(4, 0, 3, 1)))\n\n tf.RaggedTensor.fromrowsplits\n\nIf you know the index where each row starts and ends, then you can use a row_splits row-partitioning tensor:\n\ntf$RaggedTensor$fromrowsplits(\n    values=as.integer(c(3, 1, 4, 1, 5, 9, 2, 6)),\n    row_splits=as.integer(c(0, 4, 4, 7, 8)))\n\nSee the tf.RaggedTensor class documentation for a full list of factory methods.\n\nWhat you can store in a ragged tensor\n\nAs with normal Tensors, the values in a RaggedTensor must all have the same\ntype; and the values must all be at the same nesting depth (the rank of the\ntensor):\n\ntf$ragged$constant(list(list(\"Hi\"), list(\"How\", \"are\", \"you\")))  ok: type=string, rank=2\n\ntf$ragged$constant(list(list(\"one\", \"two\"), list(3, 4))) # bad: multiple types\n\ntf$ragged$constant(list(\"A\", list(\"B\", \"C\"))) # bad: multiple nesting depths\n\nThis is a small introduction to Ragged Tensors in TensorFlow. See the complete tutorial (in Python) here.","id":76},{"path":"/guide/tensorflow/tensors","title":"\"TensorFlow tensors\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"\"docs\"","menu":"menu:","  main":" main:","    name":"   name: \"Tensors\"","    identifier":"   identifier: \"custom-basic-tensors\"","    parent":"   parent: \"custom-basic-top\"","    weight":"   weight: 30","content":"\n\nknitr::opts_chunk$set(eval = TRUE)\n\nTensorFlow, as the name indicates, is a framework to define and run computations\ninvolving tensors. A tensor is a generalization of vectors and matrices to\npotentially higher dimensions. Internally, TensorFlow represents tensors as\nn-dimensional arrays of base datatypes.\n\nWhen writing a TensorFlow program, the main object you manipulate and pass\naround is the tf$Tensor. A tf$Tensor object represents a partially defined\ncomputation that will eventually produce a value. TensorFlow programs work by\nfirst building a graph of tf$Tensor objects, detailing how each tensor is\ncomputed based on the other available tensors and then by running parts of this\ngraph to achieve the desired results.\n\nA tf$Tensor has the following properties:\n\n a data type (float32, int32, or string, for example)\n a shape\n\nEach element in the Tensor has the same data type, and the data type is always\nknown. The shape (that is, the number of dimensions it has and the size of each\ndimension) might be only partially known. Most operations produce tensors of\nfully-known shapes if the shapes of their inputs are also fully known, but in\nsome cases it's only possible to find the shape of a tensor at graph execution\ntime.\n\nSome types of tensors are special, and these will be covered in other\nunits of the TensorFlow guide. The main ones are:\n\n  tf$Variable\n  tf$constant\n  tf$SparseTensor\n\nWith the exception of tf.Variable, the value of a tensor is immutable, which\nmeans that in the context of a single execution tensors only have a single\nvalue. However, evaluating the same tensor twice can return different values;\nfor example that tensor can be the result of reading data from disk, or\ngenerating a random number.\n\nRank\n\nThe rank of a tf$Tensor object is its number of dimensions. Synonyms for\nrank include order or degree or n-dimension.\nNote that rank in TensorFlow is not the same as matrix rank in mathematics.\nAs the following table shows, each rank in TensorFlow corresponds to a\ndifferent mathematical entity:\n\nRank | Math entity\n--- | ,0 | Scalar (magnitude only)\n1 | Vector (magnitude and direction)\n2 | Matrix (table of numbers)\n3 | 3-Tensor (cube of numbers)\nn | n-Tensor (you get the idea)\n\n Rank 0\n\nThe following snippet demonstrates creating a few rank 0 variables:\n\nlibrary(tensorflow)\nmammal <- tf$Variable(\"Elephant\", tf$string)\nignition <- tf$Variable(451, tf$int16)\nfloating <- tf$Variable(3.14159265359, tf$float64)\nits_complicated <- tf$Variable(12.3 - 4.85i, tf$complex64)\n\nNote: A string is treated as a single object in TensorFlow, not as a sequence of\ncharacters. It is possible to have scalar strings, vectors of strings, etc.\n\nRank 1\n\nTo create a rank 1 tf.Tensor object, you can pass a list of items as the\ninitial value. For example:\n\nmystr <- tf$Variable(c(\"Hello\"), tf$string)\ncool_numbers <- tf$Variable(c(3.14159, 2.71828), tf$float32)\nfirst_primes <- tf$Variable(c(2, 3, 5, 7, 11), tf$int32)\nitsverycomplicated <- tf$Variable(c(12.3 - 4.85i, 7.5 - 6.23i), tf$complex64)\n\n Higher ranks\n\nA rank 2 tf$Tensor object consists of at least one row and at least\none column:\n\nmymat <- tf$Variable(list(c(7, 2),c(11, 3)), tf$int16)\n\nHigher-rank Tensors, similarly, consist of an n-dimensional array. For example,\nduring image processing, many tensors of rank 4 are used, with dimensions\ncorresponding to example-in-batch, image height, image width, and color channel.\n\nmy_image <- tf$zeros(shape = shape(10, 299, 299, 3)) # batch x height x width x color\n\nGetting a tf$Tensor object's rank\n\nTo determine the rank of a tf$Tensor object, call the tf$rank method.\nFor example, the following method programmatically determines the rank\nof the tf$Tensor defined in the previous section:\n\nr <- tf$rank(my_image)\nr\n\n Referring to tf$Tensor slices\n\nTensor elements can be extracted either by using functions like tf$gather() and tf$slice(), or by using [ syntax.\n\nExtracting tensor elements with [ in R is similar to extracting elements from standard R arrays, albeit with some minor differences in capabilities. In contrast to most tf$ functions, [ defaults to R style 1-based rather than 0-based indexes. Currently, only numeric indexes in [ are supported (no logical or character indexes)\n\nExtracting works identically to R arrays if the slicing index is missing, supplied as a scalar, or as a sequence (e.g, created by : or seq_len())  \n\nx[,1] )      # all rows, first column\nx[1:2,] )    # first two rows, all columns\nx[,1, drop = FALSE] ) # all rows, first column, but preserving the tensor rank\n\n` also supports slices with a strided step, which can be specified in traditional R style with seq()` or with a python-style second colon. If you are unfamiliar with python-style strided step syntax, see here for a [quick primer\n\nx[, seq(1, 5, by = 2)]  # R style\nx[, 1:5:2]              # Equivalent python-style strided step \n\nMissing arguments for python syntax are valid, but they must be supplied as NULL or whole expression must by backticked.\n\nx[, ::2] \nx[, NULL:NULL:2] \nx[, 2:] \n\n[ also accepts  tf$newaxis and all_dims() as arguments\n\nx[,, tf$newaxis]\nx[alldims(), 1] # alldims expands to the shape of the tensor\n\nAn important difference between extracting R arrays and tensorflow tensors with [ is how negative numbers are interpreted. For tensorflow tensors, negative numbers are interpreted as selecting elements by counting from tail (e.g, they are interpreted python-style).  \n\nx[-1,]  # the last row\n\nTensors are accepted by [ as well, but note that tensors suplied to [ are not translated from R to python. Meaning that tensors are interpreted as 0-based, and if slicing a range with :, then the returned arrays is exclusive of the upper bound.\n\nx[, tf$constant(1L)] # second column\nx[, tf$constant(0L):tf$constant(2L)] # first two columns\n\nIf you are translating existing python code to R, note that you can set an option to have all [ arguments be interpreted pure-python style by setting options(tensorflow.extract.style = \"python\"). See \nShape\n\nThe shape of a tensor is the number of elements in each dimension.\nTensorFlow automatically infers shapes during graph construction. These inferred\nshapes might have known or unknown rank. If the rank is known, the sizes of each\ndimension might be known or unknown.\n\nThe TensorFlow documentation uses three notational conventions to describe\ntensor dimensionality: rank, shape, and dimension number. The following table\nshows how these relate to one another:\n\nRank | Shape | Dimension number | Example\n--- | --- | --- | ,0 | [] | 0-D | A 0-D tensor.  A scalar.\n1 | [D0] | 1-D | A 1-D tensor with shape [5].\n2 | [D0, D1] | 2-D | A 2-D tensor with shape [3, 4].\n3 | [D0, D1, D2] | 3-D | A 3-D tensor with shape [1, 4, 3].\nn | [D0, D1, ... Dn-1] | n-D | A tensor with shape [D0, D1, ... Dn-1].\n\nShapes can be represented via lists of ints, or with the\ntf$TensorShape.\n\n Getting a tf$Tensor object's shape\n\nThere are two ways of accessing the shape of a tf$Tensor. While building the\ngraph, it is often useful to ask what is already known about a tensor's\nshape. This can be done by reading the shape property of a tf$Tensor object.\nThis method returns a TensorShape object, which is a convenient way of\nrepresenting partially-specified shapes (since, when building the graph, not all\nshapes will be fully known).\n\nIt is also possible to get a tf$Tensor that will represent the fully-defined\nshape of another tf$Tensor at runtime. This is done by calling the tf$shape\noperation. This way, you can build a graph that manipulates the shapes of\ntensors by building other tensors that depend on the dynamic shape of the input\ntf$Tensor.\n\nFor example, here is how to make a vector of zeros with the same size as the\nnumber of columns in a given matrix:\n\nzeros <- tf$zeros(my_image$shape[2])\n\nChanging the shape of a tf$Tensor\n\nThe number of elements of a tensor is the product of the sizes of all its\nshapes. The number of elements of a scalar is always 1. Since there are often\nmany different shapes that have the same number of elements, it's often\nconvenient to be able to change the shape of a tf.Tensor, keeping its elements\nfixed. This can be done with tf.reshape.\n\nThe following examples demonstrate how to reshape tensors:\n\nrankthreetensor <- tf$ones(shape(3, 4, 5))\nmatrix <- tf$reshape(rankthreetensor, shape(6, 10))   Reshape existing content into\n                                                 # a 6x10 matrix\nmatrixB <- tf$reshape(matrix, shape(3, -1))  #  Reshape existing content into a 3x20\n                                       # matrix. -1 tells reshape to calculate\n                                       # the size of this dimension.\nmatrixAlt <- tf$reshape(matrixB, shape(4, 3, -1))  # Reshape existing content into a\n                                             #4x3x5 tensor\n\nNote that the number of elements of the reshaped Tensors has to match the\n original number of elements. Therefore, the following example generates an\nerror because no possible value for the last dimension will match the number\n of elements.\n\nData types\n\nIn addition to dimensionality, Tensors have a data type. Refer to the\ntf$DType page for a complete list of the data types.\n\nIt is not possible to have a tf$Tensor with more than one data type. It is\npossible, however, to serialize arbitrary data structures as strings and store\nthose in tf$Tensors.\n\nIt is possible to cast tf$Tensors from one datatype to another using\ntf$cast:\n\n Cast a constant integer tensor into floating point.\nfloat_tensor <- tf$cast(tf$constant(c(1L, 2L, 3L)), dtype=tf$float32)\n\nTo inspect a tf$Tensor's data type use the Tensor.dtype property.\n\nWhen creating a tf$Tensor from an R object you may optionally specify the\ndatatype. If you don't, TensorFlow chooses a datatype that can represent your\ndata. TensorFlow converts R integers to tf$int32 and R floating\npoint numbers to tf$float32. Otherwise TensorFlow uses the same rules numpy\nuses when converting to arrays.\n\n","id":77},{"path":"/guide/tensorflow/variable","title":"\"TensorFlow variables\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown} ","type":"\"docs\"","menu":"menu:","  main":" main:","    name":"   name: \"Variables\"","    identifier":"   identifier: \"custom-basic-variables\"","    parent":"   parent: \"custom-basic-top\"","    weight":"   weight: 20","content":"\n\nknitr::opts_chunk$set(eval = TRUE)\n\nA TensorFlow variable is the best way to represent shared, persistent state\nmanipulated by your program.\n\nVariables are manipulated via the tf$Variable class. A tf$Variable\nrepresents a tensor whose value can be changed by running ops on it.  Specific\nops allow you to read and modify the values of this tensor. Higher level\nlibraries like Keras use tf$Variable to store model parameters. This\nguide covers how to create, update, and manage tf$Variables in TensorFlow.\n\nCreate a variable\n\nTo create a variable, simply provide the initial value\n\nlibrary(tensorflow)\nmy_variable <- tf$Variable(tf$zeros(c(1,2,3)))\n\nThis creates a variable which is a three-dimensional tensor with shape `[1, 2,\n3] filled with zeros. This variable will, by default, have the dtype`\ntf$float32. The dtype is, if not specified, inferred from the initial\nvalue.\n\nIf there's a tf$device scope active, the variable will be placed on that\ndevice; otherwise the variable will be placed on the \"fastest\" device compatible\nwith its dtype (this means most variables are automatically placed on a GPU if\none is available). For example, the following snippet creates a variable named\nv and places it on the second GPU device:\n\nwith(tf$device(\"/device:GPU:1\"),  )\n\nIdeally though you should use the tf$distribute API, as that allows you to\nwrite your code once and have it work under many different distributed setups.\n\n Use a variable\n\nTo use the value of a tf$Variable in a TensorFlow graph, simply treat it like\na normal tf$Tensor:\n\nv <- tf$Variable(0)\nw <- v + 1 # w is a tf.Tensor which is computed based on the value of v.\nAny time a variable is used in an expression it gets automatically\n converted to a tf$Tensor representing its value.\n\nTo assign a value to a variable, use the methods assign, assign_add, and\nfriends in the tf$Variable class. For example, here is how you can call these\nmethods:\n\nv <- tf$Variable(0)\nv$assign_add(1)\n\nMost TensorFlow optimizers have specialized ops that efficiently update the\nvalues of variables according to some gradient descent-like algorithm. See\ntf$keras$optimizers$Optimizer for an explanation of how to use optimizers.\n\nYou can also explicitly read the current value of a variable, using\nread_value:\n\nv <- tf$Variable(0)\nv$assign_add(1)\nv$read_value()\n\nWhen the last reference to a tf$Variable goes out of scope its memory is\nfreed.\n\n","id":78},{"path":"/guide/tfdatasets/feature_columns","title":"\"Feature columns\"","output":"","  rmarkdown":" rmarkdown::html_vignette: default","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfdatasets","menu":"menu:","  main":" main:","    name":"   name: \"Feature columns\"","    identifier":"   identifier: \"feature-columns\"","    parent":"   parent: \"data-feature-spec-top\"","    weight":"   weight: 20","aliases":"aliases:","content":"\nknitr::opts_chunk$set(eval = TRUE)\n\n This document is an adaptation of the official TensorFlow Feature Columns guide.\n\nThis document details feature columns and how they can be used as inputs to neural\nnetworks using TensorFlow. Feature columns are very rich, enabling you to transform \na diverse range of raw data into formats that neural networks can use, allowing \neasy experimentation. \n\nWhat kind of data can a deep neural network operate on? The answer is, of course, \nnumbers (for example, tf$float32). After all, every neuron in a neural network \nperforms multiplication and addition operations on weights and input data. Real-life \ninput data, however, often contains non-numerical (categorical) data. For example, \nconsider a product_class feature that can contain the following three non-numerical \nvalues:\n\nkitchenware\nelectronics\nsports\n\nML models generally represent categorical values as simple vectors in which a 1 represents \nthe presence of a value and a 0 represents the absence of a value. For example, when \nproductclass is set to sports, an ML model would usually represent productclass as \n[0, 0, 1], meaning:\n\n0: kitchenware is absent\n0: electronics is absent\n1: sports is present\n\nSo, although raw data can be numerical or categorical, an ML model represents all \nfeatures as numbers.\n\nThis document explains nine of the feature columns available in tfdatasets. As the \nfollowing figure shows, all nine functions return either a categorical_column or a \ndensecolumn object, except bucketizedcolumn, which inherits from both classes:\n\nLet's look at these functions in more detail.\n\nFeature spec interface\n\nWe are going to use the featurespec interface in the examples. The featurespec \ninterface is an abstraction that makes it easier to define feature columns in R. \n\nYou can initialize a feature_spec with:\n\nlibrary(tfdatasets)\nheartsdataset <- tensorslices_dataset(hearts)\nspec <- featurespec(heartsdataset, target ~ .)\n\nWe then add steps in order to define featurecolumns. Read the featurespec \nvignette for more information.\n\n Numeric column\n\nWe use stepnumericcolumn to add numeric columns to our spec. \n\nspec %% \n  stepnumericcolumn(age)\n\nBy default, a numeric column creates a single value (scalar). Use the shape argument \nto specify another shape. For example:\n\nRepresent a 10-element vector in which each cell contains a tf$float32.\nspec %% \n  stepnumericcolumn(bowling, shape = 10)\n\n Represent a 10x5 matrix in which each cell contains a tf$float32.\nspec %% \n  stepnumericcolumn(my_matrix, shape = c(10, 5))\n\nA nice feature of stepnumericcolumnis that you can also specify normalizer functions.\nWhen using a scaler, the scaling constants will be learned from data when fitting\nthe feature_spec.\n\nuse a function that defines tensorflow ops.\nspec %% \n  stepnumericcolumn(age, normalizer_fn = function(x) (x-10)/5)\n\n use a scaler\nspec %% \n  stepnumericcolumn(age, normalizerfn = scalerstandard())\n\nBucketized column\n\nOften, you don't want to feed a number directly into the model, but instead split \nits value into different categories based on numerical ranges. To do so, create a \nbucketized_column. For example, consider raw data that represents the year a \nhouse was built. Instead of representing that year as a scalar numeric column, we \ncould split the year into the following four buckets:\n\n \nWhy would you want to split a number — a perfectly valid input to your model — into a \ncategorical value? Well, notice that the categorization splits a single input number \ninto a four-element vector. Therefore, the model now can learn four individual weights \nrather than just one; four weights creates a richer model than one weight. More importantly,\nbucketizing enables the model to clearly distinguish between different year categories \nsince only one of the elements is set (1) and the other three elements are cleared (0). \nFor example, when we just use a single number (a year) as input, a linear model \ncan only learn a linear relationship. So, bucketing provides the model with additional \nflexibility that the model can use to learn.\n\nThe following code demonstrates how to create a bucketized feature:\n\n First, convert the raw input to a numeric column.\nspec - spec %% \n  stepnumericcolumn(age)\n\nThen, bucketize the numeric column.\nspec -  spec %% \n  stepbucketizedcolumn(age, boundaries = c(30, 50, 70))\n\nNote that specifying a three-element boundaries vector creates a four-element bucketized vector.\n\n Categorical identity column\n\nCategorical identity columns are to tfdatasets what factors are to R.\nPut differently, they can be seen as a special case of bucketized columns. \nIn traditional bucketized columns, each bucket represents a range of values \n(for example, from 1960 to 1979). In a categorical identity column, each bucket \nrepresents a single, unique integer. For example, let's say you want to represent \nthe integer range [0, 4). That is, you want to represent the integers 0, 1, 2, or 3. \nIn this case, the categorical identity mapping looks like this:\n\nAs with bucketized columns, a model can learn a separate weight for each class in\na categorical identity column. For example, instead of using a string to represent \nthe product_class, let's represent each class with a unique integer value. That is:\n\n0=\"kitchenware\"\n1=\"electronics\"\n2=\"sport\"\n\nCall stepcategoricalcolumnwithidentity to add a categorical identity column to\nthe feature_spec. For example:\n\nCreate categorical output for an integer feature named \"myfeatureb\",\n The values of myfeatureb must be = 0 and < num_buckets\nspec - spec %% \n  stepcategoricalcolumnwithidentity(myfeatureb, num_buckets = 4)\n\nCategorical vocabulary column\n\nWe cannot input strings directly to a model. Instead, we must first map strings to numeric or categorical values. Categorical vocabulary columns provide a good way to represent strings as a one-hot vector. For example:\n\nAs you can see, categorical vocabulary columns are kind of an enum version of \ncategorical identity columns. tfdatasets provides two different functions to create\ncategorical vocabulary columns:\n\nstepcategoricalcolumnwithvocabulary_list\nstepcategoricalcolumnwithvocabulary_file\n\ncategoricalcolumnwithvocabularylist maps each string to an integer based on an explicit vocabulary list. For example:\n\nspec - spec %% \n  stepcategoricalcolumnwithvocabulary_list(\n    thal, \n    vocabulary_list = c(\"fixed\", \"normal\", \"reversible\")\n  )\n\nNote that the vocabulary_list argument is optional in R and the vocabulary will be\ndiscovered when fitting the featture_spec which saves us a lot of typing.\n\nYou can also place the vocabulary in a separate file that should contain one line \nfor each vocabulary element. You can then use:\n\nspec - spec %% \n  stepcategoricalcolumnwithvocabularyfile(thal, vocabularyfile = \"thal.txt\")\n\n Hashed Column\n\nSo far, we've worked with a naively small number of categories. For example, our \nproduct_class example has only 3 categories. Often though, the number of categories \ncan be so big that it's not possible to have individual categories for each vocabulary \nword or integer because that would consume too much memory. For these cases, we can \ninstead turn the question around and ask, \"How many categories am I willing to have \nfor my input?\" In fact, the stepcategoricalcolumnwithhash_bucket \nfunction enables you to specify the number of categories. For this type of feature \ncolumn the model calculates a hash value of the input, then puts it into one of \nthe hashbucketsize categories using the modulo operator, as in the following \npseudocode:\n\npseudocode\nfeatureid = hash(rawfeature) % hashbucketsize\n\nThe code to add the featurecolumn to the featurespec might look something\nlike this:\n\nspec - spec %% \n  stepcategoricalcolumnwithhashbucket(thal, hashbucket_size = 100)\n\nAt this point, you might rightfully think: \"This is crazy!\" After all, we are \nforcing the different input values to a smaller set of categories. This means \nthat two probably unrelated inputs will be mapped to the same category, and \nconsequently mean the same thing to the neural network. The following figure \nillustrates this dilemma, showing that kitchenware and sports both get assigned \nto category (hash bucket) 12:\n\nAs with many counterintuitive phenomena in machine learning, it turns out that \nhashing often works well in practice. That's because hash categories provide the\nmodel with some separation. The model can use additional features to further \nseparate kitchenware from sports.\n\n Crossed column\n\nCombining features into a single feature, better known as feature crosses, enables \nthe model to learn separate weights for each combination of features.\n\nMore concretely, suppose we want our model to calculate real estate prices in Atlanta, GA. \nReal-estate prices within this city vary greatly depending on location. Representing \nlatitude and longitude as separate features isn't very useful in identifying real-estate \nlocation dependencies; however, crossing latitude and longitude into a single feature can\npinpoint locations. Suppose we represent Atlanta as a grid of 100x100 rectangular sections, \nidentifying each of the 10,000 sections by a feature cross of latitude and longitude. This \nfeature cross enables the model to train on pricing conditions related to each individual \nsection, which is a much stronger signal than latitude and longitude alone.\n\nThe following figure shows our plan, with the latitude & longitude values for the \ncorners of the city in red text:\n\nFor the solution, we used a combination of the bucketized_column we looked at earlier, \nwith the stepcrossedcolumn function.\n\nspec - feature_spec(dataset, target ~ latitute + longitude) %% \n  stepnumericcolumn(latitude, longitude) %% \n  stepbucketizedcolumn(latitude, boundaries = c(latitude_edges)) %% \n  stepbucketizedcolumn(longitude, boundaries = c(longitude_edges)) %% \n  stepcrossedcolumn(latitudelongitude = c(latitude, longitude), hashbucket_size = 100)\n\nYou may create a feature cross from either of the following:\n\nAny categorical column, except categoricalcolumnwithhashbucket (since crossed_column hashes the input).\n\nWhen the feature columns latitudebucketfc and longitudebucketfc are crossed, \nTensorFlow will create (latitudefc, longitudefc) pairs for each example. This would \nproduce a full grid of possibilities as follows:\n\n(0,0),  (0,1)...  (0,99)\n (1,0),  (1,1)...  (1,99)\n   ...     ...       ...\n(99,0), (99,1)...(99, 99)\n\nExcept that a full grid would only be tractable for inputs with limited vocabularies. \nInstead of building this, potentially huge, table of inputs, the crossed_column only \nbuilds the number requested by the hashbucketsize argument. The feature column assigns\nan example to a index by running a hash function on the tuple of inputs, followed by a \nmodulo operation with hashbucketsize.\n\nAs discussed earlier, performing the hash and modulo function limits the number of \ncategories, but can cause category collisions; that is, multiple (latitude, longitude) feature\ncrosses will end up in the same hash bucket. In practice though, performing feature \ncrosses still adds significant value to the learning capability of your models.\n\nSomewhat counterintuitively, when creating feature crosses, you typically still \nshould include the original (uncrossed) features in your model (as in the preceding code snippet). \nThe independent latitude and longitude features help the model distinguish between examples \nwhere a hash collision has occurred in the crossed feature.\n\nIndicator and embedding columns\n\nIndicator columns and embedding columns never work on features directly, but instead \ntake categorical columns as input.\n\nWhen using an indicator column, we're telling TensorFlow to do exactly what we've \nseen in our categorical product_class example. That is, an indicator column treats \neach category as an element in a one-hot vector, where the matching category has value \n1 and the rest have 0s:\n\nHere's how you create an indicator column:\n\nspec - feature_spec(dataset, target ~ .) %% \n  stepcategoricalcolumnwithvocabularylist(productclass) %% \n  stepindicatorcolumn(product_class)\n\nNow, suppose instead of having just three possible classes, we have a million. \nOr maybe a billion. For a number of reasons, as the number of categories grow large, \nit becomes infeasible to train a neural network using indicator columns.\n\nWe can use an embedding column to overcome this limitation. Instead of representing \nthe data as a one-hot vector of many dimensions, an embedding column represents that \ndata as a lower-dimensional, ordinary vector in which each cell can contain any number, \nnot just 0 or 1. By permitting a richer palette of numbers for every cell, \nan embedding column contains far fewer cells than an indicator column.\n\nLet's look at an example comparing indicator and embedding columns. Suppose our \ninput examples consist of different words from a limited palette of only 81 words. \nFurther suppose that the data set provides the following input words in 4 \nseparate examples:\n\n\"dog\"\n\"spoon\"\n\"scissors\"\n\"guitar\"\n\nIn that case, the following figure illustrates the processing path for embedding columns or indicator columns.\n\nWhen an example is processed, one of the categoricalcolumnwith_[...] functions maps the example string to a numerical categorical value. For example, a function maps \"spoon\" to [32]. (The 32 comes from our imagination — the actual values depend on the mapping function.) You may then represent these numerical categorical values in either of the following two ways:\n\nAs an indicator column. A function converts each numeric categorical value into an 81-element vector (because our palette consists of 81 words), placing a 1 in the index of the categorical value (0, 32, 79, 80) and a 0 in all the other positions.\n\nAs an embedding column. A function uses the numerical categorical values (0, 32, 79, 80) as indices to a lookup table. Each slot in that lookup table contains a 3-element vector.\n\nHow do the values in the embeddings vectors magically get assigned? Actually, the assignments happen during training. That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem. Embedding columns increase your model's capabilities, since an embeddings vector learns new relationships between categories from the training data.\n\nWhy is the embedding vector size 3 in our example? Well, the following \"formula\" provides a general rule of thumb about the number of embedding dimensions:\n\nembeddingdimensions =  numberof_categories**0.25\n\nThat is, the embedding vector dimension should be the 4th root of the number of \ncategories. Since our vocabulary size in this example is 81, the recommended \nnumber of dimensions is 3:\n\n3 =  81**0.25\n\n Note: This is just a general guideline; you can set the number of embedding dimensions as you please.\n\nCall stepembeddingcolumn to create an embedding_column as suggested by the \nfollowing snippet:\n\nspec - feature_spec(dataset, target ~ .) %% \n  stepcategoricalcolumnwithvocabularylist(productclass) %% \n  stepembeddingcolumn(product_class, dimension = 3)\n\nEmbeddings is a significant topic within machine learning. This information was just \nto get you started using them as feature columns.\n\n Passing feature columns to Keras\n\nAfter creating and fitting a featurespec object you can use the densefeatures\nto get the Dense features from the specifications. You can then use the layerdensefeatures\nfunction from Keras to create a layer that will automatically initialize values.\n\nHere is an an example of how it would work:\n\nlibrary(keras)\nlibrary(dplyr)\n\nspec - feature_spec(hearts, target ~ .) %% \n  stepnumericcolumn(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizerfn = scalerstandard()\n  ) %% \n  stepcategoricalcolumnwithvocabulary_list(thal) %% \n  stepbucketizedcolumn(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %% \n  stepindicatorcolumn(thal) %% \n  stepembeddingcolumn(thal, dimension = 2) %% \n  stepcrossedcolumn(c(thal, bucketizedage), hashbucket_size = 10) %%\n  stepindicatorcolumn(crossedthalbucketized_age)\n\nspec <- fit(spec)\n\ninput - layerinputfrom_dataset(hearts %% select(-target))\noutput - input %% \n  layerdensefeatures(featurecolumns = densefeatures(spec)) %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\nmodel %% \n  compile(\n    loss = \"binary_crossentropy\", \n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n    )\n\nmodel %% \n  fit(\n    x = hearts %% select(-target), y = hearts$target,\n    validation_split = 0.2\n  )\n\n","id":79},{"path":"/guide/tfdatasets/feature_spec","title":"\"Feature Spec interface\"","output":"","  rmarkdown":" rmarkdown::html_vignette: default","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfdatasets","menu":"menu:","  main":" main:","    name":"   name: \"Feature Spec interface\"","    identifier":"   identifier: \"feature-spec\"","    parent":"   parent: \"data-feature-spec-top\"","    weight":"   weight: 10","aliases":"aliases:","content":"\nknitr::opts_chunk$set(eval = TRUE) \n\nOverview\n\nIn this document we will demonstrate the basic usage of the feature_spec interface\nin tfdatasets.\n\nThe featurespec interface is a user friendly interface to featurecolumns.\nIt allows us to specify column transformations and representations when working with\nstructured data.\n\nWe will use the hearts dataset and it can be loaded with data(hearts).\n\nlibrary(tfdatasets)\nlibrary(dplyr)\ndata(hearts)\n\nhead(hearts)\n\nWe want to train a model to predict the target variable using Keras but, before\nthat we need to prepare the data. We need to transform the categorical variables\ninto some form of dense variable, we usually want to normalize all numeric columns too.\n\nThe feature spec interface works with data.frames or TensorFlow datasets objects.\n\nids_train <- sample.int(nrow(hearts), size = 0.75*nrow(hearts))\nheartstrain <- hearts[idstrain,]\nheartstest <- hearts[-idstrain,]\n\nNow let's start creating our feature specification: \n\nspec <- featurespec(heartstrain, target ~ .)\n\nThe first thing we need to do after creating the feature_spec is decide on the variables' types.\n\nWe can do this by adding steps to the spec object.\n\nspec - spec %% \n  stepnumericcolumn(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizerfn = scalerstandard()\n  ) %% \n  stepcategoricalcolumnwithvocabulary_list(thal)\n\nThe following steps can be used to define the variable type:\n\nstepnumericcolumn to define numeric variables\nstepcategoricalwithvocabularylist for categorical variables with a fixed vocabulary\nstepcategoricalcolumnwithhash_bucket for categorical variables using the hash trick\nstepcategoricalcolumnwithidentity to store categorical variables as integers\nstepcategoricalcolumnwithvocabulary_file when you have the possible vocabulary in a file\n\nWhen using stepcategoricalcolumnwithvocabulary_list you can also provide a vocabulary argument\nwith the fixed vocabulary. The recipe will find all the unique values in the dataset and use it\nas the vocabulary.\n\nYou can also specify a normalizerfn to the stepnumeric_column. In this case the variable will be\ntransformed by the feature column. Note that the transformation will occur in the TensorFlow Graph,\nso it must use only TensorFlow ops. Like in the example we offer pre-made normalizers - and they will\ncompute the normalizing function during the recipe preparation.\n\nYou can also use selectors like:\n\nstartswith(), endswith(), matches() etc. (from tidyselect)\nall_numeric() to select all numeric variables\nall_nominal() to select all strings\nhas_type(\"float32\") to select based on TensorFlow variable type.\n\nNow we can print the recipe:\n\nspec\n\nAfter specifying the types of the columns you can add transformation steps. \nFor example you may want to bucketize a numeric column:\n\nspec - spec %% \n  stepbucketizedcolumn(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))\n\nYou can also specify the kind of numeric representation that you want to use for\nyour categorical variables.\n\nspec - spec %% \n  stepindicatorcolumn(thal) %% \n  stepembeddingcolumn(thal, dimension = 2)\n\nAnother common transformation is to add interactions between variables using crossed\ncolumns. \n\nspec - spec %% \n  stepcrossedcolumn(thalandage = c(thal, bucketizedage), hashbucket_size = 1000) %% \n  stepindicatorcolumn(thalandage)\n\nNote that the crossed_column is a categorical column, so we need to also specify what\nkind of numeric tranformation we want to use. Also note that we can name the transformed\nvariables - each step uses a default naming for columns, eg. bucketized_age is the\ndefault name when you use stepbucketizedcolumn with column called age.\n\nWith the above code we have created our recipe. Note we can also define the\nrecipe by chaining a sequence of methods:\n\nspec - featurespec(heartstrain, target ~ .) %% \n  stepnumericcolumn(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizerfn = scalerstandard()\n  ) %% \n  stepcategoricalcolumnwithvocabulary_list(thal) %% \n  stepbucketizedcolumn(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %% \n  stepindicatorcolumn(thal) %% \n  stepembeddingcolumn(thal, dimension = 2) %% \n  stepcrossedcolumn(c(thal, bucketizedage), hashbucket_size = 10) %%\n  stepindicatorcolumn(crossedthalbucketized_age)\n\nAfter defining the recipe we need to fit it. It's when fitting that we compute the vocabulary\nlist for categorical variables or find the mean and standard deviation for the normalizing functions.\nFitting involves evaluating the full dataset, so if you have provided the vocabulary list and \nyour columns are already normalized you can skip the fitting step (TODO).\n\nIn our case, we will fit the feature spec, since we didn't specify the vocabulary list\nfor the categorical variables.\n\nspec_prep <- fit(spec)\n\nAfter preparing we can see the list of dense features that were defined:\n\nstr(specprep$densefeatures())\n\nNow we are ready to define our model in Keras. We will use a specialized layerdensefeatures that\nknows what to do with the feature columns specification.\n\nWe also use a new layerinputfrom_dataset that is useful to create a Keras input object copying the structure from a data.frame or TensorFlow dataset.\n\nlibrary(keras)\n\ninput - layerinputfromdataset(heartstrain %% select(-target))\n\noutput - input %% \n  layerdensefeatures(densefeatures(specprep)) %% \n  layer_dense(units = 32, activation = \"relu\") %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\nmodel %% compile(\n  loss = lossbinarycrossentropy, \n  optimizer = \"adam\", \n  metrics = \"binary_accuracy\"\n)\n\nWe can finally train the model on the dataset:\n\nhistory - model %% \n  fit(\n    x = hearts_train %% select(-target),\n    y = hearts_train$target, \n    epochs = 15, \n    validation_split = 0.2\n  )\n\nplot(history)\n\nFinally we can make predictions in the test set and calculate performance \nmetrics like the AUC of the ROC curve:\n\nheartstest$pred <- predict(model, heartstest)\nMetrics::auc(heartstest$target, heartstest$pred)\n\n","id":80},{"path":"/guide/tfdatasets/introduction","title":"\"R interface to TensorFlow Dataset API\"","output":"","  rmarkdown":" rmarkdown::html_vignette: default","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfdatasets","menu":"menu:","  main":" main:","    name":"   name: \"Using Datasets\"","    identifier":"   identifier: \"data-using\"","    parent":"   parent: \"data-overview-top\"","    weight":"   weight: 10","aliases":"aliases:","content":"\nknitr::opts_chunk$set(eval = FALSE)\n\nOverview\n\nThe TensorFlow Dataset API provides various facilities for creating scalable input pipelines for TensorFlow models, including:\n\nReading data from a variety of formats including CSV files and TFRecords files (the standard binary format for TensorFlow training data).\n\nTransforming datasets in a variety of ways including mapping arbitrary functions against them. \n\nShuffling, batching, and repeating datasets over a number of epochs.\n\nStreaming interface to data for reading arbitrarily large datasets.\n\nReading and transforming data are TensorFlow graph operations, so are executed in C++ and in parallel with model training.\n\nThe R interface to TensorFlow datasets provides access to the Dataset API, including high-level convenience functions for easy integration with the keras and tfestimators R packages.\n\n Installation\n\nTo use tfdatasets you need to install both the R package as well as TensorFlow itself.\n\nFirst, install the tfdatasets R package from GitHub as follows:\n\ndevtools::install_github(\"rstudio/tfdatasets\")\n\nThen, use the install_tensorflow() function to install TensorFlow:\n\nlibrary(tfdtasets)\ninstall_tensorflow()\n\nCreating a Dataset\n\nTo create a dataset, use one of the dataset creation functions. Dataset can be created from delimted text files, TFRecords files, as well as from in-memory data.\n\n Text Files\n\nFor example, to create a dataset from a text file, first create a specification for how records will be decoded from the file, then call textlinedataset() with the file to be read and the specification:\n\nlibrary(tfdatasets)\n\ncreate specification for parsing records from an example file\nirisspec <- csvrecord_spec(\"iris.csv\")\n\n read the datset\ndataset <- textlinedataset(\"iris.csv\", recordspec = irisspec) \n\ntake a glimpse at the dataset\nstr(dataset)\n<MapDataset shapes:  , types:  \n\nIn the example above, the csvrecordspec() function is passed an example file which is used to  automatically detect column names and types (done by reading up to the first 1,000 lines of the file). You can also provide explicit column names and/or data types using the names and types parameters (note that in this case we don't pass an example file):\n\n provide colum names and types explicitly\nirisspec <- csvrecord_spec(\n  names = c(\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"),\n  types = c(\"double\", \"double\", \"double\", \"double\", \"integer\"), \n  skip = 1\n)\n\nread the datset\ndataset <- textlinedataset(\"iris.csv\", recordspec = irisspec)\n\nNote that we've also specified skip = 1 to indicate that the first row of the CSV that contains column names should be skipped.\n\nSupported column types are integer, double, and character. You can also provide types in a more compact form using single-letter abbreviations (e.g. types = \"dddi\"). For example:\n\nmtcarsspec <- csvrecord_spec(\"mtcars.csv\", types = \"dididddiiii\")\n\n Parallel Decoding\n\nDecoding lines of text into a record can be computationally expensive. You can parallelize these computations using the parallel_records parameter. For example:\n\ndataset <- textlinedataset(\"iris.csv\", recordspec = irisspec, parallel_records = 4)\n\nYou can also parallelize the reading of data from storage by requesting that a buffer of records be prefected. You do this with the dataset_prefetch() function. For example:\n\ndataset - textlinedataset(\"iris.csv\", recordspec = irisspec, parallel_records = 4) %% \n  dataset_batch(128) %% \n  dataset_prefetch(1)\n\nThis code will result in the prefetching of a single batch of data on a background thread (i.e. in parallel with training operations).\n\nIf you have multiple input files, you can also parallelize reading of these files both across multiple machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). See the section on [Reading Multiple Files] below for additional details.\n\nTFRecords Files\n\nYou can read datasets from TFRecords files using the tfrecord_dataset() function.\n\nIn many cases you'll want to map the records in the dataset into a set of named columns. You can do this using the datasetmap() function along with the tf$parsesingle_example() function. for example:\n\n Creates a dataset that reads all of the examples from two files, and extracts\nthe image and label features.\nfilenames <- c(\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\")\ndataset - tfrecord_dataset(filenames) %%\n  datasetmap(function(exampleproto)  )\n\nYou can parallelize reading of TFRecord files using the numparallelreads option, for example:\n\nfilenames <- c(\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\")\ndataset <- tfrecorddataset(filenames, numparallel_reads = 4)\n\n SQLite Databases\n\nYou can read datasets from SQLite databases using the sqlitedataset() function. To use sqlitedataset() you provide the filename of the database, a SQL query to execute, and sqlrecordspec() that describes the names and TensorFlow types of columns within the query. For example:\n\nlibrary(tfdatasets)\n\nrecordspec <- sqlrecord_spec(\n  names = c(\"disp\", \"drat\", \"vs\", \"gear\", \"mpg\", \"qsec\", \"hp\", \"am\", \"wt\",  \"carb\", \"cyl\"),\n  types = c(tf$float64, tf$int32, tf$float64, tf$int32, tf$float64, tf$float64,\n            tf$float64, tf$int32, tf$int32, tf$int32, tf$int32)\n)\n\ndataset <- sqlite_dataset(\n  \"data/mtcars.sqlite3\",\n  \"select * from mtcars\",\n  record_spec\n)\n\ndataset\n<MapDataset shapes:  , types:  \n\nNote that for floating point data you must use tf$float64 (reading tf$float32 is not supported for SQLite databases).\n\nTransformations\n\n Mapping\n\nYou can map arbitrary transformation functions onto dataset records using the dataset_map() function. For example, to transform the \"Species\" column into a one-hot encoded vector you would do this:\n\ndataset - dataset %% \n  dataset_map(function(record)  )\n\nNote that while dataset_map() is defined using an R function, there are some special constraints on this function which allow it to execute not within R but rather within the TensorFlow graph. \n\nFor a dataset created with the csvdataset() function, the passed record will be named list of tensors (one for each column of the dataset). The return value should be another set of tensors which were created from TensorFlow functions (e.g. tf$onehot as illustrated above). This function will be converted to a TensorFlow graph operation that performs the transformation within native code. \n\nParallel Mapping\n\nIf these transformations are computationally expensive they can be executed on multiple threads using the numparallelcalls parameter. For example:\n\ndataset - dataset %% \n  datasetmap(numparallel_calls = 4, function(record)  )\n\nYou can control the maximum number of processed elements that will be buffered when processing in parallel using the dataset_prefetch() transformation. For example:\n\ndataset - dataset %% \n  datasetmap(numparallel_calls = 4, function(record)  ) %% \n  datset_prefetch(1)\n\nIf you are batching your data for training, you can optimize performance using the datasetmapand_batch() function (which fuses together the map and batch operations). \nFor example:\n\ndataset - dataset %% \n  datasetmapandbatch(batchsize = 128, function(record)  ) %% \n  datset_prefetch(1)\n\n Filtering\n\nYou can filter the elements of a dataset using the dataset_filter() function, which takes a predicate function that returns a boolean tensor for records that should be included. For example:\n\ndataset - csv_dataset(\"mtcars.csv\") %%\n  dataset_filter(function(record)  )\n\ndataset - csv_dataset(\"mtcars.csv\") %%\n  dataset_filter(function(record)  )\n\nNote that the functions used inside the predicate must be tensor operations (e.g. tf$not_equal, tf$less, etc.). R generic methods for relational operators (e.g. , , <=, etc.) and logical operators (e.g. !, &, |, etc.) are provided so you can use shorthand syntax for most common comparisons (as illustrated above).\n\nFeatures and Response \n\nA common transformation is taking a column oriented dataset (e.g. one created by csvdataset() or tfrecorddataset()) and transforming it into a two-element list with features (\"x\") and response (\"y\"). You can use the dataset_prepare() function to do this type of transformation. For example:\n\nmtcarsdataset - textlinedataset(\"mtcars.csv\", recordspec = mtcars_spec) %% \n  dataset_prepare(x = c(mpg, disp), y = cyl)\n\nirisdataset - textlinedataset(\"iris.csv\", recordspec = iris_spec) %% \n  dataset_prepare(x = -Species, y = Species)\n\nThe dataset_prepare() function also accepts standard R formula syntax for defining features and response:\n\nmtcarsdataset - textlinedataset(\"mtcars.csv\", recordspec = mtcars_spec) %% \n  dataset_prepare(cyl ~ mpg + disp)\n\nIf you are batching your data for training you add a batchsize parameter to fuse together the datasetprepare() and dataset_batch() steps (which generally results in faster training). For example:\n\nmtcarsdataset - textlinedataset(\"mtcars.csv\", recordspec = mtcars_spec) %% \n  datasetprepare(cyl ~ mpg + disp, batchsize = 16)\n\n Shuffling and Batching \n\nThere are several functions which control how batches are drawn from the dataset. For example, the following specifies that data will be drawn in batches of 128 from a shuffled window of 1000 records, and that the dataset will be repeated for 10 epochs:\n\ndataset - dataset %% \n  dataset_shuffle(1000) %%\n  dataset_repeat(10) %% \n  dataset_batch(128) %% \n\nNote that you can optimize performance by fusing the shuffle and repeat operations into a single step using the datasetshuffleand_repeat() function. For example:\n\ndataset - dataset %% \n  datasetshuffleandrepeat(buffersize = 1000, count = 10) %%\n  dataset_batch(128)\n\nPrefetching\n\nEarlier we alluded to the dataset_prefetch() function, which enables you to ensure that a given number of records (or batches of records) are prefetched in parallel so they are ready to go when the next batch is processed. For example:\n\ndataset - dataset %% \n  datasetmapandbatch(batchsize = 128, function(record)  ) %% \n  dataset_prefetch(1)\n\nIf you are using a GPU for training, you can also use the datasetprefetchto_device() function to specify that the parallel prefetch operation stage the data directly into GPU memory. For example:\n\ndataset - dataset %% \n  datasetmapandbatch(batchsize = 128, function(record)  ) %% \n  datasetprefetchto_device(\"/gpu:0\")\n\nIn this case the buffer size for prefetches is determined automatically (you can manually speicfy it using the buffer_size parameter).\n\n Complete Example\n\nHere's a complete example of using the various dataset transformation functions together. We'll read the mtcars dataset from a CSV, filter it on some threshold values, map it into x and y components for modeling, and specify desired shuffling and batch iteration behavior:\n\ndataset - textlinedataset(\"mtcars.csv\", recordspec = mtcarsspec) %%\n  dataset_filter(function(record)  ) %% \n  datasetshuffleandrepeat(buffersize = 1000, count = 10) %% \n  datasetprepare(cyl ~ mpg + disp, batchsize = 128) %% \n  dataset_prefetch(1)\n\nReading Datasets\n\nThe method for reading data from a TensorFlow Dataset varies depending upon which API you are using to build your models. If you are using the keras or tfestimators packages, then TensorFlow Datasets can be used much like in-memory R matrices and arrays. If you are using the lower-level tensorflow core API then you'll use explicit dataset iteration functions.\n\nThe sections below provide additional details and examples for each of the supported APIs.\n\n keras package\n\nIMPORTANT NOTE: Using TensorFlow Datasets with Keras requires that you are running the very latest versions of Keras (v2.2) and TensorFlow (v1.9). You can ensure that you have the latest versions of the core Keras and TensorFlow libraries with:\n\nlibrary(keras)\ninstall_keras()\n\nKeras models are often trained by passing in-memory arrays directly to the fit function. For example:\n\nmodel %% fit(\n  xtrain, ytrain, \n  epochs = 30, \n  batch_size = 128\n)\n\nHowever, this requires loading data into an R data frame or matrix before calling fit. You can use the trainonbatch() function to stream data one batch at a time, however the reading and processing of the input data is still being done serially and outside of native code.\n\nAlternatively, Keras enables you to pass a dataset directly as the x argument to fit() and evaluate(). Here's a complete example that uses datasets to read from TFRecord files containing MNIST digits:\n\nlibrary(keras)\nlibrary(tfdatasets)\n\nbatch_size = 128\nstepsperepoch = 500\n\nfunction to read and preprocess mnist dataset\nmnist_dataset <- function(filename)  ) %%\n    dataset_repeat() %%\n    dataset_shuffle(1000) %%\n    datasetbatch(batchsize, drop_remainder = TRUE) %%\n    dataset_prefetch(1)\n \n\nmodel - kerasmodelsequential() %%\n  layerdense(units = 256, activation = 'relu', inputshape = c(784)) %%\n  layer_dropout(rate = 0.4) %%\n  layer_dense(units = 128, activation = 'relu') %%\n  layer_dropout(rate = 0.3) %%\n  layer_dense(units = 10, activation = 'softmax')\n\nmodel %% compile(\n  loss = 'categorical_crossentropy',\n  optimizer = optimizer_rmsprop(),\n  metrics = c('accuracy')\n)\n\nhistory - model %% fit(\n  mnist_dataset(\"mnist/train.tfrecords\"),\n  stepsperepoch = stepsperepoch,\n  epochs = 20,\n  validationdata = mnistdataset(\"mnist/validation.tfrecords\"),\n  validationsteps = stepsper_epoch\n)\n\nscore - model %% evaluate(\n  mnist_dataset(\"mnist/test.tfrecords\"),\n  steps = stepsperepoch\n)\n\nprint(score)\n\nNote that all data preprocessing (e.g. one-hot encoding of the response variable) is done within the dataset_map() operation.\n\nAlso note that we pass dropremainder = TRUE to the datasetbatch() function (this is to make sure that all batches are of equal size, a requirement for Keras tensor inputs).\n\ntfestimators package\n\nModels created with tfestimators use an input function to consume data for training, evaluation, and prediction. For example, here is an example of using an input function to feed data from an in-memory R data frame to an estimators model:\n\nmodel %% train(\n  input_fn(mtcars, features = c(mpg, disp), response = cyl,\n           batch_size = 128, epochs = 3)\n)\n\nIf you are using tfdatasets with the tfestimators package, you can create an estimators input function directly from a dataset as follows:\n\nlibrary(tfestimators)\nlibrary(tfdatasets)\n\nmtcarsspec <- csvrecord_spec(\"mtcars.csv\")\ndataset - textlinedataset(\"mtcars.csv\", recordspec = mtcarsspec) %% \n  dataset_batch(128) %% \n  dataset_repeat(3)\n\ncols <- feature_columns(\n  column_numeric(\"mpg\"),\n  column_numeric(\"disp\")\n)\n\nmodel <- linearregressor(featurecolumns = cols)\n\nmodel %% train(\n  input_fn(dataset, features = c(mpg, disp), response = cyl)\n)\n\nNote that we don't use the datasetprepare() function in this example. Rather, this function is used under the hood to provide the inputfn interface expected by tfestimators models.\n\nAs with dataset_prepare(), you can alternatively use an R formula to specify features and response:\n\nmodel %% train(\n  input_fn(dataset, cyl ~ mpg + disp)\n)\n\n tensorflow package\n\nYou read batches of data from a dataset by using tensors that yield the next batch. You can obtain this tensor from a dataset via the makeiteratoroneshot() and iteratorget_next() functions. For example:\n\ndataset - textlinedataset(\"mtcars.csv\", recordspec = mtcarsspec) %% \n  dataset_prepare(cyl ~ mpg + disp) %% \n  dataset_shuffle(20) %% \n  dataset_batch(5)\n\niter <- makeiteratorone_shot(dataset)\nnextbatch <- iteratorget_next(iter)\nnext_batch\n$x\nTensor(\"IteratorGetNext_13:0\", shape=(?, 2), dtype=float32)\n\n$y\nTensor(\"IteratorGetNext_13:1\", shape=(?,), dtype=int32)\n\nAs you can see next_batch isn't the data itself but rather a tensor that will yield the next batch of data when it is evaluated:\n\nsess <- tf$Session()\nsess$run(next_batch)\n$x\n     [,1] [,2]\n[1,] 21.0  160\n[2,] 21.0  160\n[3,] 22.8  108\n[4,] 21.4  258\n[5,] 18.7  360\n\n$y\n[1] 6 6 4 6 8\n\nIf you are iterating over a dataset using these functions, you will need to determine at what point to stop iteration. One approach to this is to use the dataset_repeat() function to create an dataset that yields values infinitely. For example:\n\nlibrary(tfdatasets)\n\nsess <- tf$Session()\n\nmtcarsspec <- csvrecord_spec(\"mtcars.csv\")\ndataset - textlinedataset(\"mtcars.csv\", recordspec = mtcarsspec) %% \n  dataset_shuffle(5000) %% \n  dataset_repeat() # repeat infinitely\n  dataset_prepare(x = c(mpg, disp), y = cyl) %% \n  dataset_batch(128) %% \n\niter <- makeiteratorone_shot(dataset)\nnextbatch <- iteratorget_next(iter)\n\nsteps <- 200\nfor (i in 1:steps)  \n\nIn this case the steps variable is used to determine when to stop drawing new batches of training data (we could have equally included code to detect a learning plateau or any other custom method of determining when to stop training).\n\nAnother approach is to detect when all batches have been yielded from the dataset. When a dataset iterator reaches the end, an out of range runtime error will occur. You can catch and ignore the error when it occurs by using outofrange_handler as the error argument to tryCatch(). For example:\n\nlibrary(tfdatasets)\n\nsess <- tf$Session()\n\nmtcarsspec <- csvrecord_spec(\"mtcars.csv\")\ndataset - textlinedataset(\"mtcars.csv\", recordspec = mtcarsspec) %% \n  dataset_prepare(x = c(mpg, disp), y = cyl) %% \n  dataset_batch(128) %% \n  dataset_repeat(10)\n  \niter <- makeiteratorone_shot(dataset)\nnextbatch <- iteratorget_next(iter)\n\ntryCatch( \n , error = outofrange_handler)\n\nYou can write this iteration more elegantly using the untiloutof_range() function, which automatically handles the error and provides the while(TRUE) around an expression:\n\nuntiloutof_range( )\n\nWhen running under eager execution, you organize the code a bit differently (since you don't need to explicitly run() tensors):\n\niter <- makeiteratorone_shot(dataset)\n\nuntiloutof_range( )\n\nReading Multiple Files\n\nIf you have multiple input files you can process them in parallel both across machines (sharding) and/or on multiple threads per-machine (parallel reads with interleaving). The read_files() function provides a high-level interface to parallel file reading. \n\nThe readfiles() function takes a set of files and a read function along with various options to orchestrate parallel reading. For example, the following function reads all CSV files in a directory using the textline_dataset() function:\n\ndataset <- readfiles(\"data/*.csv\", textlinedataset, recordspec = mtcars_spec,\n                      parallelfiles = 4, parallelinterleave = 16) %% \n  dataset_prefetch(5000) %% \n  datasetshuffleandrepeat(buffersize = 1000, count = 3) %% \n  dataset_batch(128)\n\nThe parallelfiles argument requests that 4 files be processed in parallel and the parallelinterleave argument requests that blocks of 16 consecutive records from each file be interleaved in the resulting dataset.\n\nNote that because we are processing files in parallel we do not pass the parallelrecords argument to textline_dataset(), since we are already parallelizing at the file level.\n\n Multiple Machines\n\nIf you are training on multiple machines and the training supervisor passes a shard index to your training script, you can also parallelizing reading by sharding the file list. For example:\n\ncommand line flags for training script (shard info is passed by training \n supervisor that executes the script)\nFLAGS <- flags(\n  flaginteger(\"numshards\", 1),\n  flaginteger(\"shardindex\", 1)\n)\n\nforward shard info to read_files\ndataset <- readfiles(\"data/*.csv\", textlinedataset, recordspec = mtcars_spec,\n                      parallelfiles = 4, parallelinterleave = 16,\n                      numshards = FLAGS$numshards, shardindex = FLAGS$shardindex) %% \n  datasetshuffleandrepeat(buffersize = 1000, count = 3) %% \n  dataset_batch(128) %% \n  dataset_prefetch(1) %% \n\n","id":81},{"path":"/guide/tfhub/hub-with-keras","title":"\"TensorFlow Hub with Keras\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfhub","menu":"menu:","  main":" main:","    name":"   name: \"Using with Keras\"","    identifier":"   identifier: \"tfhub-with-keras\"","    parent":"   parent: \"tfhub-basics-top\"","    weight":"   weight: 20","content":"\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#\", \n  eval = TRUE\n)\n\nTensorFlow Hub is a way to share pretrained model components. See the TensorFlow Module Hub for a searchable listing of pre-trained models. This tutorial demonstrates:\n\nHow to use TensorFlow Hub with Keras.\nHow to do image classification using TensorFlow Hub.\nHow to do simple transfer learning.\n\nSetup\n\nlibrary(keras)\nlibrary(tfhub)\nlibrary(magick)\n\n An ImageNet classifier\n\nDownload the classifier\n\nUse layer_hub to load a mobilenet and transform it into a Keras layer. \nAny TensorFlow 2 compatible image classifier URL from tfhub.dev will work here.\n\nclassifierurl <- \"https://tfhub.dev/google/tf2-preview/mobilenetv2/classification/2\" \nmobilenetlayer <- layerhub(handle = classifier_url)\n\nWe can then create our Keras model:\n\ninput <- layer_input(shape = c(224, 224, 3))\noutput - input %% \n  mobilenet_layer()\n\nmodel <- keras_model(input, output)\n\n Run it on a single image\n\nDownload a single image to try the model on.\n\nimg - imageread('https://storage.googleapis.com/download.tensorflow.org/exampleimages/grace_hopper.jpg') %%\n  image_resize(geometry = \"224x224x3!\") %% \n  image_data() %% \n  as.numeric() %% \n  abind::abind(along = 0) # expand to batch dimension\n\nplot(as.raster(img[1,,,]))\n\nresult <- predict(model, img)\nmobilenetdecodepredictions(result[,-1, drop = FALSE])\n\nSimple transfer learning\n\nUsing TF Hub it is simple to retrain the top layer of the model to recognize the \nclasses in our dataset.\n\n Dataset\n\nFor this example you will use the TensorFlow flowers dataset:\n\ndataroot <- getfile(\n  fname = \"flower_photos.tgz\",\n  origin = \"https://storage.googleapis.com/download.tensorflow.org/exampleimages/flowerphotos.tgz\"\n  )\nif (!dir.exists(\"flower_photos\"))  \ndataroot <- \"flowerphotos\"\n\nThe simplest way to load this data into our model is using imagedatagenerator\n\nAll of TensorFlow Hub's image modules expect float inputs in the [0, 1] range. Use the imagedatagenerator's rescale parameter to achieve this.\n\nimagegenerator <- imagedatagenerator(rescale = 1/255, validationsplit = 0.2)\ntrainingdata <- flowimagesfromdirectory(\n  directory = data_root, \n  generator = image_generator,\n  target_size = c(224, 224), \n  subset = \"training\"\n)\n\nvalidationdata <- flowimagesfromdirectory(\n  directory = data_root, \n  generator = image_generator,\n  target_size = c(224, 224), \n  subset = \"validation\"\n)\n\nThe resulting object is an iterator that returns imagebatch, labelbatch pairs.\n\nDownload the headless model\n\nTensorFlow Hub also distributes models without the top classification layer. These \ncan be used to easily do transfer learning.\n\nAny Tensorflow 2 compatible image feature vector URL from tfhub.dev will work here.\n\nfeatureextractorurl <- \"https://tfhub.dev/google/tf2-preview/mobilenetv2/featurevector/2\"\nfeatureextractorlayer <- layerhub(handle = featureextractor_url)\n\n Attach a classification head\n\nNow we can create our classification model by attaching a classification head into\nthe feature extractor layer. We define the following model:\n\ninput <- layer_input(shape = c(224, 224, 3))\noutput - input %% \n  featureextractorlayer() %% \n  layerdense(units = trainingdata$num_classes, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\nsummary(model)\n\nTrain the model\n\nWe can now train our model in the same way we would train any other Keras model.\nWe first use compile to configure the training process:\n\nmodel %% \n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"acc\"\n  )\n\nWe can then use the fit function  to fit our model.\n\nmodel %% \n  fit_generator(\n    training_data, \n    stepsperepoch = trainingdata$n/trainingdata$batch_size,\n    validationdata = validationdata\n  )\n\nYou can then export your model with:\n\nsavemodeltf(model, \"model\")\n\nYou can also reload the modelfromsaved_model function. Note that you need to\npass the custom_object with the definition of the KerasLayer since it/s not\na default Keras layer.\n\nreloadedmodel <- loadmodel_tf(\"model\")\n\nWe can verify that the predictions of both the trained model and the reloaded\nmodel are equal:\n\nsteps <- as.integer(validationdata$n/validationdata$batch_size)\nall.equal(\n  predictgenerator(model, validationdata, steps = steps),\n  predictgenerator(reloadedmodel, validation_data, steps = steps),\n)\n\nThe saved model can also be loaded for inference later or be converted to\nTFLite or TFjs.\n\n","id":82},{"path":"/guide/tfhub/intro","title":"\"Overview\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfhub","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"tfhub-overview\"","    parent":"   parent: \"tfhub-basics-top\"","    weight":"   weight: 10","content":"    \nThe tfhub package provides R wrappers to TensorFlow Hub.\n\nTensorFlow Hub is a library for reusable machine learning modules.\n\nTensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. Transfer learning can:\n\nTrain a model with a smaller dataset,\nImprove generalization, and\nSpeed up training.\n\nInstallation\n\nYou can install the released version of tfhub from CRAN with:\n\ninstall.packages(\"tfhub\")\n\nAnd the development version from GitHub with:\n\n install.packages(\"devtools\")\ndevtools::install_github(\"rstudio/tfhub\")\n\nAfter installing the tfhub package you need to install the TensorFlow Hub python \nmodule:\n\nlibrary(tfhub)\ninstall_tfhub()\n\nLoading modules\n\nModules can be loaded from URL's and local paths using hub_load()\n\nmodule <- hubload(\"https://tfhub.dev/google/tf2-preview/mobilenetv2/feature_vector/2\")\n\nModule's behave like functions and can be called with Tensors eg:\n\ninput <- tf$random$uniform(shape = shape(1,224,224,3), minval = 0, maxval = 1)\noutput <- module(input)\n\n Using with Keras\n\nThe easiest way to get started with tfhub is using layer_hub. A Keras layer that\nloads a TensorFlow Hub module and prepares it for using with your model.\n\nlibrary(tfhub)\nlibrary(keras)\n\ninput <- layer_input(shape = c(32, 32, 3))\n\noutput - input %%\n  # we are using a pre-trained MobileNet model!\n  layerhub(handle = \"https://tfhub.dev/google/tf2-preview/mobilenetv2/feature_vector/2\") %%\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\nmodel %%\n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nWe can then fit our model in the CIFAR10 dataset:\n\ncifar <- dataset_cifar10()\ncifar$train$x <- tf$image$resize(cifar$train$x/255, size = shape(224,224))\n\nmodel %%\n  fit(\n    x = cifar$train$x,\n    y = cifar$train$y,\n    validation_split = 0.2,\n    batch_size = 128\n  )\n\nUsing with tfdatasets\n\ntfhub can also be used with tfdatasets since it provides implementations of feature_columns:\n\nhubtextembedding_column()\nhubsparsetextembeddingcolumn()\nhubimageembedding_column()\n\nYou can find a working example here.\n\n Using with recipes\n\ntfhub adds a steppretrainedtext_embedding that can be used with the recipes package.\n\nAn example can be found here.\n\ntfhub.dev\n\ntfhub.dev is a gallery of pre-trained model ready to be used with TensorFlow Hub.\n\n","id":83},{"path":"/guide/tfhub/key-concepts","title":"\"Key concepts\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/tfhub","menu":"menu:","  main":" main:","    name":"   name: \"Key Concepts\"","    identifier":"   identifier: \"tfhub-key-concepts\"","    parent":"   parent: \"tfhub-key-top\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#\", \n  eval = FALSE\n)\n\nUsing a Module\n\n Instantiating a Module\n\nA TensorFlow Hub module is imported into a TensorFlow program by creating a Module object from a string with its URL or filesystem path, such as:\n\nlibrary(tfhub)\nm <- hubload(\"path/to/a/moduledir\")\n\nThis adds the module's variables to the current TensorFlow graph. \n\nCaching Modules\n\nWhen creating a module from a URL, the module content is downloaded and cached in the local system temporary directory. The location where modules are cached can be overridden using TFHUBCACHEDIR environment variable.\n\nFor example, setting TFHUBCACHEDIR to /mymodulecache:\n\nSys.setenv(TFHUBCACHEDIR = \"/mymodulecache\")\n\nand then creating a module from a URL:\n\nm <- hub_load(\"https://tfhub.dev/google/progan-128/1\")\n\nresults in downloading and unpacking the module into /mymodulecache.\n\n Applying a Module\n\nOnce instantiated, a module m can be called zero or more times like a Python function from tensor inputs to tensor outputs:\n\ny <- m(x)\n\nEach such call adds operations to the current TensorFlow graph to compute y from x. If this involves variables with trained weights, these are shared between all applications.\n\nModules can define multiple named signatures in order to allow being applied in more than one way (similar to how Python objects have methods). A module's documentation should describe the available signatures. The call above applies the signature named \"default\". Any signature can be selected by passing its name to the optional signature= argument.\n\nIf a signature has multiple inputs, they must be passed as a dict, with the keys defined by the signature. Likewise, if a signature has multiple outputs, these can be retrieved as a dict by passing asdict=True, under the keys defined by the signature. (The key \"default\" is for the single output returned if asdict=FALSE) So the most general form of applying a Module looks like:\n\noutputs <- m(list(apples=x1, oranges=x2), signature=\"fruittopet\", as_dict=TRUE)\ny1 = outputs$cats\ny2 = outputs$dogs\n\nA caller must supply all inputs defined by a signature, but there is no requirement to use all of a module's outputs. Module consumers should handle additional outputs gracefully.\n\nCreating a new Module\n\n General approach\n\nA Hub Module is simply a TensorFlow graph in the SavedModel format. In order\nto create a Module you can run the export_savedmodel function with any\nTensorFlow object.\n\nFor example:\n\nlibrary(keras)\n\nmnist <- dataset_mnist()\n\ninput <- layer_input(shape(28,28), dtype = \"int32\")\n\noutput - input %% \n  layer_flatten() %% \n  layerlambda(tensorflow::tffunction(function(x) tf$cast(x, tf$float32)/255)) %% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel <- keras_model(input, output)\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"acc\"\n  )\n\nmodel %% \n  fit(x = mnist$train$x, y = mnist$train$y, validation_split = 0.2, epochs =1 )\n\nsavemodeltf(model, \"mymodule/\", includeoptimizer = FALSE)\n\nAfter exporting the model to the SavedModel format you can load it using hub_load,\nand use it for predictions for example:\n\nmodule <- hubload(\"mymodule/\")\n\npredictions - module(mnist$test$x) %% \n  tf$argmax(axis = 1L) \n\nmean(as.integer(predictions) == mnist$test$y)\n\nExporting a module serializes its definition together with the current state of \nits variables in session into the passed path. This can be used when exporting a \nmodule for the first time, as well as when exporting a fine tuned module.\n\nModule publishers should implement a common signature when possible, so that consumers can easily exchange modules and find the best one for their problem.\n\nFine-Tuning\n\nTraining the variables of an imported module together with those of the model around it is called fine-tuning. Fine-tuning can result in better quality, but adds new complications. We advise consumers to look into fine-tuning only after exploring simpler quality tweaks.\n\n For consumers\n\nTo enable fine-tuning, instantiate the module with hubmodule(..., trainable = TRUE) to make its variables trainable and import TensorFlow's REGULARIZATIONLOSSES. If the module has multiple graph variants, make sure to pick the one appropriate for training. Usually, that's the one \nwith tags  .\n\nChoose a training regime that does not ruin the pre-trained weights, for example, \na lower learning rate than for training from scratch.\n\nFor publishers\n\nTo make fine-tuning easier for consumers, please be mindful of the following:\n\nFine-tuning needs regularization. Your module is exported with the REGULARIZATIONLOSSES collection, which is what puts your choice of layerdense(..., kernelregularizer=...) etc. into what the consumer gets from tf$losses$getregularization_losses(). Prefer this way of defining L1/L2 regularization losses.\n\nIn the publisher model, avoid defining L1/L2 regularization via the l1 and l2regularization_strength parameters of tf$train$FtrlOptimizer, tf$train$ProximalGradientDescentOptimizer, and other proximal optimizers. These are not exported alongside the module, and setting regularization strengths globally may not be appropriate for the consumer. Except for L1 regularization in wide (i.e. sparse linear) or wide & deep models, it should be possible to use individual regularization losses instead.\n\nIf you use dropout, batch normalization, or similar training techniques, set dropout rate and other hyperparameters to values that make sense across many expected uses.\n\n Hosting a Module\n\nTensorFlow Hub supports HTTP based distribution of modules. In particular the protocol allows to use the URL identifying the module both as the documentation of the module and the endpoint to fetch the module.\n\nProtocol\n\nWhen a URL such as https://example.com/module is used to identify a module to load or instantiate, the module resolver will attempt to download a compressed tar ball from the URL after appending a query parameter ?tf-hub-format=compressed.\n\nThe query param is to be interpreted as a comma separated list of the module formats that the client is interested in. For now only the \"compressed\" format is defined.\n\nThe compressed format indicates that the client expects a tar.gz archive with the module contents. The root of the archive is the root of the module directory and should contain a module e.g.:\n\n Create a compressed module from an exported module directory.\n$ tar -cz -f module.tar.gz --owner=0 --group=0 -C /tmp/export-module/ .\n\nInspect files inside a compressed module\n$ tar -tf module.tar.gz\n./\n./tfhub_module.pb\n./variables/\n./variables/variables.data-00000-of-00001\n./variables/variables.index\n./assets/\n./saved_model.pb\n\n","id":84},{"path":"/installation/_","title":"\"Quick start\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Quick start\"","    identifier":"   identifier: \"installation-get-started\"","    parent":"   parent: \"installation-overview\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(eval = FALSE)\n\nPrior to using the tensorflow R package you need to install a version of TensorFlow \non your system. Below we describe how to install TensorFlow as well the various \noptions available for customizing your installation. \n\nNote that this article principally covers the use of the R install_tensorflow() function, \nwhich provides an easy to use wrapper for the various steps required to install \nTensorFlow.\n\nYou can also choose to install TensorFlow manually (as described at https://www.tensorflow.org/install/). In that case the [Custom Installation] section \ncovers how to arrange for the tensorflow R package to use the version you installed.\n\nTensorFlow is tested and supported on the following 64-bit systems:\n\nUbuntu 16.04 or later\nWindows 7 or later\nmacOS 10.12.6 (Sierra) or later (no GPU support)\n\nInstallation\n\nFirst, install the tensorflow R package from GitHub as follows:\n\ninstall.packages(\"tensorflow\")\n\nThen, use the install_tensorflow() function to install TensorFlow. Note that on Windows you need a working installation of Anaconda.\n\nlibrary(tensorflow)\ninstall_tensorflow()\n\nYou can confirm that the installation succeeded with:\n\nlibrary(tensorflow)\ntf$constant(\"Hellow Tensorflow\")\n\nThis will provide you with a default installation of TensorFlow suitable for use \nwith the tensorflow R package. Read on if you want to learn about additional installation options, including installing a version of TensorFlow that takes advantage of Nvidia GPUs if you have the correct CUDA libraries installed.\n\n Installation methods\n\nTensorFlow is distributed as a Python package and so needs to be installed within a Python environment on your system. By default, the install_tensorflow() function attempts to install TensorFlow within an isolated Python environment (\"r-reticulate\").\n\nThese are the available methods and their behavior:\n\n| Method  | Description  |\n|--------------|----------------------------------------------------------|\n| auto  | Automatically choose an appropriate default for the current platform. |\n| virtualenv  | Install into a Python virtual environment at ~/.virtualenvs/r-reticulate |\n| conda  | Install into an Anaconda Python environment named r-reticulate |\n| system  | Install into the system Python environment  |\n\nThe \"virtualenv\" and \"conda\" methods are available on Linux and OS X and only the \"conda\" method is available on Windows. \n\ninstalltensorflow is a wraper around reticulate::pyinstall. Please refer to 'Installing Python Packages' for more information.\n\nAlternate Versions\n\nBy default, install_tensorflow() install the latest release version of TensorFlow. You can override this behavior by specifying the version parameter. For example:\n\ninstall_tensorflow(version = \"2.0.0\")\n\nNote that this should be a full major.minor.patch version specification (rather than just major and minor versions).\n\nYou can install the nightly build of TensorFlow (CPU or GPU version) with:\n\ninstall_tensorflow(version = \"nightly\")       cpu version\ninstall_tensorflow(version = \"nightly-gpu\")  # gpu version\n\nYou can install any other build of TensorFlow by specifying a URL to a TensorFlow binary. For example:\n\ninstalltensorflow(version = \"https://files.pythonhosted.org/packages/c2/c1/a035e377cf5a5b90eff27f096448fa5c5a90cbcf13b7eb0673df888f2c2d/tfnightly-1.12.0.dev20180918-cp36-cp36m-manylinux1x8664.whl\")\n\n  ","id":85},{"path":"/installation/custom","title":"\"Custom Installation\"","type":"docs","output":"","  html_document":" html_document:","    toc_depth":"   toc_depth: 3","    toc_float":"   toc_float:","        collapsed":"       collapsed: false","menu":"menu:","  main":" main:","    name":"   name: \"Custom Installation\"","    identifier":"   identifier: \"custom-installation\"","    parent":"   parent: \"installation-overview\"","    weight":"   weight: 11 ","content":"\nknitr::opts_chunk$set(eval = FALSE)\n\nThe install_tensorflow() function is provided as a convenient way to get started, but is not required. If you have an existing installation of TensorFlow or just prefer your own custom installation that's fine too.\n\nThe full instructions for installing TensorFlow on various platforms are here: https://www.tensorflow.org/install/. After installing, please refer to the sections below on locating TensorFlow and meeting additional dependencies to ensure that the tensorflow for R package functions correctly with your installation.\n\nLocating TensorFlow\n\nOnce you've installed TensorFlow you need to ensure that the tensorflow for R package can find your installation. The package scans the system for various versions of Python, and also scans available virtual environments and conda environments, so in many cases things will just work without additional effort. \n\nIf the version of TensorFlow you installed is not found automatically, then you can use the following techniques to ensure that TensorFlow is located.\n\nSpecify the RETICULATE_PYTHON environment variable to force probing within a specific Python installation. For example:\n\nlibrary(tensorflow)\nSys.setenv(RETICULATE_PYTHON=\"/usr/local/bin/python\")\n\nYou could also add the RETICULATE_PYTHON environment variable to your .RProfile.\n\nAlternatively, call the use_python family of configuration functions:\n\n| Function  | Description | \n|----------------|------------------------------------------------------------------|\n| use_python() | Specify the path a specific Python binary. | \n| use_virtualenv() | Specify the directory containing a Python virtualenv. | \n| use_condaenv() | Specify the name of a conda environment. | \n\nFor example:\n\nlibrary(tensorflow)\nuse_python(\"/usr/local/bin/python\")\nuse_virtualenv(\"~/myenv\")\nuse_condaenv(\"myenv\")\n\nNote that you can include multiple calls to the use_ functions and all provided locations will be tried in the order they were specified.\n\nYou can also use the required argument from the use_* functions. In this case\nTensorFlow will only be searched at the specified location.\n\nlibrary(tensorflow)\nusevirtualenv(\"myenv\", required = TRUE)\n\n Additional Dependencies\n\nThere are some components of TensorFlow (e.g. the Keras library) which have dependencies on additional Python packages. The install_tensorflow() function installs these dependencies automatically, however if you do a custom installation you should be sure to install them manually. \n\nYou can install the additional dependencies with the following command:\n\npip install h5py pyyaml requests Pillow scipy\n\nSupported Platforms\n\nNote that binary installations of TensorFlow are provided for Windows, OS X, and Ubuntu 16.04 or higher. It's possible that binary installations will work on other Linux variants but Ubuntu is the only platform tested and supported. \n\nIn particular, if you are running on RedHat or CentOS you will need to install from source then follow the instructions in the [Custom Installation] section to ensure that your installation of TensorFlow can be used with the tensorflow R package.\n","id":86},{"path":"/installation/gpu/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"installation-gpu-quick\"","    parent":"   parent: \"installation-gpu\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(eval = FALSE)\n\nIt's highly recommended, although not strictly necessary, that you run deep-learning code on a modern NVIDIA GPU. Some applications -- in particular, image processing with convolutional \nnetworks and sequence processing with recurrent neural networks -- will be excruciatingly slow on CPU, even a fast multicore CPU. And even for applications that can realistically be run on CPU, you'll generally see speed increase by a factor or 5 or 10 by using a modern GPU. \n\nIf your local workstation doesn't already have a GPU that you can use for deep learning (a recent, high-end NVIDIA GPU), then running deep learning experiments in the cloud is a simple, low-cost way for you to get started without having to buy any additional hardware. See the documentation below for details on using both local and cloud GPUs.\n\n|  |  |\n|---------------|---------------------------------------------------------------|\n| a href=\"local_gpu.html\"Local GPUbr/img class=\"nav-image\" src=\"images/local-gpu-illustration.png\" width=64//a | For systems that have a recent, high-end NVIDIA® GPU, TensorFlow is available in a GPU version that takes advantage of the CUDA and cuDNN libraries to accelerate training performance. Note that the GPU version of TensorFlow is currently only supported on Windows and Linux (there is no GPU version available for Mac OS X since NVIDIA GPUs are not commonly available on that platform).  |\n| a href=\"cloudml/articles/getting_started.html\"CloudMLbr/img class=\"nav-image\" src=\"images/cloud-ml-illustration.png\" width=64//a | Google CloudML is a managed service that provides on-demand access to training on GPUs, including the new Tesla P100 GPUs from NVIDIA. CloudML also provides hyperparameter tuning to optmize key attributes of model architectures in order to maximize predictive accuracy. |\n| a href=\"cloudservergpu.html\"Cloud Serverbr/img class=\"nav-image\" src=\"images/cloud-server-illustration.png\" width=64//a| Cloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally. |\n| a href=\"clouddesktopgpu.html\"Cloud Desktopbr/img class=\"nav-image\" src=\"images/cloud-desktop-illustration.png\" width=64//a | Virtual cloud desktops with GPUs are available from Paperspace. This provides an Ubuntu 16.04 desktop environment that you can access entirely within a web browser (note that this requires a reasonbly fast internet connection to be usable).|","id":87},{"path":"/installation/gpu/cloud_desktop_gpu/","title":"\"Cloud Desktop GPUs\"","output":"","  html_document":" html_document:","    toc_depth":"   toc_depth: 3","    toc_float":"   toc_float:","        collapsed":"       collapsed: false","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Cloud Desktop\"","    identifier":"   identifier: \"tools-cloud-desktop-gpu\"","    parent":"   parent: \"installation-gpu\"","    weight":"   weight: 50","content":"\n\nknitr::opts_chunk$set(echo = TRUE)\n\nOverview\n\nCloud desktops with various GPU configurations are available from Paperspace. With Paperspace, you can access a full Linux desktop running Ubuntu 16.04 all from within a web browser. An SSH interface is also available, as is a browser based RStudio Server interface (via SSH tunnel).\n\nPaperspace offers an RStudio TensorFlow template with NVIDIA GPU libraries (CUDA 8.0 and cuDNN 6.0) pre-installed, along with the GPU version of TensorFlow v1.4 and the R keras, tfestimators, and tensorflow packages. Follow the instructions below to get started with using RStudio on Paperspace.\n\n Getting Started\n\nTo get started, sign up for a Paperspace account here: https://www.paperspace.com/account/signup (you can use the RSTUDIO promo code when you sign up to receive a $5 account credit).\n\n \n\nAfter you've signed up and verified your account email, you will be taken to a Create Machine page. Here you'll select various options including your compute region and machine template. You should select the RStudio template:\n\n \n\nBe sure to select one of the GPU instances (as opposed to the CPU instances). For example, here we select the P4000 machine type which includes an NVIDIA Quadro P4000 GPU:\n\n \n\nAfter your machine is provisioned (this can take a few minutes) you are ready to access it via a web browser. Hover over the machine in the Paperspace Console and click the \"Launch\" link:\n\n \n\nAfter the machine is launched you'll see your Linux desktop within the browser you launched it from. You may need to use the Scaling Settings to adjust the desktop to a comfortable resolution:\n\n \n\nYou should also change your default password using the passwd utility (your default password should have been sent to you in an email titled \"Your new Paperspace Linux machine is ready\"):\n\n \n\nYou now have a Linux desktop equipped ready to use with TensorFlow for R! Go ahead and run RStudio from the application bar:\n\n \n\nNVIDIA GPU libraries (CUDA 9 and cuDNN 7) are pre-installed, along with the GPU version of TensorFlow v1.7. The R keras, tfestimators, and tensorflow packages are also pre-installed, as are all of the packages from the [tidyverse[(https://www.tidyverse.org/)] (dplyr, ggplot2, etc.).\n\nAn important note about the pre-installed dependencies: Since the NVIDIA CUDA libraries, TensorFlow, and Keras are all pre-installed on the Paperspace instances, you should not use the installtensorflow() or installkeras() functions, but rather rely on the existing, pre-configured versions of these libraries. Installing or updating other versions of these libraries will likely not work at all!\n\nAutomatic Shutdown\n\nYou can set Paperspace machines to automatically shutdown when they have not been used for a set period of time (this is especially important since machine time is billed by the hour). You can access this setting from the Paperspace console for your machine:\n\n \n\nHere the auto-shutdown time is set to 1 day, however you can also choose shorter or longer intervals.\n\n Terminal Access\n\nWeb Terminal\n\nYou can use the Open Terminal command on the Paperspace console for your machine to open a web based terminal to your machine:\n\n \n\nYou'll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\n SSH Login\n\nYou can also login to your Paperspace instance using a standard SSH client. This requires that you first Assign a public IP address to your machine (note that public IP addresses cost an additional $3/month).\n\nOnce you have your public IP address, you can SSH into your machine as follows:\n\n$ ssh paperspace@public IP\n\nYou'll need to login using either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\nRStudio Server \n\nYou may prefer using the RStudio Server browser-based interface to the virtual Linux desktop provided by Paperspace (especially when on slower internet connections). This section describes how to access your Paperspace machine using an SSH tunnel.\n\nTo start with, follow the instructions for [SSH Login] immediately above and ensure that you can login to your machine remotely via SSH.\n\nOnce you've verified this, you should also be able to setup an SSH tunnel to RStudio Server as follows:\n\n$ ssh -N -L 8787:127.0.0.1:8787 paperspace@public-ip\n\nYou can access RStudio Server by navigating to port 8787 on your local machine and logging in using the paperspace account and either the default password emailed to you when you created the machine or to the new password which you subsequently created.\n\nhttp://localhost:8787\n\n \n\n","id":88},{"path":"/installation/gpu/cloud_server_gpu/","title":"\"Cloud Server GPUs\"","output":"","  html_document":" html_document:","    toc_depth":"   toc_depth: 3","    toc_float":"   toc_float:","        collapsed":"       collapsed: false","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Cloud Server\"","    identifier":"   identifier: \"tools-cloud-server-gpu\"","    parent":"   parent: \"installation-gpu\"","    weight":"   weight: 40","content":"\n\nknitr::opts_chunk$set(echo = TRUE)\n\nOverview\n\nCloud server instances with GPUs are available from services like Amazon EC2 and Google Compute Engine. You can use RStudio Server on these instances, making the development experience nearly identical to working locally.\n\n Amazon EC2\n\nRStudio has AWS Marketplace offerings that are designed to provide stable, secure, and high performance execution environments for deep learning applications running on Amazon EC2. The tensorflow, tfestimators, and keras R packages (along with their pre-requisites, including the GPU version of TensorFlow) are installed as part of the image. \n\nLaunching the Server\n\nThere are AMIs on the Amazon Cloud Marketplace for both the open-source and Professional versions of RStudio Server. You can find them here:\n\nOpen Source: https://aws.amazon.com/marketplace/pp/B0785SXYB2\n\nProfessional: https://aws.amazon.com/marketplace/pp/B07B8G3FZP\n\nYou should launch these AMIs on the p2.xlarge instance type. This type includes a single GPU whereas other GPU-based images include up to 16 GPUs (however they are commensurately much more expensive). Note that you may need to select a different region than your default to be able to launch p2.xlarge instances (for example, selecting \"US East (Ohio)\" rather than \"US East (N Virginia)\"). \n\n \n\n Accessing the Server\n\nAfter you've launched the server you can access an instance of RStudio Server running on port 8787. For example: \n\nhttp://ec2-18-217-204-43.us-east-2.compute.amazonaws.com:8787\n\nNote that the above server address needs to be substituted for the public IP of the server you launched, which you can find in the EC2 Dashboard.\n\nThe first time you access the server you will be presented with a login screen:\n\n \n\nLogin with user id \"rstudio-user\" and password the instance ID of your AWS server (for example \"i-0a8ea329c18892dfa\", your specific ID is available via the EC2 dashboard). \n\nThen, use the RStudio Terminal to change the default password using the passwd utility:\n\n \n\nYour EC2 deep learning instance is now ready to use (the tensorflow, tfestimators, and keras R packages along with their pre-requisites, including the GPU version of TensorFlow, are installed as part of the image). \n\nSee the sections below for discussion of various ways in which you can make your EC2 instance more secure.\n\nLimiting Inbound Traffic\n\nThe EC2 instance is by default configured to allow access to SSH and HTTP traffic from all IP addresses on the internet, whereas it would be more desirable to restrict this to IP addresses that you know you will access the server from (this can however be challenging if you plan on accessing the server from a variety of public networks). \n\nYou can see these settings in the Security Group of your EC2 instance:\n\n \n\nEdit the Source for the SSH and HTTP protocols to limit access to specific blocks of IP addresses.\n\n Using HTTPS\n\nBy default the EC2 instance which you launched is accessed over HTTP, a non-encrypted channel. This means that data transmitted to the instance (including your username and password) can potentially be compromised during transmission.\n\nThere are many ways to add HTTPS support to a server including AWS Elastic Load Balancing, CloudFlare SSL, and setting up reverse proxy from an Nginx or Apache web server configured with SSL support.\n\nThe details of adding HTTPS support to your server are beyond the scope of this article (see the links above to learn more). An alternative to this is to prohibit external HTTP connections entirely and access the server over an SSH Tunnel, this option is covered in the next section.\n\nSSH Tunnel\n\nUsing an SSH Tunnel to access your EC2 instance provides a number of benefits, including:\n\n1) Use of the SSH authentication protocol to identify and authorize remote users\n\n2) Encrypting traffic that would otherwise be sent in the clear\n\nNote that SSH tunnel access as described below works only for Linux and OS X clients.\n\n Security Group\n\nTo use an SSH Tunnel with your EC2 instance, first configure the Security Group of your instance to only accept SSH traffic (removing any HTTP entry that existed previously):\n\n \n\nNote that you may also want to restrict the Source of SSH traffic to the specific block of IP addresses you plan to access the server from.\n\nServer Configuration\n\nNext, connect to your instance over SSH (click the Connect button in the EC2 console for instructions specific to your server):\n\nssh -i \"my-security-key.pem\" ubuntu@my-ec2-server-address\n\nNote that if you copied and pasted the command from the EC2 console you may see this error message:\n\nPlease login as the user \"ubuntu\" rather than the user \"root\".\n\nIn that case be sure that you use ubuntu@my-ec2-server-address rather than root@my-ec2-server-address.\n\nExecute the following commands to configure RStudio Server to only accept local connections:\n\n Configure RStudio to only allow local connections \nsudo /bin/bash -c \"echo 'www-address=127.0.0.1'  /etc/rstudio/rserver.conf\"\n\nRestart RStudio with new settings\nsudo rstudio-server restart\n\n Connecting to the Server\n\nYou should now be able to connect to the server via SSH tunnel as follows:\n\nssh -N -L 8787:localhost:8787 -i my-security-key.pem ubuntu@my-ec2-server-address\n\n(where my-security-key.pem and my-ec2-server-address are specific to your server configuration).\n\nOnce the SSH connection is established, RStudio Server will be available at http://localhost:8787/\n\n","id":89},{"path":"/installation/gpu/local_gpu/","title":"\"Local GPU\"","output":"","  html_document":" html_document:","    toc_depth":"   toc_depth: 3","    toc_float":"   toc_float:","        collapsed":"       collapsed: false","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Local GPU\"","    identifier":"   identifier: \"tools-local-gpu\"","    parent":"   parent: \"installation-gpu\"","    weight":"   weight: 20","aliases":"aliases:","content":"\n\nknitr::opts_chunk$set(echo = TRUE)\n\nOverview\n\nTensorFlow can be configured to run on either CPUs or GPUs. The CPU version is much easier to install and configure so is the best starting place especially when you are first learning how to use TensorFlow. Here's the guidance on CPU vs. GPU versions from the TensorFlow website:\n\nTensorFlow with CPU support only. If your system does not have a NVIDIA® GPU, you must install this version. Note that this version of TensorFlow is typically much easier to install (typically, in 5 or 10 minutes), so even if you have an NVIDIA GPU, we recommend installing this version first.\n\nTensorFlow with GPU support. TensorFlow programs typically run significantly faster on a GPU than on a CPU. Therefore, if your system has a NVIDIA® GPU meeting the prerequisites shown below and you need to run performance-critical applications, you should ultimately install this version.\n\nSo if you are just getting started with TensorFlow you may want to stick with the CPU version to start out, then install the GPU version once your training becomes more computationally demanding.\n\nThe prerequisites for the GPU version of TensorFlow on each platform are covered below. Once you've met the prerequisites installing the GPU version in a single-user / desktop environment is as simple as:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf you are using Keras you can install both Keras and the GPU version of TensorFlow with:\n\nlibrary(keras)\ninstall_keras(tensorflow = \"gpu\")\n\nNote that on all platforms you must be running an NVIDIA® GPU with CUDA® Compute Capability 3.5 or higher in order to run the GPU version of TensorFlow. See the list of CUDA-enabled GPU cards.\n\n Prerequisties\n\nWindows\n\nThis article describes how to detect whether your graphics card uses an NVIDIA® GPU:\n\nhttp://nvidia.custhelp.com/app/answers/detail/a_id/2040/~/identifying-the-graphics-card-model-and-device-id-in-a-pc\n\nOnce you've confirmed that you have an NVIDIA® GPU, the following article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN = v7.4.1:\n\nhttps://www.tensorflow.org/install/gpuhardware_requirements\n\nNote that the documentation on installation of the last component (cuDNN v7.4.1) is a bit sparse. Once you join the NVIDIA® developer program and download the zip file containing cuDNN you need to extract the zip file and add the location where you extracted it to your system PATH.\n\nUbuntu\n\nThis article describes how to install required software components including the CUDA Toolkit v10.0, required NVIDIA® drivers, and cuDNN = v7.4.1: \n\nhttps://www.tensorflow.org/install/installlinuxnvidiarequirementstoruntensorflowwithgpusupport\n\nThe specifics of installing required software differ by Linux version so please review the NVIDIA® documentation carefully to ensure you install everything correctly. \n\nThe following section provides as example of the installation commands you might use on Ubuntu 16.04. \n\nUbuntu 16.04 Example\n\nFirst, install the NVIDIA drivers:\n\n Add NVIDIA package repositories\nAdd HTTPS support for apt-key\nsudo apt-get install gnupg-curl\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x8664/cuda-repo-ubuntu160410.0.130-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu160410.0.130-1amd64.deb\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\nsudo apt-get update\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x8664/nvidia-machine-learning-repo-ubuntu16041.0.0-1_amd64.deb\nsudo apt install ./nvidia-machine-learning-repo-ubuntu16041.0.0-1amd64.deb\nsudo apt-get update\n\n Install NVIDIA driver\nIssue with driver install requires creating /usr/lib/nvidia\nsudo mkdir /usr/lib/nvidia\nsudo apt-get install --no-install-recommends nvidia-410\n Reboot. Check that GPUs are visible using the command: nvidia-smi\n\nNext install CUDA Toolkit v10.0 and cuDNN v7.4.1 with:\n\nInstall development and runtime libraries (~4GB)\nsudo apt-get install --no-install-recommends \\\n    cuda-10-0 \\\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\n \nNote that it's important to download CUDA 10.0 (rather than CUDA 10.1, which may be the choice initially presented) as v10.0 is what TensorFlow is built against.\n\nYou can see more for the installation here.\n\n Environment Variables\n\nOn Linux, part of the setup for CUDA libraries is adding the path to the CUDA binaries to your PATH and LDLIBRARYPATH as well as setting the CUDA_HOME environment variable. You will set these variables in distinct ways depending on whether you are installing TensorFlow on a single-user workstation or on a multi-user server. If you are running RStudio Server there is some additional setup required which is also covered below.\n\nIn all cases these are the environment variables that need to be set/modified in order for TensorFlow to find the required CUDA libraries. For example (paths will change depending on your specific installation of CUDA):\n\nexport CUDA_HOME=/usr/local/cuda\nexport LDLIBRARYPATH=$ :$ /lib64 \nPATH=$ /bin:$  \nexport PATH\n\nSingle-User Installation\n\nIn a single-user environment (e.g. a desktop system) you should define the environment variables within your ~/.profile file. It's necessary to use ~/.profile rather than ~/.bashrc, because ~/.profile is read by desktop applications (e.g. RStudio) as well as terminal sessions whereas ~/.bashrc applies only to terminal sessions.\n\nNote that you need to restart your system after editing the ~/.profile file for the changes to take effect. Note also that the ~/.profile file will not be read by bash if you have either a ~/.bashprofile or ~/.bashlogin file.\n\nTo summarize the recommendations above:\n\nDefine CUDA related environment variables in ~/.profile rather than ~/.bashrc;\n\nEnsure that you don't have either a  ~/.bashprofile or ~/.bashlogin file (as these will prevent bash from seeing the variables you've added into ~/.profile);\n\nRestart your system after editing ~/.profile so that the changes take effect.\n\n Multi-User Installation\n\nIn a multi-user installation (e.g. a server) you should define the environment variables within the system-wide bash startup file (/etc/profile) so all users have access to them.\n\nIf you are running RStudio Server you need to also provide these variable definitions in an R / RStudio specific fashion (as RStudio Server doesn't execute system profile scripts for R sessions). \n\nTo modify the LDLIBRARYPATH you use the rsession-ld-library-path in the /etc/rstudio/rserver.conf configuration file\n\n/etc/rstudio/rserver.conf\n\nrsession-ld-library-path=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n\nYou should set the CUDA_HOME and PATH variables in the /usr/lib/R/etc/Rprofile.site configuration file:\n\n/usr/lib/R/etc/Rprofile.site\n\nSys.setenv(CUDA_HOME=\"/usr/local/cuda\")\nSys.setenv(PATH=paste(Sys.getenv(\"PATH\"), \"/usr/local/cuda/bin\", sep = \":\"))\n\nIn a server environment you might also find it more convenient to install TensorFlow into a system-wide location where all users of the server can share access to it. Details on doing this are covered in the multi-user installation section below.\n\nMac OS X\n\nAs of version 1.2 of TensorFlow, GPU support is no longer available on Mac OS X. If you want to use a GPU on Mac OS X you will need to install TensorFlow v1.1 as follows:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"1.1-gpu\")\n\nHowever, before you install you should ensure that you have an NVIDIA® GPU and that you have the required CUDA libraries on your system.\n\nWhile some older Macs include NVIDIA® GPU's, most Macs (especially newer ones) do not, so you should check the type of graphics card you have in your Mac before proceeding. \n\nHere is a list of Mac systems which include built in NVIDIA GPU's:\n\nhttps://support.apple.com/en-us/HT204349\n\nYou can check which graphics card your Mac has via the System Report button found within the About This Mac dialog:\n\n \n\nThe MacBook Pro system displayed above does not have an NVIDIA® GPU installed (rather it has an Intel Iris Pro).\n\nIf you do have an NVIDIA® GPU, the following article describes how to install the base CUDA libraries:\n\nhttp://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\n\nYou also need to intall the cuDNN library 5.1 library for OS X from here:\n\nhttps://developer.nvidia.com/cudnn\n\nAfter installing these components, you need to ensure that both CUDA and cuDNN are available to your R session via the DYLDLIBRARYPATH. This typically involves setting environment variables in your .bash_profile as described in the NVIDIA documentation for CUDA and cuDNN.\n\nNote that environment variables set in .bash_profile will not be available by default to OS X desktop applications like R GUI and RStudio. To use CUDA within those environments you should start the application from a system terminal as follows:\n\nopen -a R          R GUI\nopen -a RStudio   # RStudio\n\nInstallation\n\n Single User\n\nIn a single-user desktop environment you can install TensorFlow with GPU support via:\n\nlibrary(tensorflow)\ninstall_tensorflow(version = \"gpu\")\n\nIf this version doesn't load successfully you should review the prerequisites above and ensure that you've provided definitions of CUDA environment variables as recommended above.\n\nSee the main installation article for details on other available  options (e.g. virtualenv vs. conda installation, installing development versions, etc.).\n\nMultiple Users\n\nIn a multi-user server environment you may want to install a system-wide version of TensorFlow with GPU support so all users can share the same configuration. To do this, start by following the directions for native pip installation of the GPU version of TensorFlow here:\n\nhttps://www.tensorflow.org/install/install_linuxInstallingNativePip\n\nThere are some components of TensorFlow (e.g. the Keras library) which have dependencies on additional Python packages.\n\nYou can install Keras and it's optional dependencies with the following command (ensuring you have the correct privilege to write to system library locations as required via sudo, etc.):\n\npip install keras h5py pyyaml requests Pillow scipy\n\nIf you have any trouble with locating the system-wide version of TensorFlow from within R please see the section on locating TensorFlow.\n\n    \n    ","id":90},{"path":"/learn/resources","title":"Learning Resources","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Resources\"","    identifier":"   identifier: \"learn-resources\"","    parent":"   parent: \"learn-top\"","    weight":"   weight: 10","aliases":"aliases:","content":"\n|  |  |\n|-----------------|---------------------------------------------------------------|\n| a href=\"https://www.amazon.com/Deep-Learning-R-Francois-Chollet/dp/161729554X\"img class=\"nav-image illustration\" src=\"../images/resources-deep-learning-with-r.png\" width=250//a  | Deep Learning with R br/Deep Learning with R is meant for statisticians, analysts, engineers, and students with a reasonable amount of R experience but no significant knowledge of machine learning and deep learning. You’ll learn from more than 30 code examples that include detailed commentary and practical recommendations. You don’t need previous experience with machine learning or deep learning: this book covers from scratch all the necessary basics. You don’t need an advanced mathematics background, either—high school level mathematics should suffice in order to follow along. |\n| a href=\"https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618\"img class=\"nav-image illustration\" src=\"../images/resources-deep-learning.jpg\" width=250//a  | Deep Learning br/An introduction to a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology. |\n| a href=\"https://github.com/rstudio/cheatsheets/raw/master/keras.pdf\"img class=\"nav-image illustration\" src=\"../images/resources-cheatsheet.png\" width=250//a  | Deep Learning with Keras Cheatsheet br/A quick reference guide to the concepts and available functions in the R interface to Keras. Covers the various types of Keras layers, data preprocessing, training workflow, and pre-trained models.  |\n| a href=\"https://blogs.rstudio.com/tensorflow/gallery.html\"img class=\"nav-image illustration\" src=\"../images/keras-customer-churn.png\" width=250//a  | Gallery br/In-depth examples of using TensorFlow with R, including detailed explanatory narrative as well as coverage of ancillary tasks like data preprocessing and visualization. A great resource for taking the next step after you've learned the basics. |\n| a href=\"examples.html\"img class=\"nav-image illustration\" src=\"../images/resources-examples.png\" width=250//a  | Tutorials br/ Introductory examples of using TensorFlow with R. These examples cover the basics of training models with the keras and tensorflow packages.  |\n\n","id":91},{"path":"/search","date":"2016-09-13T09:00:00+00:00","title":"Search","type":"\"page\"","layout":"\"search\"","content":"\n\n","id":92},{"path":"/tutorials/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"tutorials-get-started\"","    parent":"   parent: \"tutorials-top\"","    weight":"   weight: 10","content":"\nIn this section you will find tutorials that can be used to get started with TensorFlow for R or, for more advanced users, to discover best practices for loading data, building complex models and solving common problems.\n\nThe best place to get started with TensorFlow is using Keras - a Deep Learning API\ncreated by François Chollet and ported to R by JJ Allaire. Keras makes it easy to get started, and it allows you to progressively \nbuild more complex workflows as you need to use advanced models and techniques. \n\nFor beginners\n\nWe recommend the following tutorials for your first contact with TensorFlow. \nFeel free to navigate through the 'beginners' section in the sidebar.\n\nQuickstart: the minimal getting started guide to Keras.\nBasic ML with Keras: use Keras to solve basic Machine Learning tasks.\nLoad data: learn to efficiently load data to TensorFlow using tfdatasets.\n\n For experts\n\nAdvanced Quickstart: learn the subclassing API and how to create custom loops.\nCustomization: build custom layers and training loops in TensorFlow.\nDistributed Training: distribute your model training across multiple GPU's or machines.\n\nWe also provide tutorials focused on different types of data:\n\nImages: Build more advanced models for classification and segmentation of images.\nStructured Data: Build models\nfor structured data.\n","id":93},{"path":"/tutorials/advanced/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Quickstart\"","    identifier":"   identifier: \"tutorials-advanced-quickstart-quickstart\"","    parent":"   parent: \"tutorials-top\"","    weight":"   weight: 50","content":"\nThis is a short introduction to Keras advanced features. It uses:\n\ntfdatasets to manage input data.\nA custom model.\ntfautograph for building a custom training loop.\n\nBefore running the quickstart you need to have Keras installed. Please refer to the installation for installation instructions.\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tfautograph)\nlibrary(reticulate)\nlibrary(purrr)\n\nLet's start by loading and preparing the MNIST dataset. The values of the pixels are integers between 0 and 255, and we will convert them to floats between 0 and 1.\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\ndim(mnist$train$x) <- c(dim(mnist$train$x), 1)\ndim(mnist$test$x) <- c(dim(mnist$test$x), 1)\n\nNow let's use tfdatasets to batch and shuffle the dataset.\n\ntrain_ds - mnist$train %% \n  tensorslicesdataset() %%\n  dataset_take(20000) %% \n  datasetmap(~modifyat(.x, \"x\", tf$cast, dtype = tf$float32)) %% \n  datasetmap(~modifyat(.x, \"y\", tf$cast, dtype = tf$int64)) %% \n  dataset_shuffle(10000) %% \n  dataset_batch(32)\n\ntest_ds - mnist$test %% \n  tensorslicesdataset() %% \n  dataset_take(2000) %% \n  datasetmap(~modifyat(.x, \"x\", tf$cast, dtype = tf$float32)) %%\n  datasetmap(~modifyat(.x, \"y\", tf$cast, dtype = tf$int64)) %% \n  dataset_batch(32)\n\nWe will now define a Keras custom model.\n\nsimpleconvnn <- function(filters, kernel_size)  \n   )\n \n\nmodel <- simpleconvnn(filters = 32, kernel_size = 3)\n\nWe can then choose an optimizer and loss function for training:\n\nloss <- losssparsecategorical_crossentropy\noptimizer <- optimizer_adam()\n\nSelect metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n\ntrainloss <- tf$keras$metrics$Mean(name='trainloss')\ntrainaccuracy <-  tf$keras$metrics$SparseCategoricalAccuracy(name='trainaccuracy')\n\ntestloss <- tf$keras$metrics$Mean(name='testloss')\ntestaccuracy <- tf$keras$metrics$SparseCategoricalAccuracy(name='testaccuracy')\n\nWe then define a function that is able to make one training step:\n\ntrain_step <- function(images, labels)  )\n  \n  gradients <- tape$gradient(l, model$trainable_variables)\n  optimizer$apply_gradients(purrr::transpose(list(\n    gradients, model$trainable_variables\n  )))\n  \n  train_loss(l)\n  train_accuracy(labels, predictions)\n  \n \n\nWe then provide a function that is able to test the model:\n\ntest_step <- function(images, labels)  \n\nWe can then write our training loop function:\n\ntrainingloop <- tffunction(autograph(function(trainds, testds)  \n  \n  for (b2 in test_ds)  \n  \n  tf$print(\"Acc\", trainaccuracy$result(), \"Test Acc\", testaccuracy$result())\n  \n  trainloss$resetstates()\n  trainaccuracy$resetstates()\n  testloss$resetstates()\n  testaccuracy$resetstates()\n  \n ))\n\nFinally let's run our training loop for 5 epochs:\n\nfor (epoch in 1:5)  \n\nEpoch:  1  --------,Acc 0.93095 Test Acc 0.954\n Epoch:  2  --------,Acc 0.956525 Test Acc 0.95825\nEpoch:  3  --------,Acc 0.968066692 Test Acc 0.9575\n Epoch:  4  --------,Acc 0.9752 Test Acc 0.960125\nEpoch:  5  --------,Acc 0.9796 Test Acc 0.9617\n\n","id":94},{"path":"/tutorials/advanced/customization/autodiff","title":"\"Automatic differentiation and gradient tape\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Automatic differentiation\"","    identifier":"   identifier: \"tutorials-advanced-customization-autodiff\"","    parent":"   parent: \"tutorials-advanced-customization-top\"","    weight":"   weight: 70","content":"\nIn this tutorial we will cover automatic differentiation, a key technique for optimizing machine learning models.\n\nSetup\n\nWe will use the TensorFlow R package:\n\nlibrary(tensorflow)\n\n Gradient Tapes\n\nTensorFlow provides the tf$GradientTape API for automatic differentiation - computing the gradient of a computation with respect to its input variables. \n\nTensorflow \"records\" all operations executed inside the context of a tf$GradientTape onto a \"tape\". Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n\nFor example:\n\nx <- tf$ones(shape(2, 2))\n\nwith(tf$GradientTape() %as% t,  )\n\nDerivative of z with respect to the original input tensor x\ndz_dx <- t$gradient(z, x)\ndz_dx\n\nYou can also request gradients of the output with respect to intermediate values computed during a \"recorded\" tf$GradientTape context.\n\nx <- tf$ones(shape(2, 2))\n\nwith(tf$GradientTape() %as% t,  )\n\n Use the tape to compute the derivative of z with respect to the\nintermediate value y.\ndz_dy <- t$gradient(z, y)\ndz_dy\n\nBy default, the resources held by a GradientTape are released as soon as GradientTape$gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:\n\nx <- tf$constant(3)\n\nwith(tf$GradientTape(persistent = TRUE) %as% t,  )\n\n Use the tape to compute the derivative of z with respect to the\nintermediate value y.\ndz_dx <- t$gradient(z, x)  108.0 (4*x^3 at x = 3)\ndz_dx\n\ndy_dx <- t$gradient(y, x) # 6.0\ndy_dx\n\nrm(t)  # Drop the reference to the tape\n\nRecording control flow\n\nBecause tapes record operations as they are executed, R control flow (using ifs and whiles for example) is naturally handled:\n\nf <- function(x, y)  \n  output\n \n\ngrad <- function(x, y)  )\n  t$gradient(out, x)\n \n\nx <- tf$constant(2)\ngrad(x, 6)\ngrad(x, 5)\ngrad(x, 4)\n\n Higher-order gradients\n\nOperations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:\n\nx <- tf$Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n\nwith(tf$GradientTape() %as% t,  )\n  \n  # Compute the gradient inside the 't' context manager\n  # which means the gradient computation is differentiable as well.\n  dy_dx <- t2$gradient(y, x)\n  \n )\n\nd2ydx <- t$gradient(dydx, x)\n\ndy_dx\nd2y_dx\n\n","id":95},{"path":"/tutorials/advanced/customization/custom-layers","title":"\"Custom layers\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Custom layers\"","    identifier":"   identifier: \"tutorials-advanced-customization-layers\"","    parent":"   parent: \"tutorials-advanced-customization-top\"","    weight":"   weight: 65","content":"\nWe recommend using keras as a high-level API for building neural networks. That said, most TensorFlow APIs are usable with eager execution.\n\nlibrary(tensorflow)\nlibrary(keras)\n\nLayers: common sets of useful operations\n\nMost of the time when writing code for machine learning models you want to operate at a higher level of abstraction than individual operations and manipulation of individual variables.\n\nMany machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers.\n\nTensorFlow includes the full Keras API in the keras package, and the Keras layers are very useful when building your own models.\n\n To construct a layer, simply construct the object. Most layers take as \na first argument the number of output dimensions / channels.\nlayer <- layer_dense(units = 100)\n\n The number of input dimensions is often unnecessary, as it can be inferred\nthe first time the layer is used, but it can be provided if you want to\n specify it manually, which is useful in some complex models.\nlayer <- layerdense(units = 10, inputshape = shape(NULL, 5))\n\nThe full list of pre-existing layers can be seen in the documentation. It includes Dense (a fully-connected layer),\nConv2D, LSTM, BatchNormalization, Dropout, and many others.\n\nTo use a layer, simply call it.\nlayer(tf$zeros(shape(10, 5)))\n\n Layers have many useful methods. For example, you can inspect all variables\nin a layer using layer$variables and trainable variables using\n layer$trainable_variables. In this case a fully-connected layer\nwill have variables for weights and biases.\nlayer$variables\n\n The variables are also accessible through nice accessors\nlayer$kernel\nlayer$bias\n\nImplementing custom layers\n\nThe best way to implement your own layer is extending the KerasLayer class and implementing:\n\n  initialize , where you can do all input-independent initialization\n  build, where you know the shapes of the input tensors and can do the rest of the initialization\n  call, where you do the forward computation\n\nNote that you don't have to wait until build is called to create your variables, you can also create them in initialize. However, the advantage of creating them in build is that it enables late variable creation based on the shape of the inputs the layer will operate on. On the other hand, creating variables in initialize would mean that shapes required to create the variables will need to be explicitly specified.\n\nMyDenseLayer <- R6::R6Class(\"CustomLayer\",\n                                  \n  inherit = KerasLayer,\n  \n  public = list(\n    \n    num_outputs = NULL,\n    kernel = NULL,\n    \n    initialize = function(num_outputs)  ,\n    \n    build = function(input_shape)  ,\n    \n    call = function(x, mask = NULL)  \n  \n  )\n)\n\n Layer Wrapper Function\n\nIn order to use the custom layer within a Keras model you also need to create a wrapper function which instantiates the layer using the create_layer() function. For example:\n\ndefine layer wrapper function\nlayermydense <- function(object, num_outputs, name = NULL, trainable = TRUE)  \n\nSome important things to note about the layer wrapper function:\n\nIt accepts object as its first parameter (the object will either be a Keras sequential model or another Keras layer). The object parameter enables the layer to be composed with other layers using the magrittr pipe (%%) operator.\n\nIt converts it’s outputdim to integer using the as.integer() function. This is done as convenience to the user because Keras variables are strongly typed (you can’t pass a float if an integer is expected). This enables users of the function to write outputdim = 32 rather than output_dim = 32L.\n\nSome additional parameters not used by the layer (name and trainable) are in the function signature. Custom layer functions can include any of the core layer function arguments (inputshape, batchinputshape, batchsize, dtype, name, trainable, and weights) and they will be automatically forwarded to the Layer base class.\n\nWe can use the defined layer, for example:\n\nlayer <- layermydense(num_outputs = 10)\nlayer(tf$zeros(shape(10, 5)))\n\nOverall code is easier to read and maintain if it uses standard layers whenever possible, as other readers will be familiar with the behavior of standard layers. If you want to use a layer which is not present in tf.keras.layers, consider filing a github issue or, even better, sending us a pull request!\n\n Models: Composing layers\n\nMany interesting layer-like things in machine learning models are implemented by composing existing layers. For example, each residual block in a resnet is a composition of convolutions, batch normalizations, and a shortcut. Layers can be nested inside other layers.\n\nTypically you use kerasmodelcustom when you need the model methods like: fit,evaluate, and save (see Custom Keras layers and models for details).\n\nOne other feature provided by MOdel (instead of Layer) is that in addition to tracking variables, a Model also tracks its internal layers, making them easier to inspect.\n\nFor examplle here is a ResNet block:\n\nresnetidentityblock <- function(kernel_size, filters)  \n   )\n \n\nblock <- resnetidentityblock(kernel_size = 1, filters = c(1, 2, 3))\nblock(tf$zeros(shape(1, 2, 3, 3)))\n\nblock$layers\n\nlength(block$variables)\n\nMuch of the time, however, models which compose many layers simply call one layer after the other. This can be done in very little code using kerasmodelsequential:\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 1, kernel_size = c(1, 1)) %% \n  layerbatchnormalization() %% \n  layerconv2d(\n    filters = 2, \n    kernel_size = c(1,1), \n    padding = 'same'\n  ) %% \n  layerbatchnormalization() %% \n  layerconv2d(filters = 3, kernel_size = c(1, 1)) %% \n  layerbatchnormalization()\n\ntrigger model building\nmodel(tf$zeros(c(1, 2, 3, 3)))\n\nsummary(model)\n\n","id":96},{"path":"/tutorials/advanced/customization/custom-training","title":"\"Custom training: basics\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Custom training\"","    identifier":"   identifier: \"tutorials-advanced-customization-custom-training\"","    parent":"   parent: \"tutorials-advanced-customization-top\"","    weight":"   weight: 75","content":"\nIn the previous tutorial, you covered the TensorFlow APIs for automatic differentiation—a basic building block for machine learning. In this tutorial, you will use the TensorFlow primitives introduced in the prior tutorials to do some simple machine learning.\n\nTensorFlow also includes Keras —a high-level neural network API that provides useful abstractions to reduce boilerplate and makes TensorFlow easier to use without sacrificing flexibility and performance. We strongly recommend the Keras API for development. However, in this short tutorial you will learn how to train a neural network from first principles to establish a strong foundation.\n\nlibrary(tensorflow)\n\nVariables\n\nTensors in TensorFlow are immutable stateless objects. Machine learning models, however, must have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). \n\nTensorFlow has stateful operations built-in, and these are often easier than using low-level R representations for your state. Use tf$Variable to represent weights in a model.\n\nA tf$Variable object stores a value and implicitly reads from this stored value. There are operations (tf.assignsub, tf.scatterupdate, etc.) that manipulate the value stored in a TensorFlow variable.\n\nv <- tf$Variable(1)\n\n Use Python's assert as a debugging statement to test the condition\nas.numeric(v) == 1\n\nReassign the value v\nv$assign(3)\nas.numeric(v) == 3\n\n Use v in a TensorFlow tf.square() operation and reassign\nv$assign(tf$square(v))\nas.numeric(v) == 9\n\nComputations using tf$Variable are automatically traced when computing gradients. For variables that represent embeddings, TensorFlow will do sparse updates by default, which are more computation and memory efficient.\n\nA tf$Variable is also a way to show a reader of your code that a piece of state is mutable.\n\nFit a linear model\n\nLet's use the concepts you have learned so far—Tensor, Variable, and GradientTape—to build and train a simple model. This typically involves a few steps:\n\nDefine the model.\nDefine a loss function.\nObtain training data.\nRun through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n\nHere, you'll create a simple linear model, f(x) = x * W + b, which has two variables: W (weights) and b (bias). You'll synthesize data such that a well trained model would have W = 3.0 and b = 2.0.\n\n Define the model\n\nYou may organize your TensorFlow code to build models the way you prefer, here\nwe will suggest using an R6 class.\n\nModel <- R6::R6Class(\n  classname = \"Model\",\n  public = list(\n    W = NULL,\n    b = NULL,\n    \n    initialize = function()  ,\n    \n    call = function(x)  \n    \n  )\n)\n\nmodel <- Model$new()\nmodel$call(3)\n\nDefine the loss function\n\nA loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let's use the standard L2 loss, also known as the least square errors:\n\nloss <- function(ypred, ytrue)  \n\n Obtain training data\n\nFirst, synthesize the training data by adding random Gaussian (Normal) noise to the inputs:\n\nTRUE_W = 3.0\nTRUE_b = 2.0\nNUM_EXAMPLES = 1000\n\ninputs  <- tf$random$normal(shape=shape(NUM_EXAMPLES))\nnoise   <- tf$random$normal(shape=shape(NUM_EXAMPLES))\noutputs <- inputs * TRUEW + TRUEb + noise\n\nBefore training the model, visualize the loss value by plotting the model's predictions in red and the training data in blue:\n\nlibrary(tidyverse)\ntibble(\n  inputs = as.numeric(inputs), \n  outputs = as.numeric(outputs),\n  predicted = as.numeric(model$call(inputs))\n) %% \n  ggplot(aes(x = inputs)) +\n  geom_point(aes(y = outputs)) +\n  geom_line(aes(y = predicted), color = \"blue\")\n\nDefine a training loop\n\nWith the network and training data, train the model using gradient descent to update the weights variable (W) and the bias variable (b) to reduce the loss. \n\nThere are many variants of the gradient descent scheme that are captured in tf$train$Optimizer—our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of tf.GradientTape for automatic differentiation and tf.assign_sub for decrementing a value (which combines tf.assign and tf.sub):\n\ntrain <- function(model, inputs, outputs, learning_rate)  )\n  \n  d <- t$gradient(current_loss, list(model$W, model$b))\n  \n  model$W$assignsub(learningrate * d[[1]])\n  model$b$assignsub(learningrate * d[[2]])\n  current_loss\n \n\nFinally, let's repeatedly run through the training data and see how W and b evolve.\n\nmodel <- Model$new()\n\nWs <- bs <- c()\n\nfor (epoch in seq_len(20))  , Loss:  \"), \"\\n\")\n  \n\ntibble(\n  epoch = 1:20,\n  Ws = Ws,\n  bs = bs\n) %% \n  pivot_longer(\n    c(Ws, bs),\n    names_to = \"parameter\", \n    values_to = \"estimate\"\n  ) %% \n  ggplot(aes(x = epoch, y = estimate)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\")\n\nThis tutorial used tf$Variable to build and train a simple linear model.\n\nIn practice, the high-level APIs—such as Keras—are much more convenient to build neural networks. Keras provides higher level building blocks (called \"layers\"), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies, and more.\n\n","id":97},{"path":"/tutorials/advanced/customization/tensors-operations","title":"\"Tensors and operations\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Tensors and operations\"","    identifier":"   identifier: \"tutorials-advanced-customization-tensors\"","    parent":"   parent: \"tutorials-advanced-customization-top\"","    weight":"   weight: 60","content":"\nThis is an introductory TensorFlow tutorial shows how to:\n\nImport the required package\nCreate and use tensors\nUse GPU acceleration\n\nImport TensorFlow\n\nTo get started, import the tensorflow module. As of TensorFlow 2.0, eager execution is turned on by default. This enables a more interactive frontend to TensorFlow, the details of which we will discuss much later.\n\nlibrary(tensorflow)\n\n Tensors\n\nA Tensor is a multi-dimensional array. Similar to array objects in R, tf$Tensor objects have a data type and a shape. Additionally, tf$Tensors can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (tf$add, tf$matmul, tf$linalg$inv etc.) that consume and produce tf.Tensors. These operations automatically convert native R types, for example:\n\ntf$add(1, 2)\ntf$add(c(1, 2), c(3, 4))\ntf$square(5)\ntf$reduce_sum(c(1, 2, 3))\n\nOperator overloading is also supported\ntf$square(2) + tf$square(3)\n\nEach tf$Tensor has a shape and a datatype:\n\nx = tf$matmul(matrix(1,ncol = 1), matrix(c(2, 3), nrow = 1))\nx\nx$shape\nx$dtype\n\nThe most obvious differences between arrays and tf$Tensors are:\n\nTensors can be backed by accelerator memory (like GPU, TPU).\nTensors are immutable.\n\n R arrays compatibility\n\nConverting between a TensorFlow tf.Tensors and an array is easy:\n\nTensorFlow operations automatically convert R arrays to Tensors.\n\nTensors are explicitly converted to R arrays using the as.array, as.matrix or as.numeric methods. There's always a memory copy when converting from a Tensor to an array in R. \n\nTensorFlow operations convert arrays to Tensors automatically\n1 + tf$ones(shape = 1)\n\n The as.array method explicitly converts a Tensor to an array\nas.array(tf$ones(shape = 1))\n\nGPU acceleration\n\nMany TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operation—copying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed, for example:\n\nx <- tf$random$uniform(shape(3, 3))\n\n List devices\ntf$config$experimental$listphysicaldevices()\n\nWhat device is x placed\nx$device\n\n Device Names\n\nThe Tensor$device property provides a fully qualified string name of the device hosting the contents of the tensor. This name encodes many details, such as an identifier of the network address of the host on which this program is executing and the device within that host. This is required for distributed execution of a TensorFlow program. The string ends with GPU:N if the tensor is placed on the N-th GPU on the host.\n\nExplicit Device Placement\n\nIn TensorFlow, placement refers to how individual operations are assigned (placed on) a device for execution. As mentioned, when there is no explicit guidance provided, TensorFlow automatically decides which device to execute an operation and copies tensors to that device, if needed. However, TensorFlow operations can be explicitly placed on specific devices using the tf$device context manager, for example:\n\nprint(\"On CPU:0:\")\nwith(tf$device(\"CPU:0\"),  )\n\nprint(\"On GPU:0:\")\nwith(tf$device(\"GPU:0\"),  )\n\n","id":98},{"path":"/tutorials/advanced/distributed/distributed_training_with_keras","title":"\"Distributed training with Keras\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Distributed training with Keras}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Distributed training with Keras\"","    identifier":"   identifier: \"distributed-training-with-keras\"","    parent":"   parent: \"tutorials-advanced-distributed-top\"","    weight":"   weight: 20","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\n\nOverview\n\nThe tf$distribute$Strategy API provides an abstraction for distributing your training across multiple processing units. The goal is to allow users to enable distributed training using existing models and training code, with minimal changes.\n\nThis tutorial uses the tf$distribute$MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine. Essentially, it copies all of the model's variables to each processor. Then, it uses all-reduce to combine the gradients from all processors and applies the combined value to all copies of the model.\n\nMirroredStategy is one of several distribution strategy available in TensorFlow core. You can read about more strategies in the distribution strategy guide.\n\n Keras API\n\nThis example uses the keras API to build the model and training loop. For custom training loops, see the Custom training loops tutorial.\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\nused to load the MNIST dataset\nlibrary(tfds)\n\nlibrary(purrr)\nlibrary(glue)\n\n Download the dataset\n\nDownload the MNIST dataset and load it using tfds. This returns a dataset in tfdatasets format.\n\nif you haven't done it yet:\n tfds::install_tfds()\nmnist <- tfds_load(\"mnist\")\ninfo <- summary(mnist)\n\nDefine distribution strategy\n\nCreate a MirroredStrategy object. This will handle distribution, and provides a context manager (tf$distribute$MirroredStrategy$scope) to build your model inside.\n\nstrategy <- tf$distribute$MirroredStrategy()\nstrategy$numreplicasin_sync\n\n Setup input pipeline\n\nWhen training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly.\n\nnumtrainexamples <- as.integer(info$splits[[2]]$statistics$numExamples)\nnumtestexamples <- as.integer(info$splits[[1]]$statistics$numExamples)\n\nBUFFER_SIZE <- 10000\n\nBATCHSIZEPER_REPLICA <- 64\nBATCHSIZE <- BATCHSIZEPERREPLICA * strategy$numreplicasin_sync\n\nPixel values, which are 0-255, have to be normalized to the 0-1 range. Furthermore, we shuffle and batch the train and test datasets. Notice we are also keeping an in-memory cache of the training data to improve performance.\n\ntrain_dataset - mnist$train %% \n  dataset_map(function(record)  ) %%\n  dataset_cache() %%\n  datasetshuffle(BUFFERSIZE) %% \n  datasetbatch(BATCHSIZE) %% \n  dataset_map(unname)\n\ntest_dataset - mnist$test %% \n dataset_map(function(record)  ) %%\n  datasetbatch(BATCHSIZE) %% \n  dataset_map(unname)\n\nCreate the model\n\nCreate and compile the Keras model in the context of strategy$scope.\n\nwith (strategy$scope(),  )\n \n\n Define the callbacks\n\nThe callbacks used here are:\n\nTensorBoard: This callback writes a log for TensorBoard which allows you to visualize the graphs.\nModel Checkpoint: This callback saves the model after every epoch.\nLearning Rate Scheduler: Using this callback, you can schedule the learning rate to change after every epoch/batch.\n\nFor illustrative purposes, add a print callback to display the learning rate.\n\nDefine the checkpoint directory to store the checkpoints\ncheckpointdir <- './trainingcheckpoints'\n Name of the checkpoint files\ncheckpointprefix <- file.path(checkpointdir, \"ckpt_ \")\n\nFunction for decaying the learning rate.\n You can define any decay function you need.\ndecay <- function(epoch, lr)  \n\nCallback for printing the LR at the end of each epoch.\nPrintLR <- R6::R6Class(\"PrintLR\",\n  inherit = KerasCallback,\n  \n  public = list(\n    \n    losses = NULL,\n     \n    onepochend = function(epoch, logs = list())   is  \\n'))\n     \n))\n\nprint_lr <- PrintLR$new()\n\ncallbacks <- list(\n    callbacktensorboard(logdir = '/tmp/logs'),\n    callbackmodelcheckpoint(filepath = checkpointprefix, saveweights_only = TRUE),\n    callbacklearningrate_scheduler(decay),\n    print_lr\n)\n\n Train and evaluate\n\nNow, train the model in the usual way, calling fit on the model and passing in the dataset created at the beginning of the tutorial. This step is the same whether you are distributing the training or not.\n\nmodel %% fit(train_dataset, epochs = 12, callbacks = callbacks)\n\nAs you can see below, the checkpoints are getting saved.\n\nlist.files(checkpoint_dir)\n\nTo see how the model performs, load the latest checkpoint and call evaluate on the test data.\n\nmodel %% loadmodelweightstf(tf$train$latestcheckpoint(checkpoint_dir))\n\nmodel %% evaluate(test_dataset)\n\ntensorboard(log_dir = \"/tmp/logs\")\n\nExport to SavedModel\n\nExport the graph and the variables to the platform-agnostic SavedModel format. After your model is saved, you can load it with or without the scope.\n\npath <- 'saved_model/'\nmodel %% savemodeltf(path)\n\nLoad the model without strategy$scope.\n\nunreplicatedmodel <- loadmodel_tf(path)\n\nunreplicated_model %% compile(\n    loss = 'sparsecategoricalcrossentropy',\n    optimizer = 'adam',\n    metrics = 'accuracy')\n\nunreplicatedmodel %% evaluate(testdataset)\n\nLoad the model with strategy$scope.\n\nwith (strategy$scope(),  )\n\n Examples and Tutorials\n\nHere are some examples for using distribution strategy with keras fit/compile: \n\nTransformer example trained using tf.distribute.MirroredStrategy\nNCF example trained using tf.distribute.MirroredStrategy\n\nCall evaluate as before using appropriate datasets.","id":99},{"path":"/tutorials/advanced/images/cnn","title":"\"Convolutional Neural Network (CNN)\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Convolutional Neural Network\"","    identifier":"   identifier: \"tutorials-advanced-images-cnn\"","    parent":"   parent: \"tutorials-advanced-images-top\"","    weight":"   weight: 10","content":"\nThis tutorial demonstrates training a simple Convolutional Neural Network (CNN) \nto classify CIFAR images. Because this tutorial uses the Keras Sequential API, creating and training our model will take just a few lines of code.\n\nSetup\n\nlibrary(tensorflow)\nlibrary(keras)\n\n Download and prepare the CIFAR10 dataset\n\nThe CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.\n\ncifar <- dataset_cifar10()\n\nVerify the data\n\nTo verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image.\n\nclass_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck')\n\nindex <- 1:30\n\npar(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))\ncifar$train$x[index,,,] %% \n  purrr::array_tree(1) %%\n  purrr::setnames(classnames[cifar$train$y[index] + 1]) %% \n  purrr::map(as.raster, max = 255) %%\n  purrr::iwalk(~ )\n\n Create the convolutional base\n\nThe 6 lines of code below define the convolutional base using a common pattern: a stack of Conv2D and MaxPooling2D layers.\n\nAs input, a CNN takes tensors of shape (imageheight, imagewidth, colorchannels), ignoring the batch size. If you are new to these dimensions, colorchannels refers to (R,G,B). In this example, you will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument input_shape to our first layer.\n\nmodel - kerasmodelsequential() %% \n  layerconv2d(filters = 32, kernel_size = c(3,3), activation = \"relu\", \n                input_shape = c(32,32,3)) %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 64, kernel_size = c(3,3), activation = \"relu\") %% \n  layermaxpooling2d(poolsize = c(2,2)) %% \n  layerconv2d(filters = 64, kernel_size = c(3,3), activation = \"relu\")\n\nLet's display the architecture of our model so far.\n\nsummary(model)\n\nAbove, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically, as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer.\n\nAdd Dense layers on top\n\nTo complete our model, you will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D, then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs and a softmax activation.\n\nmodel %% \n  layer_flatten() %% \n  layer_dense(units = 64, activation = \"relu\") %% \n  layer_dense(units = 10, activation = \"softmax\")\nHere's the complete architecture of our model.\n\n Note Keras models are mutable objects and you don't need to re-assign model\nin the chubnk above.\n\nsummary(model)\n\nAs you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers.\n\n Compile and train the model\n\nmodel %% compile(\n  optimizer = \"adam\",\n  loss = \"sparsecategoricalcrossentropy\",\n  metrics = \"accuracy\"\n)\n\nhistory - model %% \n  fit(\n    x = cifar$train$x, y = cifar$train$y,\n    epochs = 10,\n    validation_data = unname(cifar$test),\n    verbose = 2\n  )\n\nEvaluate the model\n\nplot(history)\n\nevaluate(model, cifar$test$x, cifar$test$y, verbose = 0)\n\nOur simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! \n\n","id":100},{"path":"/tutorials/advanced/images/transfer-learning-hub","title":"\"Transfer learning with TensorFlow Hub\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Transfer Learning with tfhub\"","    identifier":"   identifier: \"tutorials-advanced-images-transfer-learning-hub\"","    parent":"   parent: \"tutorials-advanced-images-top\"","    weight":"   weight: 20","content":"\nTensorFlow Hub is a way to share pretrained model components. See the TensorFlow Module Hub for a searchable listing of pre-trained models. This tutorial demonstrates:\n\nHow to use TensorFlow Hub Keras.\nHow to do image classification using TensorFlow Hub.\nHow to do simple transfer learning.\n\nSetup\n\nlibrary(keras)\nlibrary(tfhub)\n\n An ImageNet classifier\n\nDownload the classifier\n\nUse layer_hub to load a mobilenet and wrap it up as a keras layer. Any TensorFlow 2 compatible image classifier URL from tfhub.dev will work here.\n\nclassifierurl =\"https://tfhub.dev/google/tf2-preview/mobilenetv2/classification/2\"\n\nimage_shape <- c(224L, 224L, 3L)\n\nclassifier <- layerhub(handle = classifierurl, inputshape = imageshape)\n\n Run it on a single image\n\nDownload a single image to try the model on.\n\nimageurl <- \"https://storage.googleapis.com/download.tensorflow.org/exampleimages/grace_hopper.jpg\"\n\nimg - pins::pin(imageurl, name = \"gracehopper\") %%\n  tensorflow::tf$io$read_file() %% \n  tensorflow::tf$image$decode_image(dtype = tf$float32) %% \n  tensorflow::tf$image$resize(size = image_shape[-3])\n\nimg %% \n  as.array() %% \n  as.raster() %% \n  plot()\n\nAdd a batch dimension, and pass the image to the model.\n\nresult - img %% \n  tf$expand_dims(0L) %% \n  classifier()\n\nThe result is a 1001 element vector of logits, rating the probability of each class for the image.\n\nSo the top class ID can be found with argmax:\n\npredicted_class - tf$argmax(result, axis = 1L) %% as.integer()\npredicted_class\n\nDecode the predictions\n\nWe have the predicted class ID, Fetch the ImageNet labels, and decode the predictions:\n\nlabels_url <- \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n\nimagenetlabels - pins::pin(labelsurl, \"imagenet_labels\") %% \n  readLines()\n\nimg %% \n  as.array() %% \n  as.raster() %% \n  plot()\n \ntitle(paste(\"Prediction:\" , imagenetlabels[predictedclass + 1]))\n\nSimple transfer learning\n\nUsing TF Hub it is simple to retrain the top layer of the model to recognize the classes in our dataset.\n\nflowers <- pins::pin(\"https://storage.googleapis.com/download.tensorflow.org/exampleimages/flowerphotos.tgz\", \"flower_photos\")\n\nThe simplest way to load this data into our model is using imagedatagenerator.\n\nAll of TensorFlow Hub's image modules expect float inputs in the [0, 1] range. Use the imagedatagenerator rescale parameter to achieve this.\n\nThe image size will be handled later.\n\nimagegenerator <- imagedata_generator(rescale=1/255)\nimage_data - flowers[1] %% \n  dirname() %% \n  dirname() %% \n  flowimagesfromdirectory(imagegenerator, targetsize = imageshape[-3])\n\nThe resulting object is an iterator that returns imagebatch, labelbatch pairs.\nWe can iterate over it using the iter_next from reticulate:\n\nstr(reticulate::iternext(imagedata))\n\n Run the classifier on a batch of images\n\nNow run the classifier on the image batch.\n\nimagebatch <- reticulate::iternext(image_data)\npredictions <- classifier(tf$constant(image_batch[[1]], tf$float32))\npredictedclassnames <- imagenetlabels[as.integer(tf$argmax(predictions, axis = 1L) + 1L)]\n\npar(mfcol = c(4,8), mar = rep(1, 4), oma = rep(0.2, 4))\nimage_batch[[1]] %% \n  purrr::array_tree(1) %%\n  purrr::setnames(predictedclassnames) %% \n  purrr::map(as.raster) %%\n  purrr::iwalk(~ )\n\nSee the LICENSE.txt file for image attributions.\n\nThe results are far from perfect, but reasonable considering that these are not the classes the model was trained for (except \"daisy\").\n\nDownload the headless model\n\nTensorFlow Hub also distributes models without the top classification layer. These can be used to easily do transfer learning.\n\nAny Tensorflow 2 compatible image feature vector URL from tfhub.dev will work here.\n\nfeatureextractorurl <- \"https://tfhub.dev/google/tf2-preview/mobilenetv2/featurevector/2\"\n\nCreate the feature extractor.\n\nfeatureextractorlayer <- layerhub(handle = featureextractor_url, \n                                     inputshape = imageshape)\n\nIt returns a 1280-length vector for each image:\n\nfeaturebatch <- featureextractorlayer(tf$constant(imagebatch[[1]], tf$float32))\nfeature_batch\n\nFreeze the variables in the feature extractor layer, so that the training only modifies the new classifier layer.\n\nfreezeweights(featureextractor_layer)\n\n Attach a classification head\n\nNow let's create a sequential model using the feature extraction layer and add a new classification layer.\n\nmodel <- kerasmodelsequential(list(\n  featureextractorlayer,\n  layerdense(units = imagedata$num_classes, activation='softmax')\n))\n\nsummary(model)\n\nTrain the model\n\nUse compile to configure the training process:\n\nmodel %% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = \"accuracy\"\n)\n\nNow use the fit method to train the model.\n\nTo keep this example short train just 2 epochs. \n\nhistory - model %% fit_generator(\n  image_data, epochs=2, \n  stepsperepoch = imagedata$n / imagedata$batch_size,\n  verbose = 2\n)\n\nNow after, even just a few training iterations, we can already see that the model is making progress on the task.\n\nWe can then verify the predictions:\n\nimagebatch <- reticulate::iternext(image_data)\npredictions <- predictclasses(model, imagebatch[[1]])\n\npar(mfcol = c(4,8), mar = rep(1, 4), oma = rep(0.2, 4))\nimage_batch[[1]] %% \n  purrr::array_tree(1) %%\n  purrr::setnames(names(imagedata$class_indices)[predictions + 1]) %% \n  purrr::map(as.raster) %%\n  purrr::iwalk(~ )\n\n Export your model\n\nNow that you've trained the model, export it as a saved model:\n\nsavemodeltf(model, \"mymodel/\", include_optimizer = FALSE)\n\nNow confirm that we can reload it, and it still gives the same results:\n\nmodel_ <- loadmodeltf(\"mymodel/\")\n\nx <- tf$constant(image_batch[[1]], tf$float32)\nall.equal(\n  as.matrix(model(x)),\n  as.matrix(model_(x))\n)\n\nunlink(\"mymodel/\")\n\n","id":101},{"path":"/tutorials/advanced/structured/classify","title":"\"Classify structured data with feature columns\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Classify structured data\"","    identifier":"   identifier: \"tutorials-advanced-structured-classify\"","    parent":"   parent: \"tutorials-advanced-structured-top\"","    weight":"   weight: 20","editor_options":"","  chunk_output_type":" chunk_output_type: console","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nThis tutorial demonstrates how to classify structured data (e.g. tabular data in a CSV). We will use Keras to define the model, and feature columns as a bridge to map from columns in a CSV to features used to train the model. This tutorial contains complete code to:\n\nLoad a CSV file using the tidyverse.\nBuild an input pipeline to batch and shuffle the rows using tf.data.\nMap from columns in the CSV to features used to train the model using feature columns.\nBuild, train, and evaluate a model using Keras.\n\nThe Dataset\n\nWe will use a small dataset provided by the Cleveland Clinic Foundation for Heart Disease. There are several hundred rows in the CSV. Each row describes a patient, and each column describes an attribute. We will use this information to predict whether a patient has heart disease, which in this dataset is a binary classification task.\n\nFollowing is a description of this dataset. Notice there are both numeric and categorical columns.\n\nColumn| Description| Feature Type | Data Type\n------------|--------------------|----------------------|--------------,Age | Age in years | Numerical | integer\nSex | (1 = male; 0 = female) | Categorical | integer\nCP | Chest pain type (0, 1, 2, 3, 4) | Categorical | integer\nTrestbpd | Resting blood pressure (in mm Hg on admission to the hospital) | Numerical | integer\nChol | Serum cholestoral in mg/dl | Numerical | integer\nFBS | (fasting blood sugar  120 mg/dl) (1 = true; 0 = false) | Categorical | integer\nRestECG | Resting electrocardiographic results (0, 1, 2) | Categorical | integer\nThalach | Maximum heart rate achieved | Numerical | integer\nExang | Exercise induced angina (1 = yes; 0 = no) | Categorical | integer\nOldpeak | ST depression induced by exercise relative to rest | Numerical | integer\nSlope | The slope of the peak exercise ST segment | Numerical | float\nCA | Number of major vessels (0-3) colored by flourosopy | Numerical | integer\nThal | 3 = normal; 6 = fixed defect; 7 = reversable defect | Categorical | string\nTarget | Diagnosis of heart disease (1 = true; 0 = false) | Classification | integer\n\n Setup\n\nWe will use Keras and TensorFlow datasets.\n\nlibrary(keras)\nlibrary(tfdatasets)\nlibrary(tidyverse)\nlibrary(rsample)\n\nRead the data\n\nWe will use read_csv in order to read the csv file to R. \n\nheart <- pins::pin(\"https://storage.googleapis.com/applied-dl/heart.csv\", \"heart\")\ndf <- read_csv(heart)\nglimpse(df)\n\n Split the dataframe into train, validation, and test\n\nWe are going to use the rsample package to split the data into train, validation\nand test sets.\n\nfirst we split between training and testing sets\nsplit <- initial_split(df, prop = 4/5)\ntrain <- training(split)\ntest <- testing(split)\n\n the we split the training set into validation and training\nsplit <- initial_split(train, prop = 4/5)\ntrain <- training(split)\nval <- testing(split)\n\nnrow(train)\nnrow(val)\nnrow(test)\n\nCreate an input pipeline using tfdatasets\n\nNext, we will wrap the dataframes with tfdatasets. This will enable us to use feature columns as a bridge to map from the columns in the dataframe to features used to train the model. If we were working with a very large CSV file (so large that it does not fit into memory), we would use tfdatasets to read it from disk directly. That is not covered in this tutorial.\n\ndftodataset <- function(df, shuffle = TRUE, batch_size = 32)  \n\nbatch_size <- 5\ntrainds <- dftodataset(train, batchsize = batch_size)\nvalds <- dftodataset(val, shuffle = FALSE, batchsize = batch_size)\ntestds <- dftodataset(test, shuffle = FALSE, batchsize = batch_size)\n\n Understand the input pipeline\n\nNow that we have created the input pipeline, let's call it to see the format of the data it returns. We have used a small batch size to keep the output readable.\n\ntrain_ds %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  str()\n\nWe can see that the dataset returns a list of column names (from the dataframe) that map to column values from rows in the dataframe.\n\nCreate the feature spec\n\nWe want to train a model to predict the target variable using Keras but, before\nthat we need to prepare the data. We need to transform the categorical variables\ninto some form of dense variable, we usually want to normalize all numeric columns too.\n\nThe feature spec interface works with data.frames or TensorFlow datasets objects.\n\nLet's start creating our feature specification:\n\nspec <- featurespec(trainds, target ~ .)\n\nThe first thing we need to do after creating the feature_spec is decide on the variables' types.\n\nWe can do this by adding steps to the spec object.\n\nspec - spec %% \n  stepnumericcolumn(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizerfn = scalerstandard()\n  ) %% \n  stepcategoricalcolumnwithvocabulary_list(thal)\n\nThe following steps can be used to define the variable type:\n\nstepnumericcolumn to define numeric variables\nstepcategoricalwithvocabularylist for categorical variables with a fixed vocabulary\nstepcategoricalcolumnwithhash_bucket for categorical variables using the hash trick\nstepcategoricalcolumnwithidentity to store categorical variables as integers\nstepcategoricalcolumnwithvocabulary_file when you have the possible vocabulary in a file\n\nWhen using stepcategoricalcolumnwithvocabulary_list you can also provide a vocabulary argument\nwith the fixed vocabulary. The recipe will find all the unique values in the dataset and use it\nas the vocabulary.\n\nYou can also specify a normalizerfn to the stepnumeric_column. In this case the variable will be\ntransformed by the feature column. Note that the transformation will occur in the TensorFlow Graph,\nso it must use only TensorFlow ops. Like in the example we offer pre-made normalizers - and they will\ncompute the normalizing function during the recipe preparation.\n\nYou can also use selectors like:\n\nstartswith(), endswith(), matches() etc. (from tidyselect)\nall_numeric() to select all numeric variables\nall_nominal() to select all strings\nhas_type(\"float32\") to select based on TensorFlow variable type.\n\nNow we can print the recipe:\n\nspec\n\nAfter specifying the types of the columns you can add transformation steps. \nFor example you may want to bucketize a numeric column:\n\nspec - spec %% \n  stepbucketizedcolumn(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65))\n\nYou can also specify the kind of numeric representation that you want to use for\nyour categorical variables.\n\nspec - spec %% \n  stepindicatorcolumn(thal) %% \n  stepembeddingcolumn(thal, dimension = 2)\n\nAnother common transformation is to add interactions between variables using crossed\ncolumns. \n\nspec - spec %% \n  stepcrossedcolumn(thalandage = c(thal, bucketizedage), hashbucket_size = 1000) %% \n  stepindicatorcolumn(thalandage)\n\nNote that the crossed_column is a categorical column, so we need to also specify what\nkind of numeric tranformation we want to use. Also note that we can name the transformed\nvariables - each step uses a default naming for columns, eg. bucketized_age is the\ndefault name when you use stepbucketizedcolumn with column called age.\n\nWith the above code we have created our recipe. Note we can also define the\nrecipe by chaining a sequence of methods:\n\nspec - featurespec(trainds, target ~ .) %% \n  stepnumericcolumn(\n    all_numeric(), -cp, -restecg, -exang, -sex, -fbs,\n    normalizerfn = scalerstandard()\n  ) %% \n  stepcategoricalcolumnwithvocabulary_list(thal) %% \n  stepbucketizedcolumn(age, boundaries = c(18, 25, 30, 35, 40, 45, 50, 55, 60, 65)) %% \n  stepindicatorcolumn(thal) %% \n  stepembeddingcolumn(thal, dimension = 2) %% \n  stepcrossedcolumn(c(thal, bucketizedage), hashbucket_size = 10) %%\n  stepindicatorcolumn(crossedthalbucketized_age)\n\nAfter defining the recipe we need to fit it. It's when fitting that we compute the vocabulary\nlist for categorical variables or find the mean and standard deviation for the normalizing functions.\nFitting involves evaluating the full dataset, so if you have provided the vocabulary list and \nyour columns are already normalized you can skip the fitting step (TODO).\n\nIn our case, we will fit the feature spec, since we didn't specify the vocabulary list\nfor the categorical variables.\n\nspec_prep <- fit(spec)\n\nAfter preparing we can see the list of dense features that were defined:\n\nstr(specprep$densefeatures())\n\n Build the model\n\nNow we are ready to define our model in Keras. We will use a specialized layerdensefeatures that\nknows what to do with the feature columns specification.\n\nWe also use a new layerinputfrom_dataset that is useful to create a Keras input object copying the structure from a data.frame or TensorFlow dataset.\n\nmodel - kerasmodelsequential() %% \n  layerdensefeatures(densefeatures(specprep)) %% \n  layer_dense(units = 32, activation = \"relu\") %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %% compile(\n  loss = lossbinarycrossentropy, \n  optimizer = \"adam\", \n  metrics = \"binary_accuracy\"\n)\n\nTrain the model\n\nWe can finally train the model on the dataset:\n\nhistory - model %% \n  fit(\n    datasetusespec(trainds, spec = specprep),\n    epochs = 15, \n    validationdata = datasetusespec(valds, spec_prep),\n    verbose = 2\n  )\nplot(history)\n\nFinally we can make predictions in the test set and calculate performance \nmetrics like the AUC of the ROC curve:\n\npred <- predict(model, test)\nMetrics::auc(test$target, pred)\n\n","id":102},{"path":"/tutorials/beginners/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Quickstart\"","    identifier":"   identifier: \"tutorials-beginners-quickstart-quickstart\"","    parent":"   parent: \"tutorials-top\"","    weight":"   weight: 23","content":"\nThis short introduction uses Keras to:\n\nBuild a neural network that classifies images.\nTrain this neural network.\nAnd, finally, evaluate the accuracy of the model.\nSave and restore the created model.\n\nBefore running the quickstart you need to have Keras installed. Please refer to the installation section for installation instructions.\n\nlibrary(keras)\n\nLet's start by loading and preparing the MNIST dataset. The values of the pixels are integers between 0 and 255, and we will convert them to floats between 0 and 1.\n\nmnist <- dataset_mnist()\nmnist$train$x <- mnist$train$x/255\nmnist$test$x <- mnist$test$x/255\n\nNow, let's define the a Keras model using the sequential API.\n\nmodel - kerasmodelsequential() %% \n  layerflatten(inputshape = c(28, 28)) %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dropout(0.2) %% \n  layer_dense(10, activation = \"softmax\")\n\nNote that when using the Sequential API the first layer must specify the input_shape argument which represents the dimensions of the input. In our case, the images are 28x28.\n\nAfter defining the model, you can see information about layers, number of parameters, etc. with the summary function:\n\nsummary(model)\n\nThe next step after building the model is to compile it. It's at compile time that we define what loss will be optimized and what optimizer will be used. You can also specify metrics, callbacks etc. that are meant to be run during the model fitting.\n\nCompiling is done with the compile function:\n\nmodel %% \n  compile(\n    loss = \"sparsecategoricalcrossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nNote that compile and fit (which we are going to see next) modify the model object in place, unlike most R functions. \n\nNow let's fit our model:\n\nmodel %% \n  fit(\n    x = mnist$train$x, y = mnist$train$y,\n    epochs = 5,\n    validation_split = 0.3,\n    verbose = 2\n  )\n\nWe can now make predictions with our model using the predict function:\n\npredictions <- predict(model, mnist$test$x)\nhead(predictions, 2)\n\nBy default predict will return the output of the last Keras layer.\nIn our case this is the probability for each class. You can also use predictclasses and predictproba to generate class and  probability outputs - these functions are slightly different from predict, since they will be run in batches.\n\nYou can assess model performance on a different dataset using the evaluate function, for example:\n\nmodel %% \n  evaluate(mnist$test$x, mnist$test$y, verbose = 0)\n\nOur model achieved ~90% accuracy on the test set. \n\nUnlike models built with the lm function, to save Keras models for later prediction, you need to use specialized functions, like savemodeltf:\n\nsavemodeltf(object = model, filepath = \"model\")\n\nYou can then reload the model and use it to make predictions:\n\nreloadedmodel <- loadmodel_tf(\"model\")\nall.equal(predict(model, mnist$test$x), predict(reloaded_model, mnist$test$x))\n\nMore information about saving and serializing models, as well as about different model types, is available in the guides.\n\nsystem(\"rm -r model\")\n\n","id":103},{"path":"/tutorials/beginners/basic-ml/_","title":"\"Overview\"","type":"docs","menu":"menu:","  main":" main:","    name":"   name: \"Overview\"","    identifier":"   identifier: \"tutorials-beginners-basic-ml-overview\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 10","content":"\nThis session includes tutorials about basic concepts of Machine Learning using \nKeras.\n\nImage Classification: image classification using the Fashing MNIST dataset.\nRegression: regression using the Boston Housing dataset.\nText Classification: text classification using the IMDB dataset.\nOverfitting and Underfitting: learn about these inportant concepts in ML.\nSave and Restore: learn how to save and restore TensorFlow models.\n","id":104},{"path":"/tutorials/beginners/basic-ml/tutorial_basic_classification","title":"\"Basic Image Classification\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Basic Classification}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Image Classification\"","    identifier":"   identifier: \"keras-tutorial-basic-classification\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 20","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nIn this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts. It's fine if you don't understand all the details, this is a fast-paced overview of a complete Keras program with the details explained as we go.\n\nlibrary(keras)\n\nImport the Fashion MNIST dataset\n\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n\nThis guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashionmnist <- datasetfashion_mnist()\n\nc(trainimages, trainlabels) %<-% fashion_mnist$train\nc(testimages, testlabels) %<-% fashion_mnist$test\n\nAt this point we have four arrays: The trainimages and trainlabels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the testimages, and testlabels arrays.\n\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\nDigit  | Class\n-------| ----------,0 | T-shirt/top\n1 | Trouser\n2 | Pullover\n3 | Dress\n4 | Coat\n5 | Sandal\n6 | Shirt\n7 | Sneaker\n8 | Bag\n9 | Ankle boot\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we'll store them in a vector to use later when plotting the images.\n\nclass_names = c('T-shirt/top',\n                'Trouser',\n                'Pullover',\n                'Dress',\n                'Coat', \n                'Sandal',\n                'Shirt',\n                'Sneaker',\n                'Bag',\n                'Ankle boot')\n\n Explore the data\n\nLet's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\n\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\n\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n[1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\n\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\n\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000\n\nPreprocess the data\n\nThe data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nimage1 <- as.data.frame(trainimages[1, , ])\ncolnames(image1) <- seqlen(ncol(image_1))\nimage1$y <- seqlen(nrow(image_1))\nimage1 <- gather(image1, \"x\", \"value\", -y)\nimage1$x <- as.integer(image1$x)\n\nggplot(image_1, aes(x = x, y = y, fill = value)) +\n  geom_tile() +\n  scalefillgradient(low = \"white\", high = \"black\", na.value = NA) +\n  scale_y_reverse() +\n  theme_minimal() +\n  theme(panel.grid = element_blank())   +\n  theme(aspect.ratio = 1) +\n  xlab(\"\") +\n  ylab(\"\")\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255. \n\nIt's important that the training set and the testing set are preprocessed in the same way:\n\ntrainimages <- trainimages / 255\ntestimages <- testimages / 255\n\nDisplay the first 25 images from the training set and display the class name below each image. \nVerify that the data is in the correct format and we're ready to build and train the network.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25)  \n\n Build the model\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\n\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense, have parameters that are learned during training.\n\nmodel <- kerasmodelsequential()\nmodel %%\n  layerflatten(inputshape = c(28, 28)) %%\n  layer_dense(units = 128, activation = 'relu') %%\n  layer_dense(units = 10, activation = 'softmax')\n\nThe first layer in this network, layer_flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two dense layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n Compile the model\n\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n\n Loss function — This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n Optimizer — This is how the model is updated based on the data it sees and its loss function.\n  Metrics —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\nmodel %% compile(\n  optimizer = 'adam', \n  loss = 'sparsecategoricalcrossentropy',\n  metrics = c('accuracy')\n)\n\nTrain the model\n\nTraining the neural network model requires the following steps:\n\n Feed the training data to the model — in this example, the trainimages and trainlabels arrays.\n The model learns to associate images and labels.\n We ask the model to make predictions about a test set — in this example, the testimages array. We verify that the predictions match the labels from the testlabels array.\n\nTo start training, call the fit method — the model is \"fit\" to the training data:\n\nmodel %% fit(trainimages, trainlabels, epochs = 5, verbose = 2)\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data.\n\n Evaluate accuracy\n\nNext, compare how the model performs on the test dataset:\n\nscore - model %% evaluate(testimages, testlabels, verbose = 0)\n\ncat('Test loss:', score$loss, \"\\n\")\ncat('Test accuracy:', score$acc, \"\\n\")\n\n10000/10000 [==============================] - 0s 19us/step\nTest loss: 0.3755946 \nTest accuracy: 0.8644 \n\nIt turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data.\n\nMake predictions\n\nWith the model trained, we can use it to make predictions about some images.\n\npredictions - model %% predict(test_images)\n\nHere, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:\n\npredictions[1, ]\n\nA prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\nAlternatively, we can also directly get the class prediction:\n\nclasspred - model %% predictclasses(test_images)\nclass_pred[1:20]\n\nAs the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot.\nAnd we can check the test label to see this is correct:\n\ntest_labels[1]\n\nLet's plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.\n\npar(mfcol=c(5,5))\npar(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\nfor (i in 1:25)   else  \n  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n        main = paste0(classnames[predictedlabel + 1], \" (\",\n                      classnames[truelabel + 1], \")\"),\n        col.main = color)\n \n\nFinally, use the trained model to make a prediction about a single image.\n\nGrab an image from the test dataset\n take care to keep the batch dimension, as this is expected by the model\nimg <- test_images[1, , , drop = FALSE]\ndim(img)\n\nNow predict the image:\n\npredictions - model %% predict(img)\npredictions\n\npredict returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:\n\nsubtract 1 as labels are 0-based\nprediction <- predictions[1, ] - 1\nwhich.max(prediction)\n\nOr, directly getting the class prediction again:\n\nclasspred - model %% predictclasses(img)\nclass_pred\n\nAnd, as before, the model predicts a label of 9.\n","id":105},{"path":"/tutorials/beginners/basic-ml/tutorial_basic_regression","title":"\"Basic Regression\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Basic Regression}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Regression\"","    identifier":"   identifier: \"keras-tutorial-basic-regression\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 25","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nIn a regression problem, we aim to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where we aim to predict a discrete label (for example, where a picture contains an apple or an orange).\n\nThis notebook builds a model to predict the median price of homes in a Boston suburb during the mid-1970s. To do this, we'll provide the model with some data points about the suburb, such as the crime rate and the local property tax rate.\n\nlibrary(keras)\nlibrary(tfdatasets)\n\nThe Boston Housing Prices dataset\n\nThe Boston Housing Prices dataset is accessible directly from keras.\n\nbostonhousing <- datasetboston_housing()\n\nc(traindata, trainlabels) %<-% boston_housing$train\nc(testdata, testlabels) %<-% boston_housing$test\n\n Examples and features\n\nThis dataset is much smaller than the others we've worked with so far: it has 506 total examples that are split between 404 training examples and 102 test examples:\n\npaste0(\"Training entries: \", length(traindata), \", labels: \", length(trainlabels))\n\nThe dataset contains 13 different features:\n\n Per capita crime rate.\n The proportion of residential land zoned for lots over 25,000 square feet.\n The proportion of non-retail business acres per town.\n Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n Nitric oxides concentration (parts per 10 million).\n The average number of rooms per dwelling.\n The proportion of owner-occupied units built before 1940.\n Weighted distances to five Boston employment centers.\n Index of accessibility to radial highways.\n Full-value property-tax rate per $10,000.\n Pupil-teacher ratio by town.\n 1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n Percentage lower status of the population.\n\nEach one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. \n\ntrain_data[1, ] # Display sample features, notice the different scales\n\nLet's add column names for better data inspection.\n\nlibrary(dplyr)\n\ncolumn_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')\n\ntraindf - traindata %% \n  astibble(.namerepair = \"minimal\") %% \n  setNames(column_names) %% \n  mutate(label = train_labels)\n\ntestdf - testdata %% \n  astibble(.namerepair = \"minimal\") %% \n  setNames(column_names) %% \n  mutate(label = test_labels)\n\nLabels\n\nThe labels are the house prices in thousands of dollars. (You may notice the mid-1970s prices.)\n\ntrain_labels[1:10]  Display first 10 entries\n\nNormalize features\n\nIt's recommended to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model more dependent on the choice of units used in the input.\n\nWe are going to use the featurespec interface implemented in the tfdatasets package for normalization. The featurecolumns interface allows for other common pre-processing operations on tabular data.\n\nspec - featurespec(traindf, label ~ . ) %% \n  stepnumericcolumn(allnumeric(), normalizerfn = scaler_standard()) %% \n  fit()\n\nspec\n\nThe spec created with tfdatasets can be used together with layerdensefeatures to perform  pre-processing directly in the TensorFlow graph.\n\nWe can take a look at the output of a dense-features layer created by this spec:\n\nlayer <- layerdensefeatures(\n  featurecolumns = densefeatures(spec), \n  dtype = tf$float32\n)\nlayer(train_df)\n\nNote that this returns a matrix (in the sense that it's a 2-dimensional Tensor) with\nscaled values.\n\n Create the model\n\nLet's build our model. Here we will use the Keras functional API - which is the recommended way when using the featurespec API. Note that we only need to pass the densefeatures from the spec we just created.\n\ninput - layerinputfromdataset(traindf %% select(-label))\n\noutput - input %% \n  layerdensefeatures(dense_features(spec)) %% \n  layer_dense(units = 64, activation = \"relu\") %%\n  layer_dense(units = 64, activation = \"relu\") %%\n  layer_dense(units = 1) \n\nmodel <- keras_model(input, output)\n\nsummary(model)\n\nWe then compile the model with:\n\nmodel %% \n  compile(\n    loss = \"mse\",\n    optimizer = optimizer_rmsprop(),\n    metrics = list(\"meanabsoluteerror\")\n  )\n\nWe will wrap the model building code into a function in order to be able to reuse it for different experiments. Remember that Keras fit modifies the model in-place.\n\nbuild_model <- function()  \n\nTrain the model\n\nThe model is trained for 500 epochs, recording training and validation accuracy in a kerastraininghistory object.\nWe also show how to use a custom callback, replacing the default training output by a single dot per epoch.\n\n Display training progress by printing a single dot for each completed epoch.\nprintdotcallback <- callback_lambda(\n  onepochend = function(epoch, logs)  \n)    \n\nmodel <- build_model()\n\nhistory - model %% fit(\n  x = train_df %% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(printdotcallback)\n)\n\nNow, we visualize the model's training progress using the metrics stored in the history variable. We want to use this data to determine how long to train before the model stops making progress.\n\nlibrary(ggplot2)\nplot(history)\n\nThis graph shows little improvement in the model after about 200 epochs. Let's update the fit method to automatically stop training when the validation score doesn't improve. We'll use a callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, it automatically stops the training.\n\nThe patience parameter is the amount of epochs to check for improvement.\nearlystop <- callbackearlystopping(monitor = \"valloss\", patience = 20)\n\nmodel <- build_model()\n\nhistory - model %% fit(\n  x = train_df %% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(early_stop)\n)\n\nplot(history)\n\nThe graph shows the average error is about $2,500 dollars. Is this good? Well, $2,500 is not an insignificant amount when some of the labels are only $15,000.\n\nLet's see how did the model performs on the test set:\n\nc(loss, mae) %-% (model %% evaluate(testdf %% select(-label), testdf$label, verbose = 0))\n\npaste0(\"Mean absolute error on test set: $\", sprintf(\"%.2f\", mae * 1000))\n\n Predict\n\nFinally, predict some housing prices using data in the testing set:\n\ntestpredictions - model %% predict(testdf %% select(-label))\ntest_predictions[ , 1]\n\nConclusion\n\nThis notebook introduced a few techniques to handle a regression problem.\n\n Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems).\n Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\nWhen input data features have values with different ranges, each feature should be scaled independently.\nIf there is not much training data, prefer a small network with few hidden layers to avoid overfitting.\nEarly stopping is a useful technique to prevent overfitting.\n","id":106},{"path":"/tutorials/beginners/basic-ml/tutorial_basic_text_classification","title":"\"Text Classification\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Text Classification}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Text Classification\"","    identifier":"   identifier: \"keras-tutorial-text-classification\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 30","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\n Note: This tutorial requires TensorFlow version = 2.1\n\nThis tutorial classifies movie reviews as positive or negative using the text of the review. This is an example of binary — or two-class — classification, an important and widely applicable kind of machine learning problem.\n\nWe'll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n\nLet's start and load Keras, as well as a few other required libraries.\n\nlibrary(keras)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\nDownload the Movie Reviews dataset\n\nWe will use the Movie Reviews dataset created by Bo Pang and Lillian Lee. This dataset is \nredistributed with NLTK with permission from the authors.\n\nThe dataset can be found here\nand can be downloaded from the Kaggle UI or using the pins\npackage. \n\nIf you are going to use pins follow this tutorial to register\nthe Kaggle board. Then you can run:\n\npaths <- pins::pin_get(\"nltkdata/movie-review\", \"kaggle\")\n we only need the movie_review.csv file\npath <- paths[1]\n\nNow let's read it to R using the read_csv funcntion from the readr package.\n\ndf <- readr::read_csv(path)\nhead(df)\n\nExplore the data\n\nLet's take a moment to understand the format of the data. The dataset has 60k rows, each\none representing a movie review. The text column has the actual review and the tag\nrepresents shows us the classified sentiment for the review.\n\ndf %% count(tag)\n\nAround half of the reviews are negative and the other half are positive.\nHere is an example of a review:\n\ndf$text[1]\n\nLet's also split our dataset into training and testing:\n\ntraining_id <- sample.int(nrow(df), size = nrow(df)*0.8)\ntraining <- df[training_id,]\ntesting <- df[-training_id,]\n\nIt's also useful to find out what is the distribution of the number of words\nin each review.\n\ndf$text %% \n  strsplit(\" \") %% \n  sapply(length) %% \n  summary()\n\n Prepare the data\n\nThe reviews — the text — must be converted to tensors before fed into the neural network. \nFirst, we create a dictionary and represent each of the 10,000 most common words by an integer. In this case, every review will be represented by a sequence of integers.\n\nThen we can represent reviews in a couple of ways:\n\n One-hot-encode the arrays to convert them into vectors of 0s and 1s. For example, the sequence [3, 5] would become a 10,000-dimensional vector that is all zeros except for indices 3 and 5, which are ones. Then, make this the first layer in our network — a dense layer — that can handle floating point vector data. This approach is memory intensive, though, requiring a numwords * numreviews size matrix.\n\n Alternatively, we can pad the arrays so they all have the same length, then create an integer tensor of shape numexamples * maxlength. We can use an embedding layer capable of handling this shape as the first layer in our network.\n\nIn this tutorial, we will use the second approach.\nNow, let's define our Text Vectorization layer, it will be responsible to take\nthe string input and convert it to a Tensor. \n\nnum_words <- 10000\nmax_length <- 50\ntextvectorization <- layertext_vectorization(\n  maxtokens = numwords, \n  outputsequencelength = max_length, \n)\n\nNow, we need to adapt the Text Vectorization layer. It's when we call adapt\nthat the layer will learn about unique words in our dataset and assign an integer\nvalue for each one.\n\ntext_vectorization %% \n  adapt(df$text)\n\nWe can now see the vocabulary is in our text vectorization layer.\n\nTODO see https://github.com/tensorflow/tensorflow/pull/34529\ngetvocabulary(textvectorization)\n\nYou can see how the text vectorization layer transforms it's inputs:\n\ntext_vectorization(matrix(df$text[1], ncol = 1))\n\n Build the model\n\nThe neural network is created by stacking layers — this requires two main architectural decisions:\n\nHow many layers to use in the model?\nHow many hidden units to use for each layer?\n\nIn this example, the input data consists of an array of word-indices. The labels to predict are either 0 or 1. Let's build a model for this problem:\n\ninput <- layer_input(shape = c(1), dtype = \"string\")\n\noutput - input %% \n  text_vectorization() %% \n  layerembedding(inputdim = numwords + 1, outputdim = 16) %%\n  layerglobalaveragepooling1d() %%\n  layer_dense(units = 16, activation = \"relu\") %%\n  layer_dropout(0.5) %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel <- keras_model(input, output)\n\nThe layers are stacked sequentially to build the classifier:\n\n The first layer is an embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n Next, a globalaveragepooling_1d layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n This fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\n The last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n\nHidden units\n\nThe above model has two intermediate or \"hidden\" layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n\nIf a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns — patterns that improve performance on training data but not on the test data. This is called overfitting, and we'll explore it later.\n\n Loss function and optimizer\n\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the binary_crossentropy loss function.\n\nThis isn't the only choice for a loss function, you could, for instance, choose meansquarederror. But, generally, binary_crossentropy is better for dealing with probabilities — it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n\nLater, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %% compile(\n  optimizer = 'adam',\n  loss = 'binary_crossentropy',\n  metrics = list('accuracy')\n)\n\nTrain the model\n\nTrain the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the xtrain and ytrain tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:\n\nhistory - model %% fit(\n  training$text,\n  as.numeric(training$tag == \"pos\"),\n  epochs = 10,\n  batch_size = 512,\n  validation_split = 0.2,\n  verbose=2\n)\n\n Evaluate the model\n\nAnd let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nresults - model %% evaluate(testing$text, as.numeric(testing$tag == \"pos\"), verbose = 0)\nresults\n\nThis fairly naive approach achieves an accuracy of about 68%. With more advanced approaches, the model should get closer to 85%.\n\nCreate a graph of accuracy and loss over time\n\nfit returns a kerastraininghistory object whose metrics slot contains loss and metrics values recorded during training.\nYou can conveniently plot the loss and metrics curves like so:\n\nplot(history)\n\nThe evolution of loss and metrics can also be seen during training in the RStudio Viewer pane.\n\nNotice the training loss decreases with each epoch and the training accuracy increases with each epoch. This is expected when using gradient descent optimization — it should minimize the desired quantity on every iteration.\n\n","id":107},{"path":"/tutorials/beginners/basic-ml/tutorial_basic_text_classification_with_tfhub","title":"\"Transfer learning with tfhub\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Text Classification}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Transfer learning with tfhub\"","    identifier":"   identifier: \"keras-tutorial-text-classification-tfhub\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 35","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nThis tutorial classifies movie reviews as positive or negative using the text of the review. This is an example of binary — or two-class — classification, an important and widely applicable kind of machine learning problem.\n\nWe'll use the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews.\n\nWe will use Keras to build and train the model\nand tfhub for Transfer Learning. We will also use tfds to load the IMDB dataset.\n\nLet's start and load the required libraries.\n\nlibrary(keras)\nlibrary(tfhub)\nlibrary(tfds)\nlibrary(tfdatasets)\n\nDownload the IMDB dataset\n\nThe IMDB dataset is available on imdb reviews or on tfds. The one that comes packaged with Keras is already pre-processed so it's not useful for this tutorial.\n\nThe following code downloads the IMDB dataset to your machine:\n\nimdb <- tfds_load(\n  \"imdb_reviews:1.0.0\", \n  split = list(\"train[:60%]\", \"train[-40%:]\", \"test\"), \n  as_supervised = TRUE\n)\nsummary(imdb)\n\ntfds_load returns a TensorFlow Dataset, an abstraction that represents a sequence of elements, in which each element consists of one or more components. \n\nTo access individual elements of a dataset you can use:\n\nfirst - imdb[[1]] %% \n  dataset_batch(1) %%  Used to get only the first example\n  reticulate::as_iterator() %% \n  reticulate::iter_next()\nstr(first)\n\nWe will see next that Keras knows how to extract elements from TensorFlow Datasets\nautomatically, making it a much more memory efficient alternative than loading the entire dataset into RAM before passing it to Keras.\n\nBuild the model\n\nThe neural network is created by stacking layers — this requires three main architectural decisions:\n\nHow to represent the text?\nHow many layers to use in the model?\nHow many hidden units to use for each layer?\n\nIn this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n\nOne way to represent the text is to convert sentences into embedding vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages: * we don't have to worry about text preprocessing, * we can benefit from transfer learning, * the embedding has a fixed size, so it's simpler to process.\n\nFor this example we will use a pre-trained text embedding model from TensorFlow Hub called google/tf2-preview/gnews-swivel-20dim/1.\n\nThere are three other pre-trained models to test for the sake of this tutorial:\n\ngoogle/tf2-preview/gnews-swivel-20dim-with-oov/1 - same as google/tf2-preview/gnews-swivel-20dim/1, but with 2.5% vocabulary converted to OOV buckets. This can help if the vocabulary of the task and the vocabulary of the model don't fully overlap.\ngoogle/tf2-preview/nnlm-en-dim50/1 - A much larger model with ~1M vocabulary size and 50 dimensions.\ngoogle/tf2-preview/nnlm-en-dim128/1 - Even larger model with ~1M vocabulary size and 128 dimensions.\n\nLet's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: (numexamples, embeddingdimension).\n\nembeddinglayer <- layerhub(handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\nembedding_layer(first[[1]])\n\nLet's now build the full model:\n\nmodel - kerasmodelsequential() %% \n  layer_hub(\n    handle = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\",\n    input_shape = list(),\n    dtype = tf$string,\n    trainable = TRUE\n  ) %% \n  layer_dense(units = 16, activation = \"relu\") %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsummary(model)\n\nThe layers are stacked sequentially to build the classifier:\n\nThe first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using (google/tf2-preview/gnews-swivel-20dim/1) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: (numexamples, embeddingdimension).\nThis fixed-length output vector is piped through a fully-connected (dense) layer with 16 hidden units.\nThe last layer is densely connected with a single output node. Using the sigmoid activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n\nLet's compile the model.\n\n Loss function and optimizer\n\nA model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the binary_crossentropy loss function.\n\nThis isn't the only choice for a loss function, you could, for instance, choose meansquarederror. But, generally, binary_crossentropy is better for dealing with probabilities — it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n\nNow, configure the model to use an optimizer and a loss function:\n\nmodel %% \n  compile(\n    optimizer = \"adam\",\n    loss = \"binary_crossentropy\",\n    metrics = \"accuracy\"\n  )\n\nTrain the model\n\nTrain the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the dataset. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:\n\nmodel %% \n  fit(\n    imdb[[1]] %% datasetshuffle(10000) %% datasetbatch(512),\n    epochs = 20,\n    validationdata = imdb[[2]] %% datasetbatch(512),\n    verbose = 2\n  )\n\n Evaluate the model\n\nAnd let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.\n\nmodel %% \n  evaluate(imdb[[3]] %% dataset_batch(512), verbose = 0)\n\nThis fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%.","id":108},{"path":"/tutorials/beginners/basic-ml/tutorial_overfit_underfit","title":"\"Tutorial: Overfitting and Underfitting\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Overfitting and Underfitting}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Overfitting and Underfitting\"","    identifier":"   identifier: \"keras-tutorial-overfitting-and-underfitting\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 50","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nIn two of the previous tutorails — classifying movie reviews, and predicting housing prices — we saw that the accuracy of our model on the validation data would peak after training for a number of epochs, and would then start decreasing.\n\nIn other words, our model would overfit to the training data. Learning how to deal with overfitting is important. Although it's often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to testing data (or data they haven't seen before).\n\nThe opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\n\nIf you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we'll explore below is a useful skill.\n\nTo prevent overfitting, the best solution is to use more training data. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n\nIn this tutorial, we'll explore two common regularization techniques — weight regularization and dropout — and use them to improve our IMDB movie review classification results.\n\nlibrary(keras)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\n\nDownload the IMDB dataset\n\nnum_words <- 1000\nimdb <- datasetimdb(numwords = num_words)\n\nc(traindata, trainlabels) %<-% imdb$train\nc(testdata, testlabels) %<-% imdb$test\n\nRather than using an embedding as in the previous notebook, here we will multi-hot encode the sentences. This model will quickly overfit to the training set. It will be used to demonstrate when overfitting occurs, and how to fight it.\n\nMulti-hot-encoding our lists means turning them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence [3, 5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones.\n\nmultihotsequences <- function(sequences, dimension)  \n  multi_hot\n \n\ntraindata <- multihotsequences(traindata, num_words)\ntestdata <- multihotsequences(testdata, num_words)\n\nLet's look at one of the resulting multi-hot vectors. The word indices are sorted by frequency, so it is expected that there are more 1-values near index zero, as we can see in this plot:\n\nfirsttext <- data.frame(word = 1:numwords, value = train_data[1, ])\nggplot(first_text, aes(x = word, y = value)) +\n  geom_line() +\n  theme(axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\n Demonstrate overfitting\n\nThe simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\". Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\n\nAlways keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\n\nOn the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between \"too much capacity\" and \"not enough capacity\".\n\nUnfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or what the right size for each layer). You will have to experiment using a series of different architectures.\n\nTo find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss. Let's try this on our movie review classification network.\n\nWe'll create a simple model using only dense layers, then well a smaller version, and compare them.\n\nCreate a baseline model\n\nbaseline_model <- \n  kerasmodelsequential() %%\n  layerdense(units = 16, activation = \"relu\", inputshape = num_words) %%\n  layer_dense(units = 16, activation = \"relu\") %%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nbaseline_model %% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(baseline_model)\n\nbaselinehistory - baselinemodel %% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validationdata = list(testdata, test_labels),\n  verbose = 2\n)\n\n Create a smaller model\n\nLet's create a model with less hidden units to compare against the baseline model that we just created:\n\nsmaller_model <- \n  kerasmodelsequential() %%\n  layerdense(units = 4, activation = \"relu\", inputshape = num_words) %%\n  layer_dense(units = 4, activation = \"relu\") %%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nsmaller_model %% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(smaller_model)\n\nAnd train the model using the same data:\n\nsmallerhistory - smallermodel %% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validationdata = list(testdata, test_labels),\n  verbose = 2\n)\n\nCreate a bigger model\n\nNext, let's add to this benchmark a network that has much more capacity, far more than the problem would warrant:\n\nbigger_model <- \n  kerasmodelsequential() %%\n  layerdense(units = 512, activation = \"relu\", inputshape = num_words) %%\n  layer_dense(units = 512, activation = \"relu\") %%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nbigger_model %% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nsummary(bigger_model)\n\nAnd, again, train the model using the same data:\n\nbiggerhistory - biggermodel %% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validationdata = list(testdata, test_labels),\n  verbose = 2\n)\n\n Plot the training and validation loss\n\nNow, let's plot the loss curves for the 3 models.\nThe smaller network begins overfitting a litle later than the baseline model and its performance degrades much more slowly once it starts overfitting. Notice that the larger network begins overfitting almost right away, after just one epoch, and overfits much more severely. The more capacity the network has, the quicker it will be able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).\n\ncompare_cx <- data.frame(\n  baselinetrain = baselinehistory$metrics$loss,\n  baselineval = baselinehistory$metrics$val_loss,\n  smallertrain = smallerhistory$metrics$loss,\n  smallerval = smallerhistory$metrics$val_loss,\n  biggertrain = biggerhistory$metrics$loss,\n  biggerval = biggerhistory$metrics$val_loss\n) %%\n  rownamestocolumn() %%\n  mutate(rowname = as.integer(rowname)) %%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")\n\nStrategies\n\n Add weight regularization\n\nYou may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n\nA \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to only take on small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n\n L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n\n L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n\nIn Keras, weight regularization is added by passing weight regularizer instances to layers. Let's add L2 weight regularization to the baseline model now.\n\nl2_model <- \n  kerasmodelsequential() %%\n  layerdense(units = 16, activation = \"relu\", inputshape = num_words,\n              kernelregularizer = regularizerl2(l = 0.001)) %%\n  layer_dense(units = 16, activation = \"relu\",\n              kernelregularizer = regularizerl2(l = 0.001)) %%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\nl2_model %% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\nl2history - l2model %% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validationdata = list(testdata, test_labels),\n  verbose = 2\n)\n\nl2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weightcoefficientvalue to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time.\n\nHere's the impact of our L2 regularization penalty:\n\ncompare_cx <- data.frame(\n  baselinetrain = baselinehistory$metrics$loss,\n  baselineval = baselinehistory$metrics$val_loss,\n  l2train = l2history$metrics$loss,\n  l2val = l2history$metrics$val_loss\n) %%\n  rownamestocolumn() %%\n  mutate(rowname = as.integer(rowname)) %%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")\n\nAs you can see, the L2 regularized model has become much more resistant to overfitting than the baseline model, even though both models have the same number of parameters.\n\nAdd dropout\n\nDropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\n\nIn Keras you can introduce dropout in a network via layer_dropout, which gets applied to the output of the layer right before.\n\nLet's add two dropout layers in our IMDB network to see how well they do at reducing overfitting:\n\ndropout_model <- \n  kerasmodelsequential() %%\n  layerdense(units = 16, activation = \"relu\", inputshape = num_words) %%\n  layer_dropout(0.6) %%\n  layer_dense(units = 16, activation = \"relu\") %%\n  layer_dropout(0.6) %%\n  layer_dense(units = 1, activation = \"sigmoid\")\n\ndropout_model %% compile(\n  optimizer = \"adam\",\n  loss = \"binary_crossentropy\",\n  metrics = list(\"accuracy\")\n)\n\ndropouthistory - dropoutmodel %% fit(\n  train_data,\n  train_labels,\n  epochs = 20,\n  batch_size = 512,\n  validationdata = list(testdata, test_labels),\n  verbose = 2\n)\n\nHow well did it work?\n\ncompare_cx <- data.frame(\n  baselinetrain = baselinehistory$metrics$loss,\n  baselineval = baselinehistory$metrics$val_loss,\n  dropouttrain = dropouthistory$metrics$loss,\n  dropoutval = dropouthistory$metrics$val_loss\n) %%\n  rownamestocolumn() %%\n  mutate(rowname = as.integer(rowname)) %%\n  gather(key = \"type\", value = \"value\", -rowname)\n  \nggplot(compare_cx, aes(x = rowname, y = value, color = type)) +\n  geom_line() +\n  xlab(\"epoch\") +\n  ylab(\"loss\")\n\nAdding dropout is a clear improvement over the baseline model.\n\nTo recap: here the most common ways to prevent overfitting in neural networks:\n\n Get more training data.\n Reduce the capacity of the network.\n Add weight regularization.\n Add dropout.\n\nAnd two important approaches not covered in this guide are data augmentation and batch normalization.\n\n","id":109},{"path":"/tutorials/beginners/basic-ml/tutorial_save_and_restore","title":"\"Tutorial: Save and Restore Models\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Save and Restore Models}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Save and Restore Models\"","    identifier":"   identifier: \"keras-tutorial-save-and-restore\"","    parent":"   parent: \"tutorials-beginners-basic-ml-top\"","    weight":"   weight: 60","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\nModel progress can be saved after as well as during training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\n\ncode to create the model, and\nthe trained weights, or parameters, for the model\n\nSharing this data helps others understand how the model works and try it themselves with new data.\n\nOptions\n\nThere are many different ways to save TensorFlow models—depending on the API you're using. This guide uses Keras, a high-level API to build and train models in TensorFlow. For other approaches, see the TensorFlow Save and Restore guide or Saving in eager.\n\n Setup\n\nWe'll use the MNIST dataset to train our model to demonstrate saving weights. To speed up these demonstration runs, only use the first 1000 examples:\n\nlibrary(keras)\n\nmnist <- dataset_mnist()\n\nc(trainimages, trainlabels) %<-% mnist$train\nc(testimages, testlabels) %<-% mnist$test\n\ntrainlabels <- trainlabels[1:1000]\ntestlabels <- testlabels[1:1000]\n\ntrainimages - trainimages[1:1000, , ] %%\n  array_reshape(c(1000, 28 * 28))\ntrainimages <- trainimages / 255\n\ntestimages - testimages[1:1000, , ] %%\n  array_reshape(c(1000, 28 * 28))\ntestimages <- testimages / 255\n\nDefine a model\n\nLet's build a simple model we'll use to demonstrate saving and loading weights.\n\n Returns a short sequential model\ncreate_model <- function()  \n\nmodel <- create_model()\nsummary(model)\n\nSave the entire model\n\nCall savemodel to save the a model's architecture, weights, and training configuration in a single file/folder. This allows you to export a model so it can be used without access to the original code. Since the optimizer-state is recovered, you can resume training from exactly where you left off.\n\nSaving a fully-functional model is very useful—you can load them in TensorFlow.js (HDF5, Saved Model) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (HDF5, Saved Model)\n\n*Custom objects (e.g. subclassed models or layers) require special attention when saving and loading. See the \"Saving custom objects\" section below.\n\n SavedModel format\n\nThe SavedModel format is a way to serialize models. Models saved in this format can be restored using loadmodeltf and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to serve/inspect the SavedModel. The section below illustrates the steps to saving and restoring the model.\n\nmodel <- create_model()\n\nmodel %% fit(trainimages, trainlabels, epochs = 5, verbose = 2)\n\nmodel %% savemodeltf(\"model\")\n\nThe SavedModel format is a directory containing a protobuf binary and a Tensorflow checkpoint. Inspect the saved model directory:\n\nlist.files(\"model\")\n\nReload a fresh Keras model from the saved model:\n\nnewmodel <- loadmodel_tf(\"model\")\nsummary(new_model)\n\nHDF5 format\n\nKeras provides a basic saving format using the HDF5 standard.\n\nmodel <- create_model()\n\nmodel %% fit(trainimages, trainlabels, epochs = 5, verbose = 2)\n\nmodel %% savemodelhdf5(\"my_model.h5\")\n\nNow recreate the model from that file:\n\nnewmodel <- loadmodelhdf5(\"mymodel.h5\")\nsummary(new_model)\n\nThis technique saves everything:\n\nThe weight values\nThe model's configuration(architecture)\nThe optimizer configuration\n\nKeras saves models by inspecting the architecture. Currently, it is not able to save TensorFlow optimizers (from tf$train). When using those you will need to re-compile the model after loading, and you will lose the state of the optimizer.\n\n Saving custom objects\n\nIf you are using the SavedModel format, you can skip this section. The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph. \n\nThus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the orginal code.\n\nTo save custom objects to HDF5, you must do the following:\n\nDefine a getconfig method in your object, and optionally a fromconfig classmethod.\n    get_config() returns a JSON-serializable dictionary of parameters needed to recreate the object.\n    fromconfig(config) uses the returned config from getconfig to create a new object. By default, this function will use the config as initialization arguments.\nPass the object to the customobjects argument when loading the model. The argument must be a named list mapping the string class name to the class definition. E.g. loadkerasmodelhdf5(path, custom_objects=list(\"CustomLayer\" =  CustomLayer))\n\nSee the Writing layers and models from scratch tutorial for examples of customobjects and getconfig.\n\nSave checkpoints during training\n\nIt is useful to automatically save checkpoints during and at the end of training. This way you can use a trained model without having to retrain it, or pick-up training where you left of, in case the training process was interrupted.\n\ncallbackmodelcheckpoint is a callback that performs this task. \n\nThe callback takes a couple of arguments to configure checkpointing. By default, saveweightsonly is set to false, which means the complete model is being saved - including architecture and configuration. You can then restore the model as outlined in the previous paragraph.\n\nNow here, let's focus on just saving and restoring weights. In the following code snippet, we are setting saveweightsonly to true, so we will need the model definition on restore.\n\n Checkpoint callback usage\n\nTrain the model and pass it the callbackmodelcheckpoint:\n\ncheckpoint_path <- \"checkpoints/cp.ckpt\"\n\nCreate checkpoint callback\ncpcallback <- callbackmodel_checkpoint(\n  filepath = checkpoint_path,\n  saveweightsonly = TRUE,\n  verbose = 0\n)\n\nmodel <- create_model()\n\nmodel %% fit(\n  train_images,\n  train_labels,\n  epochs = 10, \n  validationdata = list(testimages, test_labels),\n  callbacks = list(cp_callback),   pass callback to training\n  verbose = 2\n)\n\nInspect the files that were created:\n\nlist.files(dirname(checkpoint_path))\n\nCreate a new, untrained model. When restoring a model from only weights, you must have a model with the same architecture as the original model. Since it's the same model architecture, we can share weights despite that it's a different instance of the model.\n\nNow rebuild a fresh, untrained model, and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):\n\nfreshmodel <- createmodel()\nfreshmodel %% evaluate(testimages, test_labels, verbose = 0)\n\nThen load the weights from the latest checkpoint (epoch 10), and re-evaluate:\n\nfreshmodel %% loadmodelweightstf(filepath = checkpoint_path)\nfreshmodel %% evaluate(testimages, test_labels, verbose = 0)\n\nCheckpoint callback options\n\nAlternatively, you can decide to save only the best model, where best by default is defined as validation loss.\nSee the documentation for callbackmodelcheckpoint for further information.\n\ncheckpoint_path <- \"checkpoints/cp.ckpt\"\n\n Create checkpoint callback\ncpcallback <- callbackmodel_checkpoint(\n  filepath = checkpoint_path,\n  saveweightsonly = TRUE,\n  savebestonly = TRUE,\n  verbose = 1\n)\n\nmodel <- create_model()\n\nmodel %% fit(\n  train_images,\n  train_labels,\n  epochs = 10, \n  validationdata = list(testimages, test_labels),\n  callbacks = list(cp_callback), # pass callback to training,\n  verbose = 2\n)\n\nlist.files(dirname(checkpoint_path))\n\nunlink(dirname(checkpoint_path), recursive = TRUE)\n\nWhat are these files?\n\nThe above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain: \n\nOne or more shards that contain your model's weights. \nAn index file that indicates which weights are stored in a which shard.\n\nIf you are only training a model on a single machine, you'll have one shard with the suffix: .data-00000-of-00001\n\nunlink(\"model\", recursive = TRUE)\nfile.remove(\"my_model.h5\")\n\n Manually save the weights\n\nYou saw how to load the weights into a model. Manually saving them is just as simple with the savemodelweights_tf function.\n\nSave the weights\nmodel %% savemodelweights_tf(\"checkpoints/cp.ckpt\")\n\n Create a new model instance\nnewmodel <- createmodel()\n\nRestore the weights\nnewmodel %% loadmodelweightstf('checkpoints/cp.ckpt')\n\n Evaluate the model\nnewmodel %% evaluate(testimages, test_labels, verbose = 0)\n\nunlink(\"checkpoints\", recursive = TRUE)\n\n","id":110},{"path":"/tutorials/beginners/load/load_csv","title":"\"Loading CSV data\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Basic Classification}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Load CSV data\"","    identifier":"   identifier: \"keras-tutorial-basic-load-csv\"","    parent":"   parent: \"tutorials-beginners-load-top\"","    weight":"   weight: 10","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\n Note: this is the R version of this tutorial in the TensorFlow official webiste.\n\nThis tutorial provides an example of how to load CSV data from a file into \na TensorFlow Dataset using tfdatasets.\n\nThe data used in this tutorial are taken from the Titanic passenger list. The\nmodel will predict the likelihood a passenger survived based on characteristics \nlike age, gender, ticket class, and wether the person was traveling alone.\n\nSetup\n\nlibrary(keras)\nlibrary(tfdatasets)\n\nTRAINDATAURL <- \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\nTESTDATAURL <- \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n\ntrainfilepath <- getfile(\"traincsv\", TRAINDATAURL)\ntestfilepath <- getfile(\"eval.csv\", TESTDATA_URL)\n\nYou coud load this using read.csv, and pass the arrays to TensorFlow. If you need \nto scale up to a large set of files, or need a loader that integrates with TensorFlow and tfdatasets then use the makecsvdataset function:\n\nNow read the CSV data from the file and create a dataset.\n\ntraindataset <- makecsv_dataset(\n  trainfilepath, \n  field_delim = \",\",\n  batch_size = 5, \n  num_epochs = 1\n)\n\ntestdataset <- traindataset <- makecsvdataset(\n  testfilepath, \n  field_delim = \",\",\n  batch_size = 5, \n  num_epochs = 1\n)\n\nWe can see an element of the dataset with:\n\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  reticulate::pytor()\n\nYou can see that makecsvdataset creates a list of Tensors each representing a column. This resembles a lot like R's data.frame, the most significative difference\nis that a TensorFlow dataset is an iterator - meaning that each time you call iter_next it will yield a different batch of rows from the dataset.\n\nAs you can see above, the columns in the CSV are named. The dataset constructor will pick these names up automatically. If the file you are working with does not contain the column names in the first line, pass them in a character vector to the columnnames argument in the makecsv_dataset function.\n\nIf you need to omit some columns from the dataset, create a list of just the columns you plan to use, and pass it into the (optional) select_columns argument of the constructor.\n\n Data preprocessing\n\nA CSV file can contain a variety of data types. Typically you want to convert from those mixed types to a fixed length vector before feeding the data into your model.\n\nYou can preprocess your data using any tool you like (like nltk or sklearn), and just pass the processed output to TensorFlow.\n\nTensorFlow has a built-in system for describing common input conversions: feature_column, which we are going to use via the high-level interface\ncalled feature_spec.\n\nThe primary advantage of doing the preprocessing inside your model is that when you export the model it includes the preprocessing. This way you can pass the raw data directly to your model.\n\nFirst let's define the spec. \n\nspec <- featurespec(traindataset, survived ~ .)\n\nWe can now add steps to our spec telling how to transform our data.\n\nContinuous data\n\nFor continuous data we use the stepnumericcolumn:\n\nspec - spec %% \n  stepnumericcolumn(all_numeric())\n\nAfter adding a step we need to fit our spec:\n\nspec <- fit(spec)\n\nWe can then create a layerdensefeatures that receives our dataset as input and returns an array containing all dense features:\n\nlayer <- layerdensefeatures(featurecolumns = densefeatures(spec))\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  layer()\n\nIt's usually a good idea to normalize all numeric features in a neural network. We can use the same stepnumericcolumn with an additional argument ``:\n\nspec <- featurespec(traindataset, survived ~ .)\nspec - spec %% \n  stepnumericcolumn(allnumeric(), normalizerfn = scaler_standard())\n\nWe can then fit and creat the layerdensefeatures to take a look at the output:\n\nspec <- fit(spec)\nlayer <- layerdensefeatures(featurecolumns = densefeatures(spec))\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  layer()\n\nNow, the outputs are scaled.\n\n Categorical data\n\nCategorical data can't be directly included in the model matrix - we need to perform some kind of transformation in order to represent them as numbers. Representing categorical variables as a set of one-hot encoded columns is very common in practice. \n\nWe can also perform this transformation using the feature_spec API:\n\nLet's again define our spec and add some steps:\n\nspec <- featurespec(traindataset, survived ~ .)\nspec - spec %% \n  stepcategoricalcolumnwithvocabulary_list(sex) %% \n  stepindicatorcolumn(sex)\n\nWe can now see the output with:\n\nspec <- fit(spec)\nlayer <- layerdensefeatures(featurecolumns = densefeatures(spec))\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  layer()\n\nWe can see that this generates 2 columns, one for each different category in the column sex of the dataset.\n\nIt's straightforward to make this transformation for all the categorical features in the dataset:\n\nspec <- featurespec(traindataset, survived ~ .)\nspec - spec %% \n  stepcategoricalcolumnwithvocabularylist(allnominal()) %% \n  stepindicatorcolumn(all_nominal())\n\nNow let's see the output:\n\nspec <- fit(spec)\nlayer <- layerdensefeatures(featurecolumns = densefeatures(spec))\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  layer()\n\nCombining everything\n\nWe demonstrated how to use the feature_spec interface both for continuous and categorical data separetedly. It's also possible to combine all transformations in a single spec:\n\nspec - featurespec(traindataset, survived ~ .) %% \n  stepnumericcolumn(allnumeric(), normalizerfn = scaler_standard()) %% \n  stepcategoricalcolumnwithvocabularylist(allnominal()) %% \n  stepindicatorcolumn(all_nominal())\n\nNow, let's fit the spec and take a look at the output:\n\nspec <- fit(spec)\nlayer <- layerdensefeatures(featurecolumns = densefeatures(spec))\ntrain_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  layer()\n\nThis concludes our data preprocessing step and we can now focus on building a training a model.\n\n Building the model\n\nWe will use the Keras sequential API do build a model that uses the \ndense features we have defined in the spec:\n\nmodel - kerasmodelsequential() %% \n  layerdensefeatures(featurecolumns = densefeatures(spec)) %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %% compile(\n  loss = \"binary_crossentropy\",\n  optimizer = \"adam\",\n  metrics = \"accuracy\"\n)\n\nTrain, evaluate and predict\n\nNow the model can be instantiated and trained.\n\nmodel %% \n  fit(\n    traindataset %% datasetusespec(spec) %% datasetshuffle(500),\n    epochs = 20,\n    validationdata = testdataset %% datasetusespec(spec),\n    verbose = 2\n  )\n\nOnce the model is trained, you can check its accuracy on the test_data set.\n\nmodel %% evaluate(testdataset %% datasetuse_spec(spec), verbose = 0)\n\nYou can also use predict to infer labels on a batch or a dataset of batches:\n\nbatch - test_dataset %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next() %% \n  reticulate::pytor()\npredict(model, batch)\n\n","id":111},{"path":"/tutorials/beginners/load/load_image","title":"\"Loading image data\"","output":"rmarkdown::html_vignette","vignette":">","  %\\VignetteIndexEntry{Tutorial":" %\\VignetteIndexEntry{Tutorial: Basic Classification}","  %\\VignetteEngine{knitr":" %\\VignetteEngine{knitr::rmarkdown}","type":"docs","repo":"https://github.com/rstudio/keras","menu":"menu:","  main":" main:","    name":"   name: \"Load image data\"","    identifier":"   identifier: \"keras-tutorial-basic-load-image\"","    parent":"   parent: \"tutorials-beginners-load-top\"","    weight":"   weight: 20","content":"\nknitr::opts_chunk$set(echo = TRUE, eval = TRUE)\n\n Note: this is the R version of this tutorial in the TensorFlow oficial webiste.\n\nThis tutorial provides a simple example of how to load an image dataset using tfdatasets.\n\nThe dataset used in this example is distributed as directories of images, with one \nclass of image per directory.\n\nSetup\n\nlibrary(keras)\nlibrary(tfdatasets)\n\n Retrieve the images\n\nBefore you start any training, you will need a set of images to teach the network about the new classes you want to recognize. You can use an archive of creative-commons licensed flower photos from Google.\n\n Note: all images are licensed CC-BY, creators are listed in the LICENSE.txt file.\n\ndatadir <- getfile(\n  origin = \"https://storage.googleapis.com/download.tensorflow.org/exampleimages/flowerphotos.tgz\",\n  fname = \"flower_photos.tgz\",\n  extract = TRUE\n)\ndatadir <- file.path(dirname(datadir), \"flower_photos\")\n\nAfter downloading (218MB), you should now have a copy of the flower photos \navailable.\n\nThe directory contains 5 sub-directories, one per class:\n\nimages <- list.files(data_dir, pattern = \".jpg\", recursive = TRUE)\nlength(images)\n\nclasses <- list.dirs(data_dir, full.names = FALSE, recursive = FALSE)\nclasses\n\nLoad using tfdatasets\n\nTo load the files as a TensorFlow Dataset first create a dataset of the file paths:\n\nlistds <- filelistdataset(filepattern = paste0(data_dir, \"/*/*\"))\n\nlistds %% reticulate::asiterator() %% reticulate::iter_next()\n\nWrite a short pure-tensorflow function that converts a file paths to an (image_data, label) pair:\n\ngetlabel <- function(filepath)  \n\ndecodeimg <- function(filepath, height = 224, width = 224)  \n\npreprocesspath <- function(filepath)  \n\nUse dataset_map to create a dataset of image, label pairs:\n\n numparallelcalls are going to be autotuned\nlabeledds - listds %% \n  datasetmap(preprocesspath, numparallelcalls = tf$data$experimental$AUTOTUNE)\n\nLet's see what the output looks like:\n\nlabeled_ds %% \n  reticulate::as_iterator() %% \n  reticulate::iter_next()\n\nTraining a model\n\nTo train a model with this dataset you will want the data:\n\nTo be well shuffled.\nTo be batched.\nBatches to be available as soon as possible.\n\nThese features can be easily added using tfdatasets.\n\nFirst, let's define a function that prepares a dataset in order to feed to a Keras\nmodel.\n\nprepare <- function(ds, batchsize, shufflebuffer_size)  \n\nNow let's define a Keras model to classify the images:\n\nmodel - kerasmodelsequential() %% \n  layer_flatten() %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 128, activation = \"relu\") %% \n  layer_dense(units = 5, activation = \"softmax\")\n\nmodel %% \n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = \"accuracy\"\n  )\n\nWe can then fit the model feeding the dataset we just created:\n\n Note We are fitting this model as an example of how to the pipeline built \nwith Keras. In real use cases you should always use validation datasets in order \nto verify your model performance. \n\nmodel %% \n  fit(\n    prepare(labeledds, batchsize = 32, shufflebuffersize = 1000),\n    epochs = 5,\n    verbose = 2\n  )\n\n","id":112}]