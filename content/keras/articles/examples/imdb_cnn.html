---
title: imdb_cnn
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    parent: keras-examples
---



<div class="source-ref">
<p><span class="caption">Source: </span><a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/imdb_cnn.R</a></p>
</div>
<p>Use Convolution1D for text classification.</p>
<p>Output after 2 epochs: ~0.89
Time per epoch on CPU (Intel i5 2.4Ghz): 90s
Time per epoch on GPU (Tesla K40): 10s</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(keras)

<span class="co"># Set parameters:</span>
max_features &lt;-<span class="st"> </span><span class="dv">5000</span>
maxlen &lt;-<span class="st"> </span><span class="dv">400</span>
batch_size &lt;-<span class="st"> </span><span class="dv">32</span>
embedding_dims &lt;-<span class="st"> </span><span class="dv">50</span>
filters &lt;-<span class="st"> </span><span class="dv">250</span>
kernel_size &lt;-<span class="st"> </span><span class="dv">3</span>
hidden_dims &lt;-<span class="st"> </span><span class="dv">250</span>
epochs &lt;-<span class="st"> </span><span class="dv">2</span>


<span class="co"># Data Preparation --------------------------------------------------------</span>

<span class="co"># Keras load all data into a list with the following structure:</span>
<span class="co"># List of 2</span>
<span class="co"># $ train:List of 2</span>
<span class="co"># ..$ x:List of 25000</span>
<span class="co"># .. .. [list output truncated]</span>
<span class="co"># .. ..- attr(*, "dim")= int 25000</span>
<span class="co"># ..$ y: num [1:25000(1d)] 1 0 0 1 0 0 1 0 1 0 ...</span>
<span class="co"># $ test :List of 2</span>
<span class="co"># ..$ x:List of 25000</span>
<span class="co"># .. .. [list output truncated]</span>
<span class="co"># .. ..- attr(*, "dim")= int 25000</span>
<span class="co"># ..$ y: num [1:25000(1d)] 1 1 1 1 1 0 0 0 1 1 ...</span>
<span class="co">#</span>
<span class="co"># The x data includes integer sequences, each integer is a word.</span>
<span class="co"># The y data includes a set of integer labels (0 or 1).</span>
<span class="co"># The num_words argument indicates that only the max_fetures most frequent</span>
<span class="co"># words will be integerized. All other will be ignored.</span>
<span class="co"># See help(dataset_imdb)</span>
imdb &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/dataset_imdb.html">dataset_imdb</a></span>(<span class="dt">num_words =</span> max_features)

<span class="co"># Pad the sequences, so they have all the same length</span>
<span class="co"># This will convert the dataset into a matrix: each line is a review</span>
<span class="co"># and each column a word on the sequence. </span>
<span class="co"># Pad the sequences with 0 to the left.</span>
x_train &lt;-<span class="st"> </span>imdb<span class="op">$</span>train<span class="op">$</span>x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/pad_sequences.html">pad_sequences</a></span>(<span class="dt">maxlen =</span> maxlen)
x_test &lt;-<span class="st"> </span>imdb<span class="op">$</span>test<span class="op">$</span>x <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/pad_sequences.html">pad_sequences</a></span>(<span class="dt">maxlen =</span> maxlen)

<span class="co"># Defining Model ------------------------------------------------------</span>

<span class="co">#Initialize model</span>
model &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/keras_model_sequential.html">keras_model_sequential</a></span>()

model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="co"># Start off with an efficient embedding layer which maps</span>
<span class="st">  </span><span class="co"># the vocab indices into embedding_dims dimensions</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_embedding.html">layer_embedding</a></span>(max_features, embedding_dims, <span class="dt">input_length =</span> maxlen) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dropout.html">layer_dropout</a></span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span>

<span class="st">  </span><span class="co"># Add a Convolution1D, which will learn filters</span>
<span class="st">    </span><span class="co"># Word group filters of size filter_length:</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_conv_1d.html">layer_conv_1d</a></span>(
    filters, kernel_size, 
    <span class="dt">padding =</span> <span class="st">"valid"</span>, <span class="dt">activation =</span> <span class="st">"relu"</span>, <span class="dt">strides =</span> <span class="dv">1</span>
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="co"># Apply max pooling:</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_global_max_pooling_1d.html">layer_global_max_pooling_1d</a></span>() <span class="op">%&gt;%</span>

<span class="st">  </span><span class="co"># Add a vanilla hidden layer:</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(hidden_dims) <span class="op">%&gt;%</span>

<span class="st">  </span><span class="co"># Apply 20% layer dropout</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dropout.html">layer_dropout</a></span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_activation.html">layer_activation</a></span>(<span class="st">"relu"</span>) <span class="op">%&gt;%</span>

<span class="st">  </span><span class="co"># Project onto a single unit output layer, and squash it with a sigmoid</span>

<span class="st">  </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_activation.html">layer_activation</a></span>(<span class="st">"sigmoid"</span>)

<span class="co"># Compile model</span>
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../reference/reexports.html">compile</a></span>(
  <span class="dt">loss =</span> <span class="st">"binary_crossentropy"</span>,
  <span class="dt">optimizer =</span> <span class="st">"adam"</span>,
  <span class="dt">metrics =</span> <span class="st">"accuracy"</span>
)

<span class="co"># Training ----------------------------------------------------------------</span>

model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/reexports.html">fit</a></span>(
    x_train, imdb<span class="op">$</span>train<span class="op">$</span>y,
    <span class="dt">batch_size =</span> batch_size,
    <span class="dt">epochs =</span> epochs,
    <span class="dt">validation_data =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(x_test, imdb<span class="op">$</span>test<span class="op">$</span>y)
  )</code></pre>
