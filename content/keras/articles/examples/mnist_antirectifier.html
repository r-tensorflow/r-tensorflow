---
title: mnist_antirectifier
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    parent: keras-examples
---



<div class="source-ref">
<p><span class="caption">Source: </span><a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_antirectifier.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/mnist_antirectifier.R</a></p>
</div>
<p>Demonstrates how to write custom layers for Keras.</p>
<p>We build a custom activation layer called ‘Antirectifier’, which modifies the
shape of the tensor that passes through it. We need to specify two methods:
<code>compute_output_shape</code> and <code>call</code>.</p>
<p>Note that the same result can also be achieved via a Lambda layer.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(keras)

<span class="co"># Data Preparation --------------------------------------------------------</span>

batch_size &lt;-<span class="st"> </span><span class="dv">128</span>
num_classes &lt;-<span class="st"> </span><span class="dv">10</span>
epochs &lt;-<span class="st"> </span><span class="dv">40</span>

<span class="co"># The data, shuffled and split between train and test sets</span>
mnist &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/dataset_mnist.html">dataset_mnist</a></span>()
x_train &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>x
y_train &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>y
x_test &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>x
y_test &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>y

<span class="co"># Redimension</span>
x_train &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/reexports.html">array_reshape</a></span>(x_train, <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(x_train), <span class="dv">784</span>))
x_test &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/reexports.html">array_reshape</a></span>(x_test, <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(x_test), <span class="dv">784</span>))

<span class="co"># Transform RGB values into [0,1] range</span>
x_train &lt;-<span class="st"> </span>x_train <span class="op">/</span><span class="st"> </span><span class="dv">255</span>
x_test &lt;-<span class="st"> </span>x_test <span class="op">/</span><span class="st"> </span><span class="dv">255</span>

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/cat">cat</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(x_train), <span class="st">'train samples</span><span class="ch">\n</span><span class="st">'</span>)
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/cat">cat</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(x_test), <span class="st">'test samples</span><span class="ch">\n</span><span class="st">'</span>)

<span class="co"># Convert class vectors to binary class matrices</span>
y_train &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/to_categorical.html">to_categorical</a></span>(y_train, num_classes)
y_test &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/to_categorical.html">to_categorical</a></span>(y_test, num_classes)

<span class="co"># Antirectifier Layer -----------------------------------------------------</span></code></pre>
<p>This is the combination of a sample-wise L2 normalization with the
concatenation of the positive part of the input with the negative part
of the input. The result is a tensor of samples that are twice as large
as the input samples.</p>
<p>It can be used in place of a ReLU.
Input shape: 2D tensor of shape (samples, n)
Output shape: 2D tensor of shape (samples, 2*n)</p>
<p>When applying ReLU, assuming that the distribution of the previous output is
approximately centered around 0., you are discarding half of your input. This
is inefficient.</p>
<p>Antirectifier allows to return all-positive outputs like ReLU, without
discarding any data.</p>
<p>Tests on MNIST show that Antirectifier allows to train networks with half
the parameters yet with comparable classification accuracy as an equivalent
ReLU-based network.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Custom layer class</span>
AntirectifierLayer &lt;-<span class="st"> </span>R6<span class="op">::</span><span class="kw"><a href="https://www.rdocumentation.org/packages/R6/topics/R6Class">R6Class</a></span>(<span class="st">"KerasLayer"</span>,
  
  <span class="dt">inherit =</span> KerasLayer,
                           
  <span class="dt">public =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(
   
    <span class="dt">call =</span> <span class="cf">function</span>(x, <span class="dt">mask =</span> <span class="ot">NULL</span>) {
      x &lt;-<span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="kw"><a href="../../reference/k_mean.html">k_mean</a></span>(x, <span class="dt">axis =</span> <span class="dv">2</span>, <span class="dt">keepdims =</span> <span class="ot">TRUE</span>)
      x &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/k_l2_normalize.html">k_l2_normalize</a></span>(x, <span class="dt">axis =</span> <span class="dv">2</span>)
      pos &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/k_relu.html">k_relu</a></span>(x)
      neg &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/k_relu.html">k_relu</a></span>(<span class="op">-</span>x)
      <span class="kw"><a href="../../reference/k_concatenate.html">k_concatenate</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(pos, neg), <span class="dt">axis =</span> <span class="dv">2</span>)
      
    },
     
    <span class="dt">compute_output_shape =</span> <span class="cf">function</span>(input_shape) {
      input_shape[[<span class="dv">2</span>]] &lt;-<span class="st"> </span>input_shape[[<span class="dv">2</span>]] <span class="op">*</span><span class="st"> </span>2L 
      input_shape
    }
  )
)

<span class="co"># Create layer wrapper function</span>
layer_antirectifier &lt;-<span class="st"> </span><span class="cf">function</span>(object) {
  <span class="kw"><a href="../../reference/create_layer.html">create_layer</a></span>(AntirectifierLayer, object)
}


<span class="co"># Define &amp; Train Model -------------------------------------------------</span>

model &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/keras_model_sequential.html">keras_model_sequential</a></span>()
model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">input_shape =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="dv">784</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">layer_antirectifier</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dropout.html">layer_dropout</a></span>(<span class="dt">rate =</span> <span class="fl">0.1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">256</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">layer_antirectifier</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dropout.html">layer_dropout</a></span>(<span class="dt">rate =</span> <span class="fl">0.1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> num_classes, <span class="dt">activation =</span> <span class="st">'softmax'</span>)

<span class="co"># Compile the model</span>
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../reference/reexports.html">compile</a></span>(
  <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
  <span class="dt">optimizer =</span> <span class="st">'rmsprop'</span>,
  <span class="dt">metrics =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="st">'accuracy'</span>)
)

<span class="co"># Train the model</span>
model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../reference/reexports.html">fit</a></span>(x_train, y_train,
  <span class="dt">batch_size =</span> batch_size,
  <span class="dt">epochs =</span> epochs,
  <span class="dt">verbose =</span> <span class="dv">1</span>,
  <span class="dt">validation_data=</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(x_test, y_test)
)</code></pre>
