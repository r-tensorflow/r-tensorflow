---
title: "Overview"
type: docs
menu:
  main:
    name: "Overview"
    identifier: "deploy-overview"
    parent: "deploy-top"
    weight: 1
---

There are multiple ways to deploy TensorFlow models. In this section we will 
describe some of the most used ways of deploying those models.

- **[Plumber API](/deploy/plumber)**: Create a REST API using Plumber to deploy
your TensorFlow model. With Plumber you will still depend on having an R runtime
which be useful when you want to make the data pre-processing in R.

- **[Shiny](/deploy/shiny)**: Create a Shiny app that uses a TensorFlow model 
to generate outputs. 

- **[TensorFlow Serving](/deploy/docker)**: This is the most performant way of deploying TensorFlow models since it's based only inn the [TensorFlow serving C++ server](https://github.com/tensorflow/serving). With TF serving you don't depend
on an R runtime, so all pre-processing must be done in the TensorFlow graph.

- **[RStudio Connect](/deploy/rsconnect)**: RStudio Connect makes it easy to deploy
TensorFlow models and uses TensorFlow serving in the backend.

There are many other options to deploy TensorFlow models built with R that are not
covered in this section. For example:

- Deploy it using a Python runtime.
- Deploy using a [JavaScript runtime](https://www.tensorflow.org/js/tutorials#convert_pretained_models_to_tensorflowjs).
- Deploy to a mobile phone app using [TensorFlow Lite](https://www.tensorflow.org/lite/guide).
- Deploy to a iOS app using [Apple's Core ML tool](https://developer.apple.com/documentation/coreml/converting_trained_models_to_core_ml).
- [Use plumber and Docker to deploy your TensorFlow model](https://github.com/tmobile/r-tensorflow-api) (by T-Mobile).


