---
title: "Loading image data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial: Basic Classification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: "Load image data"
    identifier: "keras-tutorial-basic-load-image"
    parent: "tutorials-beginners-load-top"
    weight: 20
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

> **Note**: this is the R version of [this tutorial](https://www.tensorflow.org/tutorials/load_data/images) in the TensorFlow oficial webiste.

This tutorial provides a simple example of how to load an image dataset using [tfdatasets](https://github.com/rstudio/tfdatasets).

The dataset used in this example is distributed as directories of images, with one 
class of image per directory.

## Setup

```{r}
library(keras)
library(tfdatasets)
```

## Retrieve the images

Before you start any training, you will need a set of images to teach the network about the new classes you want to recognize. You can use an archive of creative-commons licensed flower photos from Google.

> **Note**: all images are licensed CC-BY, creators are listed in the LICENSE.txt file.

```{r}
data_dir <- get_file(
  origin = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz",
  fname = "flower_photos.tgz",
  extract = TRUE
)
data_dir <- file.path(dirname(data_dir), "flower_photos")
```

After downloading (218MB), you should now have a copy of the flower photos 
available.

The directory contains 5 sub-directories, one per class:

```{r}
images <- list.files(data_dir, pattern = ".jpg", recursive = TRUE)
length(images)

classes <- list.dirs(data_dir, full.names = FALSE, recursive = FALSE)
classes
```

## Load using tfdatasets

To load the files as a TensorFlow Dataset first create a dataset of the file paths:

```{r}
list_ds <- file_list_dataset(file_pattern = paste0(data_dir, "/*/*"))
```

```{r}
list_ds %>% reticulate::as_iterator() %>% reticulate::iter_next()
```

Write a short pure-tensorflow function that converts a file paths to an (image_data, label) pair:

```{r}
get_label <- function(file_path) {
  parts <- tf$strings$split(file_path, "/")
  parts[-2] %>% 
    tf$equal(classes) %>% 
    tf$cast(dtype = tf$float32)
}

decode_img <- function(file_path, height = 224, width = 224) {
  
  size <- as.integer(c(height, width))
  
  file_path %>% 
    tf$io$read_file() %>% 
    tf$image$decode_jpeg(channels = 3) %>% 
    tf$image$convert_image_dtype(dtype = tf$float32) %>% 
    tf$image$resize(size = size)
}

preprocess_path <- function(file_path) {
  list(
    decode_img(file_path),
    get_label(file_path)
  )
}
```

Use `dataset_map` to create a dataset of image, label pairs:

```{r}
# num_parallel_calls are going to be autotuned
labeled_ds <- list_ds %>% 
  dataset_map(preprocess_path, num_parallel_calls = tf$data$experimental$AUTOTUNE)
```

Let's see what the output looks like:

```{r}
labeled_ds %>% 
  reticulate::as_iterator() %>% 
  reticulate::iter_next()
```

## Training a model

To train a model with this dataset you will want the data:

* To be well shuffled.
* To be batched.
* Batches to be available as soon as possible.

These features can be easily added using tfdatasets.

First, let's define a function that prepares a dataset in order to feed to a Keras
model.

```{r}
prepare <- function(ds, batch_size, shuffle_buffer_size) {
  
  if (shuffle_buffer_size > 0)
    ds <- ds %>% dataset_shuffle(shuffle_buffer_size)
  
  ds %>% 
    dataset_batch(batch_size) %>% 
    # `prefetch` lets the dataset fetch batches in the background while the model
    # is training.
    dataset_prefetch(buffer_size = tf$data$experimental$AUTOTUNE)
}
```

Now let's define a Keras model to classify the images:

```{r}
model <- keras_model_sequential() %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")

model %>% 
  compile(
    loss = "categorical_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
  )
```

We can then fit the model feeding the dataset we just created:

> **Note** We are fitting this model as an example of how to the pipeline built 
with Keras. In real use cases you should always use validation datasets in order 
to verify your model performance. 

```{r}
model %>% 
  fit(
    prepare(labeled_ds, batch_size = 32, shuffle_buffer_size = 1000),
    epochs = 5,
    verbose = 2
  )
```








