---
title: "Automatic differentiation and gradient tape"
type: docs
menu:
  main:
    name: "Automatic differentiation"
    identifier: "tutorials-advanced-customization-autodiff"
    parent: "tutorials-advanced-customization-top"
    weight: 70
---



<p>In this tutorial we will cover automatic differentiation, a key technique for optimizing machine learning models.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>We will use the TensorFlow R package:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tensorflow)</a></code></pre></div>
</div>
<div id="gradient-tapes" class="section level2">
<h2>Gradient Tapes</h2>
<p>TensorFlow provides the <code>tf$GradientTape</code> API for automatic differentiation - computing the gradient of a computation with respect to its input variables.</p>
<p>Tensorflow “records” all operations executed inside the context of a <code>tf$GradientTape</code> onto a “tape”. Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a “recorded” computation using reverse mode differentiation.</p>
<p>For example:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">ones</span>(<span class="kw"><a href="../../../tensorflow/reference/shape.html">shape</a></span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>t, {</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">  t<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">  y &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_sum</span>(x)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">  z &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">multiply</span>(y, y)</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">})</a>
<a class="sourceLine" id="cb2-8" data-line-number="8"></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co"># Derivative of z with respect to the original input tensor x</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10">dz_dx &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">gradient</span>(z, x)</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">dz_dx</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[8. 8.]
##  [8. 8.]], shape=(2, 2), dtype=float32)</code></pre>
<p>You can also request gradients of the output with respect to intermediate values computed during a “recorded” <code>tf$GradientTape</code> context.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">ones</span>(<span class="kw"><a href="../../../tensorflow/reference/shape.html">shape</a></span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>t, {</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">  t<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">  y &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_sum</span>(x)</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">  z &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">multiply</span>(y, y)</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">})</a>
<a class="sourceLine" id="cb4-8" data-line-number="8"></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"><span class="co"># Use the tape to compute the derivative of z with respect to the</span></a>
<a class="sourceLine" id="cb4-10" data-line-number="10"><span class="co"># intermediate value y.</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">dz_dy &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">gradient</span>(z, y)</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">dz_dy</a></code></pre></div>
<pre><code>## tf.Tensor(8.0, shape=(), dtype=float32)</code></pre>
<p>By default, the resources held by a GradientTape are released as soon as <code>GradientTape$gradient()</code> method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the <code>gradient()</code> method as resources are released when the tape object is garbage collected. For example:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>(<span class="dt">persistent =</span> <span class="ot">TRUE</span>) <span class="op">%as%</span><span class="st"> </span>t, {</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">  t<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">  y &lt;-<span class="st"> </span>x <span class="op">*</span><span class="st"> </span>x</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">  z &lt;-<span class="st"> </span>y <span class="op">*</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">})</a>
<a class="sourceLine" id="cb6-8" data-line-number="8"></a>
<a class="sourceLine" id="cb6-9" data-line-number="9"><span class="co"># Use the tape to compute the derivative of z with respect to the</span></a>
<a class="sourceLine" id="cb6-10" data-line-number="10"><span class="co"># intermediate value y.</span></a>
<a class="sourceLine" id="cb6-11" data-line-number="11">dz_dx &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">gradient</span>(z, x) <span class="co"># 108.0 (4*x^3 at x = 3)</span></a>
<a class="sourceLine" id="cb6-12" data-line-number="12">dz_dx</a></code></pre></div>
<pre><code>## tf.Tensor(108.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">dy_dx &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">gradient</span>(y, x) <span class="co"># 6.0</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">dy_dx</a></code></pre></div>
<pre><code>## tf.Tensor(6.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/rm.html">rm</a></span>(t)  <span class="co"># Drop the reference to the tape</span></a></code></pre></div>
<div id="recording-control-flow" class="section level3">
<h3>Recording control flow</h3>
<p>Because tapes record operations as they are executed, R control flow (using ifs and whiles for example) is naturally handled:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">f &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">  output &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span>(y)) {</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">    <span class="cf">if</span> (i <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>i <span class="op">&lt;=</span><span class="st"> </span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb11-5" data-line-number="5">      output =<span class="st"> </span>tf<span class="op">$</span><span class="kw">multiply</span>(output, x)</a>
<a class="sourceLine" id="cb11-6" data-line-number="6">  }</a>
<a class="sourceLine" id="cb11-7" data-line-number="7">  output</a>
<a class="sourceLine" id="cb11-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb11-9" data-line-number="9"></a>
<a class="sourceLine" id="cb11-10" data-line-number="10">grad &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {</a>
<a class="sourceLine" id="cb11-11" data-line-number="11">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>t, {</a>
<a class="sourceLine" id="cb11-12" data-line-number="12">    t<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb11-13" data-line-number="13">    out &lt;-<span class="st"> </span><span class="kw">f</span>(x, y)</a>
<a class="sourceLine" id="cb11-14" data-line-number="14">  })</a>
<a class="sourceLine" id="cb11-15" data-line-number="15">  t<span class="op">$</span><span class="kw">gradient</span>(out, x)</a>
<a class="sourceLine" id="cb11-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb11-17" data-line-number="17"></a>
<a class="sourceLine" id="cb11-18" data-line-number="18">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb11-19" data-line-number="19"><span class="kw">grad</span>(x, <span class="dv">6</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(12.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">grad</span>(x, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(12.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">grad</span>(x, <span class="dv">4</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(4.0, shape=(), dtype=float32)</code></pre>
</div>
<div id="higher-order-gradients" class="section level3">
<h3>Higher-order gradients</h3>
<p>Operations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(<span class="fl">1.0</span>)  <span class="co"># Create a Tensorflow variable initialized to 1.0</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"></a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>t, {</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">  </a>
<a class="sourceLine" id="cb17-5" data-line-number="5">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>t2, {</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">    y &lt;-<span class="st"> </span>x<span class="op">*</span>x<span class="op">*</span>x</a>
<a class="sourceLine" id="cb17-7" data-line-number="7">  })</a>
<a class="sourceLine" id="cb17-8" data-line-number="8">  </a>
<a class="sourceLine" id="cb17-9" data-line-number="9">  <span class="co"># Compute the gradient inside the 't' context manager</span></a>
<a class="sourceLine" id="cb17-10" data-line-number="10">  <span class="co"># which means the gradient computation is differentiable as well.</span></a>
<a class="sourceLine" id="cb17-11" data-line-number="11">  dy_dx &lt;-<span class="st"> </span>t2<span class="op">$</span><span class="kw">gradient</span>(y, x)</a>
<a class="sourceLine" id="cb17-12" data-line-number="12">  </a>
<a class="sourceLine" id="cb17-13" data-line-number="13">})</a>
<a class="sourceLine" id="cb17-14" data-line-number="14"></a>
<a class="sourceLine" id="cb17-15" data-line-number="15">d2y_dx &lt;-<span class="st"> </span>t<span class="op">$</span><span class="kw">gradient</span>(dy_dx, x)</a>
<a class="sourceLine" id="cb17-16" data-line-number="16"></a>
<a class="sourceLine" id="cb17-17" data-line-number="17">dy_dx</a></code></pre></div>
<pre><code>## tf.Tensor(3.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">d2y_dx</a></code></pre></div>
<pre><code>## tf.Tensor(6.0, shape=(), dtype=float32)</code></pre>
</div>
</div>
