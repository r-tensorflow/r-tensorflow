---
title: "Feature columns"
output: 
  rmarkdown::html_vignette: default
vignette: >
  %\VignetteIndexEntry{Feature columns}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
type: docs
repo: https://github.com/rstudio/tfdatasets
menu:
  main:
    name: "Feature columns"
    identifier: "feature-columns"
    parent: "data-feature-spec-top"
    weight: 20
aliases:
  - /tools/tfdatasets/feature-spec
  - /tools/tfdatasets/articles/feature_columns.html
---



<blockquote>
<p>This document is an adaptation of the official TensorFlow <a href="https://www.tensorflow.org/guide/feature_columns">Feature Columns guide</a>.</p>
</blockquote>
<p>This document details feature columns and how they can be used as inputs to neural
networks using TensorFlow. Feature columns are very rich, enabling you to transform
a diverse range of raw data into formats that neural networks can use, allowing
easy experimentation.</p>
<p>What kind of data can a deep neural network operate on? The answer is, of course,
numbers (for example, <code>tf$float32</code>). After all, every neuron in a neural network
performs multiplication and addition operations on weights and input data. Real-life
input data, however, often contains non-numerical (categorical) data. For example,
consider a <code>product_class</code> feature that can contain the following three non-numerical
values:</p>
<ul>
<li>kitchenware</li>
<li>electronics</li>
<li>sports</li>
</ul>
<p>ML models generally represent categorical values as simple vectors in which a 1 represents
the presence of a value and a 0 represents the absence of a value. For example, when
<code>product_class</code> is set to sports, an ML model would usually represent <code>product_class</code> as
<code>[0, 0, 1]</code>, meaning:</p>
<ul>
<li>0: kitchenware is absent</li>
<li>0: electronics is absent</li>
<li>1: sports is present</li>
</ul>
<p>So, although raw data can be numerical or categorical, an ML model represents all
features as numbers.</p>
<p>This document explains nine of the feature columns available in tfdatasets. As the
following figure shows, all nine functions return either a <code>categorical_column</code> or a
<code>dense_column</code> object, except <code>bucketized_column</code>, which inherits from both classes:</p>
<div class="figure">
<img src="../images/some_constructors.jpg" alt="Feature column methods fall into two main categories and one hybrid category."><p class="caption">Feature column methods fall into two main categories and one hybrid category.</p>
</div>
<p>Let’s look at these functions in more detail.</p>
<div id="feature-spec-interface" class="section level2">
<h2>Feature spec interface</h2>
<p>We are going to use the <code>feature_spec</code> interface in the examples. The <code>feature_spec</code>
interface is an abstraction that makes it easier to define feature columns in R.</p>
<p>You can initialize a <code>feature_spec</code> with:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tfdatasets)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">hearts_dataset &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/tensor_slices_dataset.html">tensor_slices_dataset</a></span>(hearts)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/feature_spec.html">feature_spec</a></span>(hearts_dataset, target <span class="op">~</span><span class="st"> </span>.)</a></code></pre></div>
<p>We then add steps in order to define <code>feature_columns</code>. Read the <code>feature_spec</code>
vignette for more information.</p>
</div>
<div id="numeric-column" class="section level2">
<h2>Numeric column</h2>
<p>We use <code>step_numeric_column</code> to add numeric columns to our <code>spec</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(age)</a></code></pre></div>
<pre><code>## ── Feature Spec ────────────────────────────────────────────────────────────────────────────────────────── 
## A feature_spec with 1 steps.
## Fitted: FALSE 
## ── Steps ───────────────────────────────────────────────────────────────────────────────────────────────── 
## StepNumericColumn: age 
## ── Dense features ──────────────────────────────────────────────────────────────────────────────────────── 
## Feature spec must be fitted before we can detect the dense features.</code></pre>
<p>By default, a numeric column creates a single value (scalar). Use the shape argument
to specify another shape. For example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Represent a 10-element vector in which each cell contains a tf$float32.</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(bowling, <span class="dt">shape =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co"># Represent a 10x5 matrix in which each cell contains a tf$float32.</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(my_matrix, <span class="dt">shape =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">10</span>, <span class="dv">5</span>))</a></code></pre></div>
<p>A nice feature of <code>step_numeric_column</code>is that you can also specify normalizer functions.
When using a scaler, the scaling constants will be learned from data when fitting
the <code>feature_spec</code>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># use a function that defines tensorflow ops.</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(age, <span class="dt">normalizer_fn =</span> <span class="cf">function</span>(x) (x<span class="dv">-10</span>)<span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb5-4" data-line-number="4"></a>
<a class="sourceLine" id="cb5-5" data-line-number="5"><span class="co"># use a scaler</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(age, <span class="dt">normalizer_fn =</span> <span class="kw"><a href="../../tools/tfdatasets/reference/scaler_standard.html">scaler_standard</a></span>())</a></code></pre></div>
</div>
<div id="bucketized-column" class="section level2">
<h2>Bucketized column</h2>
<p>Often, you don’t want to feed a number directly into the model, but instead split
its value into different categories based on numerical ranges. To do so, create a
<code>bucketized_column</code>. For example, consider raw data that represents the year a
house was built. Instead of representing that year as a scalar numeric column, we
could split the year into the following four buckets:</p>
<div class="figure">
<img src="../images/bucketized_column.jpg" alt="Dividing year data into four buckets."><p class="caption">Dividing year data into four buckets.</p>
</div>
<p>Why would you want to split a number — a perfectly valid input to your model — into a
categorical value? Well, notice that the categorization splits a single input number
into a four-element vector. Therefore, the model now can learn four individual weights
rather than just one; four weights creates a richer model than one weight. More importantly,
bucketizing enables the model to clearly distinguish between different year categories
since only one of the elements is set (1) and the other three elements are cleared (0).
For example, when we just use a single number (a year) as input, a linear model
can only learn a linear relationship. So, bucketing provides the model with additional
flexibility that the model can use to learn.</p>
<p>The following code demonstrates how to create a bucketized feature:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># First, convert the raw input to a numeric column.</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2">spec &lt;-<span class="st"> </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(age)</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"></a>
<a class="sourceLine" id="cb6-5" data-line-number="5"><span class="co"># Then, bucketize the numeric column.</span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6">spec &lt;-<span class="st">  </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_bucketized_column.html">step_bucketized_column</a></span>(age, <span class="dt">boundaries =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">30</span>, <span class="dv">50</span>, <span class="dv">70</span>))</a></code></pre></div>
<p>Note that specifying a three-element boundaries vector creates a four-element bucketized vector.</p>
</div>
<div id="categorical-identity-column" class="section level1">
<h1>Categorical identity column</h1>
<p>Categorical identity columns are to <code>tfdatasets</code> what <code>factors</code> are to R.
Put differently, they can be seen as a special case of bucketized columns.
In traditional bucketized columns, each bucket represents a range of values
(for example, from 1960 to 1979). In a categorical identity column, each bucket
represents a single, unique integer. For example, let’s say you want to represent
the integer range [0, 4). That is, you want to represent the integers 0, 1, 2, or 3.
In this case, the categorical identity mapping looks like this:</p>
<div class="figure">
<img src="../images/categorical_column_with_identity.jpg" alt="A categorical identity column mapping. Note that this is a one-hot encoding, not a binary numerical encoding."><p class="caption">A categorical identity column mapping. Note that this is a one-hot encoding, not a binary numerical encoding.</p>
</div>
<p>As with bucketized columns, a model can learn a separate weight for each class in
a categorical identity column. For example, instead of using a string to represent
the <code>product_class</code>, let’s represent each class with a unique integer value. That is:</p>
<ul>
<li>0=“kitchenware”</li>
<li>1=“electronics”</li>
<li>2=“sport”</li>
</ul>
<p>Call <code>step_categorical_column_with_identity</code> to add a categorical identity column to
the <code>feature_spec</code>. For example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># Create categorical output for an integer feature named "my_feature_b",</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="co"># The values of my_feature_b must be &gt;= 0 and &lt; num_buckets</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3">spec &lt;-<span class="st"> </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_identity.html">step_categorical_column_with_identity</a></span>(my_feature_b, <span class="dt">num_buckets =</span> <span class="dv">4</span>)</a></code></pre></div>
<div id="categorical-vocabulary-column" class="section level2">
<h2>Categorical vocabulary column</h2>
<p>We cannot input strings directly to a model. Instead, we must first map strings to numeric or categorical values. Categorical vocabulary columns provide a good way to represent strings as a one-hot vector. For example:</p>
<div class="figure">
<img src="../images/categorical_column_with_vocabulary.jpg" alt="Mapping string values to vocabulary columns."><p class="caption">Mapping string values to vocabulary columns.</p>
</div>
<p>As you can see, categorical vocabulary columns are kind of an enum version of
categorical identity columns. <code>tfdatasets</code> provides two different functions to create
categorical vocabulary columns:</p>
<ul>
<li><code>step_categorical_column_with_vocabulary_list</code></li>
<li><code>step_categorical_column_with_vocabulary_file</code></li>
</ul>
<p><code>categorical_column_with_vocabulary_list</code> maps each string to an integer based on an explicit vocabulary list. For example:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1">spec &lt;-<span class="st"> </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_vocabulary_list.html">step_categorical_column_with_vocabulary_list</a></span>(</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">    thal, </a>
<a class="sourceLine" id="cb8-4" data-line-number="4">    <span class="dt">vocabulary_list =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"fixed"</span>, <span class="st">"normal"</span>, <span class="st">"reversible"</span>)</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">  )</a></code></pre></div>
<p>Note that the <code>vocabulary_list</code> argument is optional in R and the vocabulary will be
discovered when fitting the <code>featture_spec</code> which saves us a lot of typing.</p>
<p>You can also place the vocabulary in a separate file that should contain one line
for each vocabulary element. You can then use:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">spec &lt;-<span class="st"> </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_vocabulary_file.html">step_categorical_column_with_vocabulary_file</a></span>(thal, <span class="dt">vocabulary_file =</span> <span class="st">"thal.txt"</span>)</a></code></pre></div>
</div>
<div id="hashed-column" class="section level2">
<h2>Hashed Column</h2>
<p>So far, we’ve worked with a naively small number of categories. For example, our
product_class example has only 3 categories. Often though, the number of categories
can be so big that it’s not possible to have individual categories for each vocabulary
word or integer because that would consume too much memory. For these cases, we can
instead turn the question around and ask, “How many categories am I willing to have
for my input?” In fact, the <code>step_categorical_column_with_hash_bucket</code>
function enables you to specify the number of categories. For this type of feature
column the model calculates a hash value of the input, then puts it into one of
the hash_bucket_size categories using the modulo operator, as in the following
pseudocode:</p>
<pre><code># pseudocode
feature_id = hash(raw_feature) % hash_bucket_size</code></pre>
<p>The code to add the <code>feature_column</code> to the <code>feature_spec</code> might look something
like this:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1">spec &lt;-<span class="st"> </span>spec <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_hash_bucket.html">step_categorical_column_with_hash_bucket</a></span>(thal, <span class="dt">hash_bucket_size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p>At this point, you might rightfully think: “This is crazy!” After all, we are
forcing the different input values to a smaller set of categories. This means
that two probably unrelated inputs will be mapped to the same category, and
consequently mean the same thing to the neural network. The following figure
illustrates this dilemma, showing that kitchenware and sports both get assigned
to category (hash bucket) 12:</p>
<div class="figure">
<img src="../images/hashed_column.jpg" alt="Representing data with hash buckets."><p class="caption">Representing data with hash buckets.</p>
</div>
<p>As with many counterintuitive phenomena in machine learning, it turns out that
hashing often works well in practice. That’s because hash categories provide the
model with some separation. The model can use additional features to further
separate kitchenware from sports.</p>
</div>
<div id="crossed-column" class="section level2">
<h2>Crossed column</h2>
<p>Combining features into a single feature, better known as feature crosses, enables
the model to learn separate weights for each combination of features.</p>
<p>More concretely, suppose we want our model to calculate real estate prices in Atlanta, GA.
Real-estate prices within this city vary greatly depending on location. Representing
latitude and longitude as separate features isn’t very useful in identifying real-estate
location dependencies; however, crossing latitude and longitude into a single feature can
pinpoint locations. Suppose we represent Atlanta as a grid of 100x100 rectangular sections,
identifying each of the 10,000 sections by a feature cross of latitude and longitude. This
feature cross enables the model to train on pricing conditions related to each individual
section, which is a much stronger signal than latitude and longitude alone.</p>
<p>The following figure shows our plan, with the latitude &amp; longitude values for the
corners of the city in red text:</p>
<div class="figure">
<img src="../images/Atlanta.jpg" alt="Map of Atlanta. Imagine this map divided into 10,000 sections of equal size."><p class="caption">Map of Atlanta. Imagine this map divided into 10,000 sections of equal size.</p>
</div>
<p>For the solution, we used a combination of the bucketized_column we looked at earlier,
with the <code>step_crossed_column</code> function.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/feature_spec.html">feature_spec</a></span>(dataset, target <span class="op">~</span><span class="st"> </span>latitute <span class="op">+</span><span class="st"> </span>longitude) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(latitude, longitude) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_bucketized_column.html">step_bucketized_column</a></span>(latitude, <span class="dt">boundaries =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(latitude_edges)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb12-4" data-line-number="4"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_bucketized_column.html">step_bucketized_column</a></span>(longitude, <span class="dt">boundaries =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(longitude_edges)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb12-5" data-line-number="5"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_crossed_column.html">step_crossed_column</a></span>(<span class="dt">latitude_longitude =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(latitude, longitude), <span class="dt">hash_bucket_size =</span> <span class="dv">100</span>)</a></code></pre></div>
<p>You may create a feature cross from either of the following:</p>
<ul>
<li>Any categorical column, except <code>categorical_column_with_hash_bucket</code> (since crossed_column hashes the input).</li>
</ul>
<p>When the feature columns <code>latitude_bucket_fc</code> and <code>longitude_bucket_fc</code> are crossed,
TensorFlow will create <code>(latitude_fc, longitude_fc)</code> pairs for each example. This would
produce a full grid of possibilities as follows:</p>
<pre><code>(0,0),  (0,1)...  (0,99)
 (1,0),  (1,1)...  (1,99)
   ...     ...       ...
(99,0), (99,1)...(99, 99)</code></pre>
<p>Except that a full grid would only be tractable for inputs with limited vocabularies.
Instead of building this, potentially huge, table of inputs, the <code>crossed_column</code> only
builds the number requested by the <code>hash_bucket_size argument</code>. The feature column assigns
an example to a index by running a hash function on the tuple of inputs, followed by a
modulo operation with <code>hash_bucket_size</code>.</p>
<p>As discussed earlier, performing the hash and modulo function limits the number of
categories, but can cause category collisions; that is, multiple (latitude, longitude) feature
crosses will end up in the same hash bucket. In practice though, performing feature
crosses still adds significant value to the learning capability of your models.</p>
<p>Somewhat counterintuitively, when creating feature crosses, you typically still
should include the original (uncrossed) features in your model (as in the preceding code snippet).
The independent latitude and longitude features help the model distinguish between examples
where a hash collision has occurred in the crossed feature.</p>
</div>
<div id="indicator-and-embedding-columns" class="section level2">
<h2>Indicator and embedding columns</h2>
<p>Indicator columns and embedding columns never work on features directly, but instead
take categorical columns as input.</p>
<p>When using an indicator column, we’re telling TensorFlow to do exactly what we’ve
seen in our categorical product_class example. That is, an indicator column treats
each category as an element in a one-hot vector, where the matching category has value
1 and the rest have 0s:</p>
<div class="figure">
<img src="../images/categorical_column_with_identity.jpg" alt="Representing data in indicator columns."><p class="caption">Representing data in indicator columns.</p>
</div>
<p>Here’s how you create an indicator column:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/feature_spec.html">feature_spec</a></span>(dataset, target <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_vocabulary_list.html">step_categorical_column_with_vocabulary_list</a></span>(product_class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb14-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_indicator_column.html">step_indicator_column</a></span>(product_class)</a></code></pre></div>
<p>Now, suppose instead of having just three possible classes, we have a million.
Or maybe a billion. For a number of reasons, as the number of categories grow large,
it becomes infeasible to train a neural network using indicator columns.</p>
<p>We can use an embedding column to overcome this limitation. Instead of representing
the data as a one-hot vector of many dimensions, an embedding column represents that
data as a lower-dimensional, ordinary vector in which each cell can contain any number,
not just 0 or 1. By permitting a richer palette of numbers for every cell,
an embedding column contains far fewer cells than an indicator column.</p>
<p>Let’s look at an example comparing indicator and embedding columns. Suppose our
input examples consist of different words from a limited palette of only 81 words.
Further suppose that the data set provides the following input words in 4
separate examples:</p>
<ul>
<li>“dog”</li>
<li>“spoon”</li>
<li>“scissors”</li>
<li>“guitar”</li>
</ul>
<p>In that case, the following figure illustrates the processing path for embedding columns or indicator columns.</p>
<div class="figure">
<img src="../images/embedding_vs_indicator.jpg" alt="An embedding column stores categorical data in a lower-dimensional vector than an indicator column. (We just placed random numbers into the embedding vectors; training determines the actual numbers.)"><p class="caption">An embedding column stores categorical data in a lower-dimensional vector than an indicator column. (We just placed random numbers into the embedding vectors; training determines the actual numbers.)</p>
</div>
<p>When an example is processed, one of the categorical_column_with_[…] functions maps the example string to a numerical categorical value. For example, a function maps “spoon” to [32]. (The 32 comes from our imagination — the actual values depend on the mapping function.) You may then represent these numerical categorical values in either of the following two ways:</p>
<ul>
<li><p>As an indicator column. A function converts each numeric categorical value into an 81-element vector (because our palette consists of 81 words), placing a 1 in the index of the categorical value (0, 32, 79, 80) and a 0 in all the other positions.</p></li>
<li><p>As an embedding column. A function uses the numerical categorical values (0, 32, 79, 80) as indices to a lookup table. Each slot in that lookup table contains a 3-element vector.</p></li>
</ul>
<p>How do the values in the embeddings vectors magically get assigned? Actually, the assignments happen during training. That is, the model learns the best way to map your input numeric categorical values to the embeddings vector value in order to solve your problem. Embedding columns increase your model’s capabilities, since an embeddings vector learns new relationships between categories from the training data.</p>
<p>Why is the embedding vector size 3 in our example? Well, the following “formula” provides a general rule of thumb about the number of embedding dimensions:</p>
<pre><code>embedding_dimensions =  number_of_categories**0.25</code></pre>
<p>That is, the embedding vector dimension should be the 4th root of the number of
categories. Since our vocabulary size in this example is 81, the recommended
number of dimensions is 3:</p>
<pre><code>3 =  81**0.25</code></pre>
<blockquote>
<p>Note: This is just a general guideline; you can set the number of embedding dimensions as you please.</p>
</blockquote>
<p>Call <code>step_embedding_column</code> to create an embedding_column as suggested by the
following snippet:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/feature_spec.html">feature_spec</a></span>(dataset, target <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_vocabulary_list.html">step_categorical_column_with_vocabulary_list</a></span>(product_class) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_embedding_column.html">step_embedding_column</a></span>(product_class, <span class="dt">dimension =</span> <span class="dv">3</span>)</a></code></pre></div>
<p>Embeddings is a significant topic within machine learning. This information was just
to get you started using them as feature columns.</p>
</div>
</div>
<div id="passing-feature-columns-to-keras" class="section level1">
<h1>Passing feature columns to Keras</h1>
<p>After creating and fitting a <code>feature_spec</code> object you can use the <code>dense_features</code>
to get the Dense features from the specifications. You can then use the <code>layer_dense_features</code>
function from Keras to create a layer that will automatically initialize values.</p>
<p>Here is an an example of how it would work:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(keras)</a>
<a class="sourceLine" id="cb18-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(dplyr)</a></code></pre></div>
<pre><code>## 
## Attaching package: 'dplyr'</code></pre>
<pre><code>## The following objects are masked from 'package:stats':
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/feature_spec.html">feature_spec</a></span>(hearts, target <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_numeric_column.html">step_numeric_column</a></span>(</a>
<a class="sourceLine" id="cb22-3" data-line-number="3">    <span class="kw"><a href="../../tools/tfdatasets/reference/all_numeric.html">all_numeric</a></span>(), <span class="op">-</span>cp, <span class="op">-</span>restecg, <span class="op">-</span>exang, <span class="op">-</span>sex, <span class="op">-</span>fbs,</a>
<a class="sourceLine" id="cb22-4" data-line-number="4">    <span class="dt">normalizer_fn =</span> <span class="kw"><a href="../../tools/tfdatasets/reference/scaler_standard.html">scaler_standard</a></span>()</a>
<a class="sourceLine" id="cb22-5" data-line-number="5">  ) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-6" data-line-number="6"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_categorical_column_with_vocabulary_list.html">step_categorical_column_with_vocabulary_list</a></span>(thal) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-7" data-line-number="7"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_bucketized_column.html">step_bucketized_column</a></span>(age, <span class="dt">boundaries =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">18</span>, <span class="dv">25</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">45</span>, <span class="dv">50</span>, <span class="dv">55</span>, <span class="dv">60</span>, <span class="dv">65</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-8" data-line-number="8"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_indicator_column.html">step_indicator_column</a></span>(thal) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-9" data-line-number="9"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_embedding_column.html">step_embedding_column</a></span>(thal, <span class="dt">dimension =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-10" data-line-number="10"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_crossed_column.html">step_crossed_column</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(thal, bucketized_age), <span class="dt">hash_bucket_size =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb22-11" data-line-number="11"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/step_indicator_column.html">step_indicator_column</a></span>(crossed_thal_bucketized_age)</a>
<a class="sourceLine" id="cb22-12" data-line-number="12"></a>
<a class="sourceLine" id="cb22-13" data-line-number="13">spec &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/reexports.html">fit</a></span>(spec)</a>
<a class="sourceLine" id="cb22-14" data-line-number="14"></a>
<a class="sourceLine" id="cb22-15" data-line-number="15">input &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/layer_input_from_dataset.html">layer_input_from_dataset</a></span>(hearts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span>(<span class="op">-</span>target))</a>
<a class="sourceLine" id="cb22-16" data-line-number="16">output &lt;-<span class="st"> </span>input <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-17" data-line-number="17"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_dense_features.html">layer_dense_features</a></span>(<span class="dt">feature_columns =</span> <span class="kw"><a href="../../tools/tfdatasets/reference/dense_features.html">dense_features</a></span>(spec)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-18" data-line-number="18"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">"sigmoid"</span>)</a>
<a class="sourceLine" id="cb22-19" data-line-number="19"></a>
<a class="sourceLine" id="cb22-20" data-line-number="20">model &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/keras_model.html">keras_model</a></span>(input, output)</a>
<a class="sourceLine" id="cb22-21" data-line-number="21"></a>
<a class="sourceLine" id="cb22-22" data-line-number="22">model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-23" data-line-number="23"><span class="st">  </span><span class="kw"><a href="../../keras/reference/reexports.html">compile</a></span>(</a>
<a class="sourceLine" id="cb22-24" data-line-number="24">    <span class="dt">loss =</span> <span class="st">"binary_crossentropy"</span>, </a>
<a class="sourceLine" id="cb22-25" data-line-number="25">    <span class="dt">optimizer =</span> <span class="st">"adam"</span>,</a>
<a class="sourceLine" id="cb22-26" data-line-number="26">    <span class="dt">metrics =</span> <span class="st">"accuracy"</span></a>
<a class="sourceLine" id="cb22-27" data-line-number="27">    )</a>
<a class="sourceLine" id="cb22-28" data-line-number="28"></a>
<a class="sourceLine" id="cb22-29" data-line-number="29">model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-30" data-line-number="30"><span class="st">  </span><span class="kw"><a href="../../keras/reference/reexports.html">fit</a></span>(</a>
<a class="sourceLine" id="cb22-31" data-line-number="31">    <span class="dt">x =</span> hearts <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span>(<span class="op">-</span>target), <span class="dt">y =</span> hearts<span class="op">$</span>target,</a>
<a class="sourceLine" id="cb22-32" data-line-number="32">    <span class="dt">validation_split =</span> <span class="fl">0.2</span></a>
<a class="sourceLine" id="cb22-33" data-line-number="33">  )</a></code></pre></div>
<pre><code>## Train on 242 samples, validate on 61 samples
## Epoch 1/10
## 
 32/242 [==&gt;...........................] - ETA: 6s - loss: 0.7368 - accuracy: 0.5625
242/242 [==============================] - 1s 6ms/sample - loss: 0.6771 - accuracy: 0.6446 - val_loss: 0.6462 - val_accuracy: 0.6393
## Epoch 2/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.5944 - accuracy: 0.8438
242/242 [==============================] - 0s 216us/sample - loss: 0.6641 - accuracy: 0.6488 - val_loss: 0.6363 - val_accuracy: 0.6393
## Epoch 3/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.6673 - accuracy: 0.5938
242/242 [==============================] - 0s 213us/sample - loss: 0.6516 - accuracy: 0.6529 - val_loss: 0.6269 - val_accuracy: 0.6230
## Epoch 4/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.5900 - accuracy: 0.6875
242/242 [==============================] - 0s 159us/sample - loss: 0.6398 - accuracy: 0.6529 - val_loss: 0.6178 - val_accuracy: 0.6230
## Epoch 5/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.6728 - accuracy: 0.5938
242/242 [==============================] - 0s 155us/sample - loss: 0.6283 - accuracy: 0.6612 - val_loss: 0.6091 - val_accuracy: 0.6393
## Epoch 6/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.6083 - accuracy: 0.6562
242/242 [==============================] - 0s 131us/sample - loss: 0.6173 - accuracy: 0.6694 - val_loss: 0.6009 - val_accuracy: 0.6885
## Epoch 7/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.6746 - accuracy: 0.6875
242/242 [==============================] - 0s 125us/sample - loss: 0.6068 - accuracy: 0.6694 - val_loss: 0.5930 - val_accuracy: 0.6885
## Epoch 8/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.5721 - accuracy: 0.7188
242/242 [==============================] - 0s 120us/sample - loss: 0.5970 - accuracy: 0.6777 - val_loss: 0.5854 - val_accuracy: 0.6885
## Epoch 9/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.5595 - accuracy: 0.7500
242/242 [==============================] - 0s 122us/sample - loss: 0.5875 - accuracy: 0.6777 - val_loss: 0.5782 - val_accuracy: 0.6885
## Epoch 10/10
## 
 32/242 [==&gt;...........................] - ETA: 0s - loss: 0.5756 - accuracy: 0.7188
242/242 [==============================] - 0s 120us/sample - loss: 0.5784 - accuracy: 0.6860 - val_loss: 0.5711 - val_accuracy: 0.6885</code></pre>
</div>
