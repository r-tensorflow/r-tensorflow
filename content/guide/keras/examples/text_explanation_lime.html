---
title: text_explanation_lime
type: docs
repo: https://github.com/rstudio/keras
---



<div class="source-ref">
<p><span class="caption">Source: </span><a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/text_explanation_lime.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/text_explanation_lime.R</a></p>
</div>
<p>This example shows how to use lime to explain text data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(readr)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(dplyr)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(keras)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tidyverse)</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># Download and unzip data</span></span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>activity_url &lt;-<span class="st"> "https://archive.ics.uci.edu/ml/machine-learning-databases/00461/drugLib_raw.zip"</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>temp &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/tempfile.html">tempfile</a></span>()</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="kw"><a href="https://rdrr.io/r/utils/download.file.html">download.file</a></span>(activity_url, temp)</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="kw"><a href="https://rdrr.io/r/utils/unzip.html">unzip</a></span>(temp, <span class="st">"drugLibTest_raw.tsv"</span>)</span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co"># Read dataset</span></span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a>df &lt;-<span class="st"> </span><span class="kw"><a href="https://readr.tidyverse.org/reference/read_delim.html">read_delim</a></span>(<span class="st">'drugLibTest_raw.tsv'</span>,<span class="dt">delim =</span> <span class="st">'</span><span class="ch">\t</span><span class="st">'</span>)</span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="kw"><a href="https://rdrr.io/r/base/unlink.html">unlink</a></span>(temp)</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="co"># Select only rating and text from the whole dataset</span></span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>df =<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span>(rating,commentsReview) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span>(<span class="dt">rating =</span> <span class="kw"><a href="https://dplyr.tidyverse.org/reference/if_else.html">if_else</a></span>(rating <span class="op">&gt;=</span><span class="st"> </span><span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb1-22"><a href="#cb1-22"></a></span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="co"># This is our text</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>text &lt;-<span class="st"> </span>df<span class="op">$</span>commentsReview</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co"># And these are ratings given by customers</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>y_train &lt;-<span class="st"> </span>df<span class="op">$</span>rating</span>
<span id="cb1-28"><a href="#cb1-28"></a></span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="co"># text_tokenizer helps us to turn each word into integers. By selecting maximum number of features</span></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="co"># we also keep the most frequent words. Additionally, by default, all punctuation is removed.</span></span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a>max_features &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>tokenizer &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/text_tokenizer.html">text_tokenizer</a></span>(<span class="dt">num_words =</span> max_features)</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="co"># Then, we need to fit the tokenizer object to our text data</span></span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>tokenizer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../../keras/reference/fit_text_tokenizer.html">fit_text_tokenizer</a></span>(text)</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="co"># Via tokenizer object you can check word indices, word counts and other interesting properties.</span></span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a>tokenizer<span class="op">$</span>word_counts </span>
<span id="cb1-43"><a href="#cb1-43"></a>tokenizer<span class="op">$</span>word_index</span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a><span class="co"># Finally, we can replace words in dataset with integers</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>text_seqs &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/texts_to_sequences.html">texts_to_sequences</a></span>(tokenizer, text)</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a>text_seqs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/utils/head.html">head</a></span>(<span class="dv">3</span>)</span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a><span class="co"># Define the parameters of the keras model</span></span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>maxlen &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb1-53"><a href="#cb1-53"></a>batch_size &lt;-<span class="st"> </span><span class="dv">32</span></span>
<span id="cb1-54"><a href="#cb1-54"></a>embedding_dims &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb1-55"><a href="#cb1-55"></a>filters &lt;-<span class="st"> </span><span class="dv">64</span></span>
<span id="cb1-56"><a href="#cb1-56"></a>kernel_size &lt;-<span class="st"> </span><span class="dv">3</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>hidden_dims &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb1-58"><a href="#cb1-58"></a>epochs &lt;-<span class="st"> </span><span class="dv">15</span></span>
<span id="cb1-59"><a href="#cb1-59"></a></span>
<span id="cb1-60"><a href="#cb1-60"></a><span class="co"># As a final step, restrict the maximum length of all sequences and create a matrix as input for model</span></span>
<span id="cb1-61"><a href="#cb1-61"></a>x_train &lt;-<span class="st"> </span>text_seqs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../../keras/reference/pad_sequences.html">pad_sequences</a></span>(<span class="dt">maxlen =</span> maxlen)</span>
<span id="cb1-62"><a href="#cb1-62"></a></span>
<span id="cb1-63"><a href="#cb1-63"></a><span class="co"># Lets print the first 2 rows and see that max length of first 2 sequences equals to 15</span></span>
<span id="cb1-64"><a href="#cb1-64"></a>x_train[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,]</span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a><span class="co"># Create a model</span></span>
<span id="cb1-67"><a href="#cb1-67"></a>model &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/keras_model_sequential.html">keras_model_sequential</a></span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb1-68"><a href="#cb1-68"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_embedding.html">layer_embedding</a></span>(max_features, embedding_dims, <span class="dt">input_length =</span> maxlen) <span class="op">%&gt;%</span></span>
<span id="cb1-69"><a href="#cb1-69"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_dropout.html">layer_dropout</a></span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span></span>
<span id="cb1-70"><a href="#cb1-70"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_conv_1d.html">layer_conv_1d</a></span>(</span>
<span id="cb1-71"><a href="#cb1-71"></a>    filters, kernel_size, </span>
<span id="cb1-72"><a href="#cb1-72"></a>    <span class="dt">padding =</span> <span class="st">"valid"</span>, <span class="dt">activation =</span> <span class="st">"relu"</span>, <span class="dt">strides =</span> <span class="dv">1</span></span>
<span id="cb1-73"><a href="#cb1-73"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_global_max_pooling_1d.html">layer_global_max_pooling_1d</a></span>() <span class="op">%&gt;%</span></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_dense.html">layer_dense</a></span>(hidden_dims) <span class="op">%&gt;%</span></span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_dropout.html">layer_dropout</a></span>(<span class="fl">0.2</span>) <span class="op">%&gt;%</span></span>
<span id="cb1-77"><a href="#cb1-77"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_activation.html">layer_activation</a></span>(<span class="st">"relu"</span>) <span class="op">%&gt;%</span></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_dense.html">layer_dense</a></span>(<span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb1-79"><a href="#cb1-79"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/layer_activation.html">layer_activation</a></span>(<span class="st">"sigmoid"</span>)</span>
<span id="cb1-80"><a href="#cb1-80"></a></span>
<span id="cb1-81"><a href="#cb1-81"></a><span class="co"># Compile</span></span>
<span id="cb1-82"><a href="#cb1-82"></a>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../../keras/reference/reexports.html">compile</a></span>(</span>
<span id="cb1-83"><a href="#cb1-83"></a>  <span class="dt">loss =</span> <span class="st">"binary_crossentropy"</span>,</span>
<span id="cb1-84"><a href="#cb1-84"></a>  <span class="dt">optimizer =</span> <span class="st">"adam"</span>,</span>
<span id="cb1-85"><a href="#cb1-85"></a>  <span class="dt">metrics =</span> <span class="st">"accuracy"</span></span>
<span id="cb1-86"><a href="#cb1-86"></a>)</span>
<span id="cb1-87"><a href="#cb1-87"></a></span>
<span id="cb1-88"><a href="#cb1-88"></a><span class="co"># Run</span></span>
<span id="cb1-89"><a href="#cb1-89"></a>hist &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span></span>
<span id="cb1-90"><a href="#cb1-90"></a><span class="st">  </span><span class="kw"><a href="../../../keras/reference/reexports.html">fit</a></span>(</span>
<span id="cb1-91"><a href="#cb1-91"></a>    x_train,</span>
<span id="cb1-92"><a href="#cb1-92"></a>    y_train,</span>
<span id="cb1-93"><a href="#cb1-93"></a>    <span class="dt">batch_size =</span> batch_size,</span>
<span id="cb1-94"><a href="#cb1-94"></a>    <span class="dt">epochs =</span> epochs,</span>
<span id="cb1-95"><a href="#cb1-95"></a>    <span class="dt">validation_split =</span> <span class="fl">0.1</span></span>
<span id="cb1-96"><a href="#cb1-96"></a>  )</span>
<span id="cb1-97"><a href="#cb1-97"></a></span>
<span id="cb1-98"><a href="#cb1-98"></a><span class="co"># Understanding lime for Keras Embedding Layers</span></span>
<span id="cb1-99"><a href="#cb1-99"></a></span>
<span id="cb1-100"><a href="#cb1-100"></a><span class="co"># In order to explain a text with LIME, we should write a preprocess function</span></span>
<span id="cb1-101"><a href="#cb1-101"></a><span class="co"># which will help to turn words into integers. Therefore, above mentioned steps </span></span>
<span id="cb1-102"><a href="#cb1-102"></a><span class="co"># (how to encode a text) should be repeated BUT within a function. </span></span>
<span id="cb1-103"><a href="#cb1-103"></a><span class="co"># As we already have had a tokenizer object, we can apply the same object to train/test or a new text.</span></span>
<span id="cb1-104"><a href="#cb1-104"></a></span>
<span id="cb1-105"><a href="#cb1-105"></a>get_embedding_explanation &lt;-<span class="st"> </span><span class="cf">function</span>(text) {</span>
<span id="cb1-106"><a href="#cb1-106"></a>  </span>
<span id="cb1-107"><a href="#cb1-107"></a>  tokenizer <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../../keras/reference/fit_text_tokenizer.html">fit_text_tokenizer</a></span>(text)</span>
<span id="cb1-108"><a href="#cb1-108"></a>  </span>
<span id="cb1-109"><a href="#cb1-109"></a>  text_to_seq &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/texts_to_sequences.html">texts_to_sequences</a></span>(tokenizer, text)</span>
<span id="cb1-110"><a href="#cb1-110"></a>  sentences &lt;-<span class="st"> </span>text_to_seq <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../../keras/reference/pad_sequences.html">pad_sequences</a></span>(<span class="dt">maxlen =</span> maxlen)</span>
<span id="cb1-111"><a href="#cb1-111"></a>}</span>
<span id="cb1-112"><a href="#cb1-112"></a></span>
<span id="cb1-113"><a href="#cb1-113"></a></span>
<span id="cb1-114"><a href="#cb1-114"></a><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(lime)</span>
<span id="cb1-115"><a href="#cb1-115"></a></span>
<span id="cb1-116"><a href="#cb1-116"></a><span class="co"># Lets choose some text (3 rows) to explain</span></span>
<span id="cb1-117"><a href="#cb1-117"></a>sentence_to_explain &lt;-<span class="st"> </span>train_sentences<span class="op">$</span>text[<span class="dv">15</span><span class="op">:</span><span class="dv">17</span>]</span>
<span id="cb1-118"><a href="#cb1-118"></a>sentence_to_explain</span>
<span id="cb1-119"><a href="#cb1-119"></a></span>
<span id="cb1-120"><a href="#cb1-120"></a><span class="co"># You could notice that our input is just a plain text. Unlike tabular data, lime function </span></span>
<span id="cb1-121"><a href="#cb1-121"></a><span class="co"># for text classification requires a preprocess fuction. Because it will help to convert a text to integers </span></span>
<span id="cb1-122"><a href="#cb1-122"></a><span class="co"># with provided function. </span></span>
<span id="cb1-123"><a href="#cb1-123"></a>explainer &lt;-<span class="st"> </span><span class="kw"><a href="https://lime.data-imaginist.com/reference/lime.html">lime</a></span>(sentence_to_explain, <span class="dt">model =</span> model, <span class="dt">preprocess =</span> get_embedding_explanation)</span>
<span id="cb1-124"><a href="#cb1-124"></a></span>
<span id="cb1-125"><a href="#cb1-125"></a><span class="co"># Get explanation for the first 10 words</span></span>
<span id="cb1-126"><a href="#cb1-126"></a>explanation &lt;-<span class="st"> </span><span class="kw"><a href="https://dplyr.tidyverse.org/reference/explain.html">explain</a></span>(sentence_to_explain, explainer, <span class="dt">n_labels =</span> <span class="dv">1</span>, <span class="dt">n_features =</span> <span class="dv">10</span>,<span class="dt">n_permutations =</span> <span class="fl">1e4</span>)</span>
<span id="cb1-127"><a href="#cb1-127"></a></span>
<span id="cb1-128"><a href="#cb1-128"></a></span>
<span id="cb1-129"><a href="#cb1-129"></a><span class="co"># Different graphical ways to show the same information</span></span>
<span id="cb1-130"><a href="#cb1-130"></a></span>
<span id="cb1-131"><a href="#cb1-131"></a><span class="kw"><a href="https://lime.data-imaginist.com/reference/text_explanations.html">plot_text_explanations</a></span>(explanation)</span>
<span id="cb1-132"><a href="#cb1-132"></a></span>
<span id="cb1-133"><a href="#cb1-133"></a><span class="kw"><a href="https://lime.data-imaginist.com/reference/plot_features.html">plot_features</a></span>(explanation)</span>
<span id="cb1-134"><a href="#cb1-134"></a></span>
<span id="cb1-135"><a href="#cb1-135"></a><span class="kw"><a href="https://lime.data-imaginist.com/reference/interactive_text_explanations.html">interactive_text_explanations</a></span>(explainer)</span></code></pre></div>
