---
title: "Keras Examples"
---



<table>
<colgroup>
<col width="37%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th>Example</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="/guide/keras/examples/addition_rnn">addition_rnn</a></td>
<td>Implementation of sequence to sequence learning for performing addition of two numbers (as strings).</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/babi_memnn">babi_memnn</a></td>
<td>Trains a memory network on the bAbI dataset for reading comprehension.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/babi_rnn">babi_rnn</a></td>
<td>Trains a two-branch recurrent network on the bAbI dataset for reading comprehension.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/cifar10_cnn">cifar10_cnn</a></td>
<td>Trains a simple deep CNN on the CIFAR10 small images dataset.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/cifar10_densenet">cifar10_densenet</a></td>
<td>Trains a DenseNet-40-12 on the CIFAR10 small images dataset.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/conv_lstm">conv_lstm</a></td>
<td>Demonstrates the use of a convolutional LSTM network.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/deep_dream">deep_dream</a></td>
<td>Deep Dreams in Keras.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/eager_dcgan">eager_dcgan</a></td>
<td>Generating digits with generative adversarial networks and eager execution.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/eager_image_captioning">eager_image_captioning</a></td>
<td>Generating image captions with Keras and eager execution.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/eager_pix2pix">eager_pix2pix</a></td>
<td>Image-to-image translation with Pix2Pix, using eager execution.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/eager_styletransfer">eager_styletransfer</a></td>
<td>Neural style transfer with eager execution.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/fine_tuning">fine_tuning</a></td>
<td>Fine tuning of a image classification model.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/imdb_bidirectional_lstm">imdb_bidirectional_lstm</a></td>
<td>Trains a Bidirectional LSTM on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/imdb_cnn">imdb_cnn</a></td>
<td>Demonstrates the use of Convolution1D for text classification.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/imdb_cnn_lstm">imdb_cnn_lstm</a></td>
<td>Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/imdb_fasttext">imdb_fasttext</a></td>
<td>Trains a FastText model on the IMDB sentiment classification task.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/imdb_lstm">imdb_lstm</a></td>
<td>Trains a LSTM on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/lstm_text_generation">lstm_text_generation</a></td>
<td>Generates text from Nietzsche’s writings.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/lstm_seq2seq">lstm_seq2seq</a></td>
<td>This script demonstrates how to implement a basic character-level sequence-to-sequence model.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/mnist_acgan">mnist_acgan</a></td>
<td>Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/mnist_antirectifier">mnist_antirectifier</a></td>
<td>Demonstrates how to write custom layers for Keras</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/mnist_cnn">mnist_cnn</a></td>
<td>Trains a simple convnet on the MNIST dataset.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/mnist_cnn_embeddings">mnist_cnn_embeddings</a></td>
<td>Demonstrates how to visualize embeddings in TensorBoard.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/mnist_irnn">mnist_irnn</a></td>
<td>Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/mnist_mlp">mnist_mlp</a></td>
<td>Trains a simple deep multi-layer perceptron on the MNIST dataset.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/mnist_hierarchical_rnn">mnist_hierarchical_rnn</a></td>
<td>Trains a Hierarchical RNN (HRNN) to classify MNIST digits.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/mnist_tfrecord">mnist_tfrecord</a></td>
<td>MNIST dataset with TFRecords, the standard TensorFlow data format.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/mnist_transfer_cnn">mnist_transfer_cnn</a></td>
<td>Transfer learning toy example.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/neural_style_transfer">neural_style_transfer</a></td>
<td>Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/nmt_attention">nmt_attention</a></td>
<td>Neural machine translation with an attention mechanism.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/quora_siamese_lstm">quora_siamese_lstm</a></td>
<td>Classifying duplicate quesitons from Quora using Siamese Recurrent Architecture.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/reuters_mlp">reuters_mlp</a></td>
<td>Trains and evaluatea a simple MLP on the Reuters newswire topic classification task.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/stateful_lstm">stateful_lstm</a></td>
<td>Demonstrates how to use stateful RNNs to model long sequences efficiently.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/text_explanation_lime">text_explanation_lime</a></td>
<td>How to use lime to explain text data.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/variational_autoencoder">variational_autoencoder</a></td>
<td>Demonstrates how to build a variational autoencoder.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/variational_autoencoder_deconv">variational_autoencoder_deconv</a></td>
<td>Demonstrates how to build a variational autoencoder with Keras using deconvolution layers.</td>
</tr>
<tr class="odd">
<td><a href="/guide/keras/examples/tfprob_vae">tfprob_vae</a></td>
<td>A variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST.</td>
</tr>
<tr class="even">
<td><a href="/guide/keras/examples/vq_vae">vq_vae</a></td>
<td>Discrete Representation Learning with VQ-VAE and TensorFlow Probability.</td>
</tr>
</tbody>
</table>
<!--
| [cifar10_resnet](cifar10_resnet) | Trains a ResNet on the CIFAR10 dataset. |
| [conv_filter_visualization](conv_filter_visualization) | Visualization of the filters of VGG16, via gradient ascent in input space. |
| [image_ocr](image_ocr) | Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). |
| [lstm_benchmark](lstm_benchmark) | Compares different LSTM implementations on the IMDB sentiment classification task. |
| [mnist_dataset_api](mnist_dataset_api) | MNIST classification with TensorFlow's Dataset API. |
| [mnist_net2net](mnist_net2net) | Reproduction of the Net2Net experiment with MNIST in "Net2Net: Accelerating Learning via Knowledge Transfer". |
| [mnist_siamese_graph](mnist_siamese_graph) | Trains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. |
| [mnist_swwae](mnist_swwae) | Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. |
| [neural_doodle](neural_doodle) | Neural doodle. |
| [reuters_mlp_relu_vs_selu](reuters_mlp_relu_vs_selu) | Compares self-normalizing MLPs with regular MLPs.  |
| [lstm_stateful](lstm_stateful) | Example script showing how to use a stateful LSTM model
+and how its stateless counterpart performs. |
--><!--

These examples are complete however don't yet work properly (see inline comments in 
scripts for details) so we aren't listing them.


| [pretrained_word_embeddings](pretrained_word_embeddings) | Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. |

-->
