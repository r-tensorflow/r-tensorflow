---
title: "Keras Examples"
---

| Example  | Description   |
|----------------------|------------------------------------|
| [addition_rnn](addition_rnn.html) | Implementation of sequence to sequence learning for performing addition of two numbers (as strings). |
| [babi_memnn](babi_memnn.html) | Trains a memory network on the bAbI dataset for reading comprehension. |
| [babi_rnn](babi_rnn.html) | Trains a two-branch recurrent network on the bAbI dataset for reading comprehension. |
| [cifar10_cnn](cifar10_cnn.html) | Trains a simple deep CNN on the CIFAR10 small images dataset. |
| [cifar10_densenet](cifar10_densenet.html) | Trains a DenseNet-40-12 on the CIFAR10 small images dataset. |
| [conv_lstm](conv_lstm.html) | Demonstrates the use of a convolutional LSTM network. |
| [deep_dream](deep_dream.html) | Deep Dreams in Keras. |
| [eager_dcgan](eager_dcgan.html) | Generating digits with generative adversarial networks and eager execution. |
| [eager_image_captioning](eager_image_captioning.html) | Generating image captions with Keras and eager execution. |
| [eager_pix2pix](eager_pix2pix.html) | Image-to-image translation with Pix2Pix, using eager execution. |
| [eager_styletransfer](eager_styletransfer.html) | Neural style transfer with eager execution. |
| [fine_tuning](fine_tuning.html) | Fine tuning of a image classification model. | 
| [imdb_bidirectional_lstm](imdb_bidirectional_lstm.html) | Trains a Bidirectional LSTM on the IMDB sentiment classification task. |
| [imdb_cnn](imdb_cnn.html) | Demonstrates the use of Convolution1D for text classification. |
| [imdb_cnn_lstm](imdb_cnn_lstm.html) | Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task. |
| [imdb_fasttext](imdb_fasttext.html) | Trains a FastText model on the IMDB sentiment classification task. |
| [imdb_lstm](imdb_lstm.html) | Trains a LSTM on the IMDB sentiment classification task. |
| [lstm_text_generation](lstm_text_generation.html) | Generates text from Nietzsche's writings. |
| [lstm_seq2seq](lstm_seq2seq.html) | This script demonstrates how to implement a basic character-level sequence-to-sequence model. |
| [mnist_acgan](mnist_acgan.html) | Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset |
| [mnist_antirectifier](mnist_antirectifier.html) | Demonstrates how to write custom layers for Keras |
| [mnist_cnn](mnist_cnn.html) | Trains a simple convnet on the MNIST dataset. |
| [mnist_cnn_embeddings](mnist_cnn_embeddings.html) | Demonstrates how to visualize embeddings in TensorBoard. |
| [mnist_irnn](mnist_irnn.html) | Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in "A Simple Way to Initialize Recurrent Networks of Rectified Linear Units" by Le et al. |
| [mnist_mlp](mnist_mlp.html) | Trains a simple deep multi-layer perceptron on the MNIST dataset. |
| [mnist_hierarchical_rnn](mnist_hierarchical_rnn.html) | Trains a Hierarchical RNN (HRNN) to classify MNIST digits. |
| [mnist_tfrecord](mnist_tfrecord.html) | MNIST dataset with TFRecords, the standard TensorFlow data format. |
| [mnist_transfer_cnn](mnist_transfer_cnn.html) | Transfer learning toy example. |
| [neural_style_transfer](neural_style_transfer.html) | Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture). |
| [nmt_attention](nmt_attention.html) | Neural machine translation with an attention mechanism. |
| [quora_siamese_lstm](quora_siamese_lstm.html) | Classifying duplicate quesitons from Quora using Siamese Recurrent Architecture. | 
| [reuters_mlp](reuters_mlp.html) | Trains and evaluatea a simple MLP on the Reuters newswire topic classification task. |
| [stateful_lstm](stateful_lstm.html) | Demonstrates how to use stateful RNNs to model long sequences efficiently. |
| [text_explanation_lime](text_explanation_lime.html) | How to use lime to explain text data. |
| [variational_autoencoder](variational_autoencoder.html) | Demonstrates how to build a variational autoencoder. |
| [variational_autoencoder_deconv](variational_autoencoder_deconv.html) | Demonstrates how to build a variational autoencoder with Keras using deconvolution layers. |
| [tfprob_vae](tfprob_vae.html) | A variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST. |
| [vq_vae](vq_vae.html) | Discrete Representation Learning with VQ-VAE and TensorFlow Probability. |

<!--
| [cifar10_resnet](cifar10_resnet.html) | Trains a ResNet on the CIFAR10 dataset. |
| [conv_filter_visualization](conv_filter_visualization.html) | Visualization of the filters of VGG16, via gradient ascent in input space. |
| [image_ocr](image_ocr.html) | Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). |
| [lstm_benchmark](lstm_benchmark.html) | Compares different LSTM implementations on the IMDB sentiment classification task. |
| [mnist_dataset_api](mnist_dataset_api.html) | MNIST classification with TensorFlow's Dataset API. |
| [mnist_net2net](mnist_net2net.html) | Reproduction of the Net2Net experiment with MNIST in "Net2Net: Accelerating Learning via Knowledge Transfer". |
| [mnist_siamese_graph](mnist_siamese_graph.html) | Trains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. |
| [mnist_swwae](mnist_swwae.html) | Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. |
| [neural_doodle](neural_doodle.html) | Neural doodle. |
| [reuters_mlp_relu_vs_selu](reuters_mlp_relu_vs_selu.html) | Compares self-normalizing MLPs with regular MLPs.  |
| [lstm_stateful](lstm_stateful.html) | Example script showing how to use a stateful LSTM model
+and how its stateless counterpart performs. |
-->


<!--

These examples are complete however don't yet work properly (see inline comments in 
scripts for details) so we aren't listing them.


| [pretrained_word_embeddings](pretrained_word_embeddings.html) | Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. |

-->










