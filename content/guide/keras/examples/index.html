---
title: "Keras Examples"
---



<table>
<colgroup>
<col width="37%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th>Example</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="addition_rnn.html">addition_rnn</a></td>
<td>Implementation of sequence to sequence learning for performing addition of two numbers (as strings).</td>
</tr>
<tr class="even">
<td><a href="babi_memnn.html">babi_memnn</a></td>
<td>Trains a memory network on the bAbI dataset for reading comprehension.</td>
</tr>
<tr class="odd">
<td><a href="babi_rnn.html">babi_rnn</a></td>
<td>Trains a two-branch recurrent network on the bAbI dataset for reading comprehension.</td>
</tr>
<tr class="even">
<td><a href="cifar10_cnn.html">cifar10_cnn</a></td>
<td>Trains a simple deep CNN on the CIFAR10 small images dataset.</td>
</tr>
<tr class="odd">
<td><a href="cifar10_densenet.html">cifar10_densenet</a></td>
<td>Trains a DenseNet-40-12 on the CIFAR10 small images dataset.</td>
</tr>
<tr class="even">
<td><a href="conv_lstm.html">conv_lstm</a></td>
<td>Demonstrates the use of a convolutional LSTM network.</td>
</tr>
<tr class="odd">
<td><a href="deep_dream.html">deep_dream</a></td>
<td>Deep Dreams in Keras.</td>
</tr>
<tr class="even">
<td><a href="eager_dcgan.html">eager_dcgan</a></td>
<td>Generating digits with generative adversarial networks and eager execution.</td>
</tr>
<tr class="odd">
<td><a href="eager_image_captioning.html">eager_image_captioning</a></td>
<td>Generating image captions with Keras and eager execution.</td>
</tr>
<tr class="even">
<td><a href="eager_pix2pix.html">eager_pix2pix</a></td>
<td>Image-to-image translation with Pix2Pix, using eager execution.</td>
</tr>
<tr class="odd">
<td><a href="eager_styletransfer.html">eager_styletransfer</a></td>
<td>Neural style transfer with eager execution.</td>
</tr>
<tr class="even">
<td><a href="fine_tuning.html">fine_tuning</a></td>
<td>Fine tuning of a image classification model.</td>
</tr>
<tr class="odd">
<td><a href="imdb_bidirectional_lstm.html">imdb_bidirectional_lstm</a></td>
<td>Trains a Bidirectional LSTM on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="imdb_cnn.html">imdb_cnn</a></td>
<td>Demonstrates the use of Convolution1D for text classification.</td>
</tr>
<tr class="odd">
<td><a href="imdb_cnn_lstm.html">imdb_cnn_lstm</a></td>
<td>Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="imdb_fasttext.html">imdb_fasttext</a></td>
<td>Trains a FastText model on the IMDB sentiment classification task.</td>
</tr>
<tr class="odd">
<td><a href="imdb_lstm.html">imdb_lstm</a></td>
<td>Trains a LSTM on the IMDB sentiment classification task.</td>
</tr>
<tr class="even">
<td><a href="lstm_text_generation.html">lstm_text_generation</a></td>
<td>Generates text from Nietzsche’s writings.</td>
</tr>
<tr class="odd">
<td><a href="lstm_seq2seq.html">lstm_seq2seq</a></td>
<td>This script demonstrates how to implement a basic character-level sequence-to-sequence model.</td>
</tr>
<tr class="even">
<td><a href="mnist_acgan.html">mnist_acgan</a></td>
<td>Implementation of AC-GAN (Auxiliary Classifier GAN ) on the MNIST dataset</td>
</tr>
<tr class="odd">
<td><a href="mnist_antirectifier.html">mnist_antirectifier</a></td>
<td>Demonstrates how to write custom layers for Keras</td>
</tr>
<tr class="even">
<td><a href="mnist_cnn.html">mnist_cnn</a></td>
<td>Trains a simple convnet on the MNIST dataset.</td>
</tr>
<tr class="odd">
<td><a href="mnist_cnn_embeddings.html">mnist_cnn_embeddings</a></td>
<td>Demonstrates how to visualize embeddings in TensorBoard.</td>
</tr>
<tr class="even">
<td><a href="mnist_irnn.html">mnist_irnn</a></td>
<td>Reproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al.</td>
</tr>
<tr class="odd">
<td><a href="mnist_mlp.html">mnist_mlp</a></td>
<td>Trains a simple deep multi-layer perceptron on the MNIST dataset.</td>
</tr>
<tr class="even">
<td><a href="mnist_hierarchical_rnn.html">mnist_hierarchical_rnn</a></td>
<td>Trains a Hierarchical RNN (HRNN) to classify MNIST digits.</td>
</tr>
<tr class="odd">
<td><a href="mnist_tfrecord.html">mnist_tfrecord</a></td>
<td>MNIST dataset with TFRecords, the standard TensorFlow data format.</td>
</tr>
<tr class="even">
<td><a href="mnist_transfer_cnn.html">mnist_transfer_cnn</a></td>
<td>Transfer learning toy example.</td>
</tr>
<tr class="odd">
<td><a href="neural_style_transfer.html">neural_style_transfer</a></td>
<td>Neural style transfer (generating an image with the same “content” as a base image, but with the “style” of a different picture).</td>
</tr>
<tr class="even">
<td><a href="nmt_attention.html">nmt_attention</a></td>
<td>Neural machine translation with an attention mechanism.</td>
</tr>
<tr class="odd">
<td><a href="quora_siamese_lstm.html">quora_siamese_lstm</a></td>
<td>Classifying duplicate quesitons from Quora using Siamese Recurrent Architecture.</td>
</tr>
<tr class="even">
<td><a href="reuters_mlp.html">reuters_mlp</a></td>
<td>Trains and evaluatea a simple MLP on the Reuters newswire topic classification task.</td>
</tr>
<tr class="odd">
<td><a href="stateful_lstm.html">stateful_lstm</a></td>
<td>Demonstrates how to use stateful RNNs to model long sequences efficiently.</td>
</tr>
<tr class="even">
<td><a href="text_explanation_lime.html">text_explanation_lime</a></td>
<td>How to use lime to explain text data.</td>
</tr>
<tr class="odd">
<td><a href="variational_autoencoder.html">variational_autoencoder</a></td>
<td>Demonstrates how to build a variational autoencoder.</td>
</tr>
<tr class="even">
<td><a href="variational_autoencoder_deconv.html">variational_autoencoder_deconv</a></td>
<td>Demonstrates how to build a variational autoencoder with Keras using deconvolution layers.</td>
</tr>
<tr class="odd">
<td><a href="tfprob_vae.html">tfprob_vae</a></td>
<td>A variational autoencoder using TensorFlow Probability on Kuzushiji-MNIST.</td>
</tr>
<tr class="even">
<td><a href="vq_vae.html">vq_vae</a></td>
<td>Discrete Representation Learning with VQ-VAE and TensorFlow Probability.</td>
</tr>
</tbody>
</table>
<!--
| [cifar10_resnet](cifar10_resnet.html) | Trains a ResNet on the CIFAR10 dataset. |
| [conv_filter_visualization](conv_filter_visualization.html) | Visualization of the filters of VGG16, via gradient ascent in input space. |
| [image_ocr](image_ocr.html) | Trains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). |
| [lstm_benchmark](lstm_benchmark.html) | Compares different LSTM implementations on the IMDB sentiment classification task. |
| [mnist_dataset_api](mnist_dataset_api.html) | MNIST classification with TensorFlow's Dataset API. |
| [mnist_net2net](mnist_net2net.html) | Reproduction of the Net2Net experiment with MNIST in "Net2Net: Accelerating Learning via Knowledge Transfer". |
| [mnist_siamese_graph](mnist_siamese_graph.html) | Trains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. |
| [mnist_swwae](mnist_swwae.html) | Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. |
| [neural_doodle](neural_doodle.html) | Neural doodle. |
| [reuters_mlp_relu_vs_selu](reuters_mlp_relu_vs_selu.html) | Compares self-normalizing MLPs with regular MLPs.  |
| [lstm_stateful](lstm_stateful.html) | Example script showing how to use a stateful LSTM model
+and how its stateless counterpart performs. |
--><!--

These examples are complete however don't yet work properly (see inline comments in 
scripts for details) so we aren't listing them.


| [pretrained_word_embeddings](pretrained_word_embeddings.html) | Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. |

-->
