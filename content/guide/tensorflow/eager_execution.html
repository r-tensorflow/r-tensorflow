---
title: "Eager execution"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Eager execution} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteEncoding{UTF-8}
type: "docs"
menu:
  main:
    name: "Eager execution"
    identifier: "custom-basic-eager-execution"
    parent: "custom-basic-top"
    weight: 10
aliases:
  - /tensorflow/
  - /tensorflow/articles/basic_usage.html
  - /tensorflow/articles/using_tensorflow_api.html
---



<p>TensorFlow’s eager execution is an imperative programming environment that
evaluates operations immediately, without building graphs: operations return
concrete values instead of constructing a computational graph to run later. This
makes it easy to get started with TensorFlow and debug models, and it
reduces boilerplate as well. To follow along with this guide, run the code
samples below in an interactive <code>R</code> interpreter.</p>
<p>Eager execution is a flexible machine learning platform for research and
experimentation, providing:</p>
<ul>
<li>
<em>An intuitive interface</em>—Structure your code naturally and use R data
structures. Quickly iterate on small models and small data.</li>
<li>
<em>Easier debugging</em>—Call ops directly to inspect running models and test
changes. Use standard R debugging tools for immediate error reporting.</li>
<li>
<em>Natural control flow</em>—Use R control flow instead of graph control
flow, simplifying the specification of dynamic models.</li>
</ul>
<p>Eager execution supports most TensorFlow operations and GPU acceleration.</p>
<p>Note: Some models may experience increased overhead with eager execution
enabled. Performance improvements are ongoing, but please
<a href="https://github.com/tensorflow/tensorflow/issues">file a bug</a> if you find a
problem and share your benchmarks.</p>
<div id="setup-and-basic-usage" class="section level2">
<h2>Setup and basic usage</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tensorflow)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tfautograph)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(keras)</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span>(tfdatasets)</a></code></pre></div>
<p>In Tensorflow 2.0, eager execution is enabled by default.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">tf<span class="op">$</span><span class="kw">executing_eagerly</span>()</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Now you can run TensorFlow operations and the results will return immediately:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">1</span>, <span class="dt">nrow =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">m &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">matmul</span>(x, x)</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">m</a></code></pre></div>
<pre><code>## tf.Tensor([[4.]], shape=(1, 1), dtype=float64)</code></pre>
<p>Enabling eager execution changes how TensorFlow operations behave—now they
immediately evaluate and return their values to R <code>tf$Tensor</code> objects
reference concrete values instead of symbolic handles to nodes in a computational
graph. Since there isn’t a computational graph to build and run later in a
session, it’s easy to inspect results using <code><a href="https://rdrr.io/r/base/print.html">print()</a></code> or a debugger. Evaluating,
printing, and checking tensor values does not break the flow for computing
gradients.</p>
<p>Eager execution works nicely with R. TensorFlow
<a href="https://www.tensorflow.org/api_guides/python/math_ops">math operations</a> convert
R objects and R arrays to <code>tf$Tensor</code> objects. The
<code>as.array</code> method returns the object’s value as an R <code>array</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1">a &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(<span class="kw"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>), <span class="dt">ncol =</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">a</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[1. 3.]
##  [2. 4.]], shape=(2, 2), dtype=float64)</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># Broadcasting support</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">b &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">add</span>(a, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">b</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[2. 4.]
##  [3. 5.]], shape=(2, 2), dtype=float64)</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Operator overloading is supported</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2">a <span class="op">*</span><span class="st"> </span>b</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[ 2. 12.]
##  [ 6. 20.]], shape=(2, 2), dtype=float64)</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># Obtain an R value from a Tensor</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/array.html">as.array</a></span>(a)</a></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1    3
## [2,]    2    4</code></pre>
</div>
<div id="dynamic-control-flow" class="section level2">
<h2>Dynamic control flow</h2>
<p>A major benefit of eager execution is that all the functionality of the host
language is available while your model is executing. So, for example,
it is easy to write <a href="https://en.wikipedia.org/wiki/Fizz_buzz">fizzbuzz</a>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">fizzbuzz &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/pkg/tfautograph/man/autograph.html">autograph</a></span>(<span class="cf">function</span>(max_num) {</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">  counter &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">  max_num &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">convert_to_tensor</span>(max_num)</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">  <span class="cf">for</span> (num <span class="cf">in</span> (tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/range.html">range</a></span>(max_num) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)) {</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">    <span class="cf">if</span> ((num <span class="op">%%</span><span class="st"> </span><span class="dv">3</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) <span class="op">&amp;</span><span class="st"> </span>(num <span class="op">%%</span><span class="st"> </span><span class="dv">5</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)) {</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">      tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="st">"FizzBuzz"</span>)</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">    } <span class="cf">else</span> <span class="cf">if</span> (num <span class="op">%%</span><span class="st"> </span><span class="dv">3</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">      tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="st">"Fizz"</span>)</a>
<a class="sourceLine" id="cb14-9" data-line-number="9">    } <span class="cf">else</span> <span class="cf">if</span> (num <span class="op">%%</span><span class="st"> </span><span class="dv">5</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">      tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="st">"Buzz"</span>)</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb14-12" data-line-number="12">      tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(num)</a>
<a class="sourceLine" id="cb14-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb14-14" data-line-number="14">    counter &lt;-<span class="st"> </span>counter <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb14-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb14-16" data-line-number="16">})</a>
<a class="sourceLine" id="cb14-17" data-line-number="17"><span class="kw">fizzbuzz</span>(<span class="dv">15</span>)</a></code></pre></div>
<p>This has conditionals that depend on tensor values and it prints these values
at runtime.</p>
</div>
<div id="eager-training" class="section level2">
<h2>Eager training</h2>
<div id="computing-gradients" class="section level3">
<h3>Computing gradients</h3>
<p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a>
is useful for implementing machine learning algorithms such as
<a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> for training
neural networks. During eager execution, use <code>tf$GradientTape</code> to trace
operations for computing gradients later.</p>
<p>You can use <code>tf$GradientTape</code> to train and/or compute gradients in eager. It is especially useful for complicated training loops.</p>
<p>Since different operations can occur during each call, all
forward-pass operations get recorded to a “tape”. To compute the gradient, play
the tape backwards and then discard. A particular <code>tf$GradientTape</code> can only
compute one gradient; subsequent calls throw a runtime error.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">w &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">  loss &lt;-<span class="st"> </span>w <span class="op">*</span><span class="st"> </span>w</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">})</a>
<a class="sourceLine" id="cb15-5" data-line-number="5">grad &lt;-<span class="st"> </span>tape<span class="op">$</span><span class="kw">gradient</span>(loss, w)</a>
<a class="sourceLine" id="cb15-6" data-line-number="6">grad</a></code></pre></div>
<pre><code>## tf.Tensor(2.0, shape=(), dtype=float32)</code></pre>
</div>
<div id="train-a-model" class="section level3">
<h3>Train a model</h3>
<p>The following example creates a multi-layer model that classifies the standard
MNIST handwritten digits. It demonstrates the optimizer and layer APIs to build
trainable graphs in an eager execution environment.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Fetch and format the mnist data</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">mnist &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/dataset_mnist.html">dataset_mnist</a></span>()</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">dataset &lt;-<span class="st"> </span><span class="kw"><a href="../../tools/tfdatasets/reference/tensor_slices_dataset.html">tensor_slices_dataset</a></span>(mnist<span class="op">$</span>train) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/dataset_map.html">dataset_map</a></span>(<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">    x<span class="op">$</span>x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">cast</span>(x<span class="op">$</span>x, tf<span class="op">$</span>float32)<span class="op">/</span><span class="dv">255</span></a>
<a class="sourceLine" id="cb17-6" data-line-number="6">    x<span class="op">$</span>x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">expand_dims</span>(x<span class="op">$</span>x, <span class="dt">axis =</span> <span class="op">-</span>1L)</a>
<a class="sourceLine" id="cb17-7" data-line-number="7">    <span class="kw"><a href="https://rdrr.io/r/base/unname.html">unname</a></span>(x)</a>
<a class="sourceLine" id="cb17-8" data-line-number="8">  }) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/dataset_shuffle.html">dataset_shuffle</a></span>(<span class="dv">1000</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-10" data-line-number="10"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/dataset_batch.html">dataset_batch</a></span>(<span class="dv">32</span>)</a>
<a class="sourceLine" id="cb17-11" data-line-number="11"></a>
<a class="sourceLine" id="cb17-12" data-line-number="12">dataset</a></code></pre></div>
<pre><code>## &lt;BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int32)&gt;</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">mnist_model &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/keras_model_sequential.html">keras_model_sequential</a></span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_conv_2d.html">layer_conv_2d</a></span>(<span class="dt">filters =</span> <span class="dv">16</span>, <span class="dt">kernel_size =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">activation=</span> <span class="st">"relu"</span>,</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">                <span class="dt">input_shape =</span> <span class="kw"><a href="../../tensorflow/reference/shape.html">shape</a></span>(<span class="ot">NULL</span>, <span class="ot">NULL</span>, <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb19-4" data-line-number="4"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_conv_2d.html">layer_conv_2d</a></span>(<span class="dt">filters =</span> <span class="dv">16</span>, <span class="dt">kernel_size =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">activation =</span> <span class="st">"relu"</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb19-5" data-line-number="5"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_global_average_pooling_2d.html">layer_global_average_pooling_2d</a></span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb19-6" data-line-number="6"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>)</a></code></pre></div>
<p>Even without training, call the model and inspect the output in eager execution:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1">el &lt;-<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/dataset_take.html">dataset_take</a></span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb20-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../tools/tfdatasets/reference/dataset_collect.html">dataset_collect</a></span>()</a>
<a class="sourceLine" id="cb20-4" data-line-number="4"><span class="kw">mnist_model</span>(el[[<span class="dv">1</span>]])</a></code></pre></div>
<pre><code>## tf.Tensor(
## [[-0.0671434  -0.00228795  0.02841645  0.02321084 -0.00794387  0.04093533
##   -0.02603724 -0.00587457 -0.07075977 -0.0075227 ]
##  [-0.05735591 -0.00278001  0.02121549  0.02073979 -0.0073895   0.03433985
##   -0.01956675 -0.00753038 -0.06026411 -0.00651871]
##  [-0.05922092 -0.00266138  0.02498602  0.02111664 -0.00641833  0.03566717
##   -0.02121106 -0.00586844 -0.06226477 -0.00585287]
##  [-0.05554833 -0.00311579  0.02061605  0.01941078 -0.0065577   0.03258931
##   -0.01862561 -0.00789848 -0.05806542 -0.0061333 ]
##  [-0.07753677 -0.00449379  0.03206751  0.02830365 -0.0091237   0.04731926
##   -0.02736748 -0.00844761 -0.08164465 -0.00857282]
##  [-0.07391085 -0.00383051  0.02897366  0.02650399 -0.01016813  0.04452075
##   -0.02470635 -0.00925736 -0.07737272 -0.00782041]
##  [-0.0416727  -0.00087853  0.01617335  0.01545249 -0.00666692  0.02526582
##   -0.01454522 -0.00466756 -0.0437831  -0.00388646]
##  [-0.07230031 -0.00424013  0.02976754  0.02780659 -0.00860732  0.04406357
##   -0.02509249 -0.00751798 -0.07629873 -0.00727781]
##  [-0.04162367 -0.00279384  0.01170409  0.01315817 -0.00499912  0.02393872
##   -0.011888   -0.00976758 -0.0432532  -0.00504208]
##  [-0.04640616 -0.0015676   0.01399253  0.01445379 -0.00887941  0.02597157
##   -0.01438685 -0.00955507 -0.04716753 -0.00588906]
##  [-0.06403283 -0.00380265  0.02124093  0.02455575 -0.00814745  0.03761182
##   -0.02031207 -0.0098445  -0.06772884 -0.00634012]
##  [-0.04491066 -0.00192055  0.01373675  0.01568967 -0.00717469  0.02523806
##   -0.01337731 -0.00829298 -0.0464369  -0.004538  ]
##  [-0.06461196 -0.00583145  0.02437255  0.0219736  -0.00655315  0.03748057
##   -0.02119313 -0.01055494 -0.06730501 -0.00920799]
##  [-0.05014614 -0.00302279  0.01669382  0.01729116 -0.00653267  0.02888334
##   -0.01565277 -0.00878802 -0.05200947 -0.00543689]
##  [-0.03845144 -0.0002479   0.01098046  0.01292856 -0.00780198  0.02184045
##   -0.01012554 -0.00930404 -0.03896985 -0.00332377]
##  [-0.06675228 -0.00321569  0.02766551  0.02453372 -0.0071499   0.04090613
##   -0.02436337 -0.0063199  -0.07087417 -0.00724227]
##  [-0.0601491  -0.00264775  0.01701863  0.02040021 -0.01031642  0.03322306
##   -0.01726531 -0.01271318 -0.06157894 -0.00702024]
##  [-0.06843022 -0.00347692  0.0289554   0.02460677 -0.00738955  0.0423151
##   -0.02450552 -0.00665112 -0.07262111 -0.00726911]
##  [-0.05609158 -0.0027346   0.01785816  0.01743979 -0.00744173  0.032259
##   -0.01637739 -0.01152581 -0.05779178 -0.00572831]
##  [-0.06835417 -0.00294489  0.02454765  0.02362916 -0.00966506  0.04024933
##   -0.02279488 -0.01027685 -0.07080611 -0.00776874]
##  [-0.08727618 -0.00571118  0.03754998  0.03430922 -0.00924086  0.05332832
##   -0.03072381 -0.00860722 -0.09205267 -0.00947476]
##  [-0.04829386 -0.00305132  0.01601653  0.01721095 -0.00682189  0.02767116
##   -0.01456647 -0.0080084  -0.05020818 -0.00501562]
##  [-0.04422441 -0.0020681   0.01202022  0.01559719 -0.00738829  0.02439935
##   -0.01275103 -0.00930172 -0.04516624 -0.00487364]
##  [-0.08378849 -0.00576995  0.03417671  0.03234697 -0.00821263  0.05119862
##   -0.02927407 -0.00952488 -0.08916904 -0.00947819]
##  [-0.04724927 -0.00235133  0.0155276   0.01600004 -0.0072073   0.02655366
##   -0.01544726 -0.0078605  -0.04897327 -0.00568147]
##  [-0.06688089 -0.00653574  0.02568465  0.02217169 -0.00722214  0.03784863
##   -0.02210061 -0.01147156 -0.06900088 -0.01069103]
##  [-0.04264261 -0.0024927   0.01266358  0.01261594 -0.00518019  0.02425985
##   -0.01111003 -0.01041172 -0.04352606 -0.00393509]
##  [-0.04323068 -0.00095108  0.01678802  0.01518362 -0.00709633  0.02574279
##   -0.01492713 -0.00525879 -0.0449748  -0.00424648]
##  [-0.04179835 -0.00159111  0.00976408  0.01302196 -0.00738085  0.02357196
##   -0.00986397 -0.01212141 -0.04254007 -0.00415885]
##  [-0.0285618  -0.00066778  0.00675424  0.00955213 -0.00605164  0.01549255
##   -0.0070022  -0.00776743 -0.0288702  -0.00312968]
##  [-0.07178091 -0.00792911  0.02239588  0.02256102 -0.0084713   0.03762921
##   -0.02235969 -0.01710764 -0.0731018  -0.01393417]
##  [-0.05738646 -0.00389011  0.0211005   0.01745564 -0.00699965  0.03223253
##   -0.01927477 -0.01062902 -0.05824104 -0.00813134]], shape=(32, 10), dtype=float32)</code></pre>
<p>While keras models have a builtin training loop (using the fit method), sometimes you need more customization. Here’s an example, of a training loop implemented with eager:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">optimizer &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/optimizer_adam.html">optimizer_adam</a></span>()</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">loss_object &lt;-<span class="st"> </span>tf<span class="op">$</span>keras<span class="op">$</span>losses<span class="op">$</span><span class="kw">SparseCategoricalCrossentropy</span>(<span class="dt">from_logits =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3"></a>
<a class="sourceLine" id="cb22-4" data-line-number="4">loss_history &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>()</a></code></pre></div>
<p>Note: Use the assert functions in <code>tf$debugging</code> to check if a condition holds up. This works in eager and graph execution.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">train_step &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels) {</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">    logits &lt;-<span class="st"> </span><span class="kw">mnist_model</span>(images, <span class="dt">training =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb23-4" data-line-number="4">    tf<span class="op">$</span>debugging<span class="op">$</span><span class="kw">assert_equal</span>(logits<span class="op">$</span>shape, <span class="kw"><a href="../../tensorflow/reference/shape.html">shape</a></span>(<span class="dv">32</span>, <span class="dv">10</span>))</a>
<a class="sourceLine" id="cb23-5" data-line-number="5">    loss_value &lt;-<span class="st"> </span><span class="kw">loss_object</span>(labels, logits)</a>
<a class="sourceLine" id="cb23-6" data-line-number="6">  })</a>
<a class="sourceLine" id="cb23-7" data-line-number="7">  loss_history &lt;&lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/append.html">append</a></span>(loss_history, loss_value)</a>
<a class="sourceLine" id="cb23-8" data-line-number="8">  grads &lt;-<span class="st"> </span>tape<span class="op">$</span><span class="kw">gradient</span>(loss_value, mnist_model<span class="op">$</span>trainable_variables)</a>
<a class="sourceLine" id="cb23-9" data-line-number="9">  optimizer<span class="op">$</span><span class="kw">apply_gradients</span>(</a>
<a class="sourceLine" id="cb23-10" data-line-number="10">    purrr<span class="op">::</span><span class="kw"><a href="https://purrr.tidyverse.org/reference/transpose.html">transpose</a></span>(<span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(grads, mnist_model<span class="op">$</span>trainable_variables))</a>
<a class="sourceLine" id="cb23-11" data-line-number="11">  )</a>
<a class="sourceLine" id="cb23-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb23-13" data-line-number="13"></a>
<a class="sourceLine" id="cb23-14" data-line-number="14">train &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/pkg/tfautograph/man/autograph.html">autograph</a></span>(<span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb23-15" data-line-number="15">  <span class="cf">for</span> (epoch <span class="cf">in</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span>(<span class="dv">3</span>)) {</a>
<a class="sourceLine" id="cb23-16" data-line-number="16">    <span class="cf">for</span> (batch <span class="cf">in</span> dataset) {</a>
<a class="sourceLine" id="cb23-17" data-line-number="17">      <span class="kw">train_step</span>(batch[[<span class="dv">1</span>]], batch[[<span class="dv">2</span>]])</a>
<a class="sourceLine" id="cb23-18" data-line-number="18">    }</a>
<a class="sourceLine" id="cb23-19" data-line-number="19">    tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/print.html">print</a></span>(<span class="st">"Epoch"</span>, epoch, <span class="st">"finished."</span>)</a>
<a class="sourceLine" id="cb23-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb23-21" data-line-number="21">})</a>
<a class="sourceLine" id="cb23-22" data-line-number="22"></a>
<a class="sourceLine" id="cb23-23" data-line-number="23"><span class="kw"><a href="../../tensorflow/reference/train.html">train</a></span>()</a></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">history &lt;-<span class="st"> </span>loss_history <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="st">  </span>purrr<span class="op">::</span><span class="kw"><a href="https://purrr.tidyverse.org/reference/map.html">map</a></span>(as.numeric) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb24-3" data-line-number="3"><span class="st">  </span>purrr<span class="op">::</span><span class="kw"><a href="https://purrr.tidyverse.org/reference/flatten.html">flatten_dbl</a></span>()</a>
<a class="sourceLine" id="cb24-4" data-line-number="4">ggplot2<span class="op">::</span><span class="kw"><a href="https://ggplot2.tidyverse.org/reference/qplot.html">qplot</a></span>(<span class="dt">x =</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span>(history), <span class="dt">y =</span> history, <span class="dt">geom =</span> <span class="st">"line"</span>)</a></code></pre></div>
<p><img src="/guide/tensorflow/eager_execution_files/figure-html/unnamed-chunk-15-1.png" width="672"></p>
</div>
<div id="variables-and-optimizers" class="section level3">
<h3>Variables and optimizers</h3>
<p><code>tf$Variable</code> objects store mutable <code>tf$Tensor</code>-like values accessed during
training to make automatic differentiation easier.</p>
<p>The collections of variables can be encapsulated into layers or models, along with methods that operate on them. See <a href="../../keras/custom_layers/">Custom Keras layers and models</a> for details. The main difference between layers and models is that models add methods like <code>fit</code>, <code>evaluate</code>, and <code>save</code>.</p>
<p>For example, the automatic differentiation example above
can be rewritten:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">Linear &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">  <span class="kw"><a href="../../keras/reference/keras_model_custom.html">keras_model_custom</a></span>(<span class="dt">model_fn =</span> <span class="cf">function</span>(self) {</a>
<a class="sourceLine" id="cb25-3" data-line-number="3">    self<span class="op">$</span>w &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(<span class="dv">5</span>, <span class="dt">name =</span> <span class="st">"weight"</span>)</a>
<a class="sourceLine" id="cb25-4" data-line-number="4">    self<span class="op">$</span>b &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(<span class="dv">10</span>, <span class="dt">name =</span> <span class="st">"bias"</span>)</a>
<a class="sourceLine" id="cb25-5" data-line-number="5">    <span class="cf">function</span>(inputs, <span class="dt">mask =</span> <span class="ot">NULL</span>, <span class="dt">training =</span> <span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb25-6" data-line-number="6">      inputs<span class="op">*</span>self<span class="op">$</span>w <span class="op">+</span><span class="st"> </span>self<span class="op">$</span>b</a>
<a class="sourceLine" id="cb25-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb25-8" data-line-number="8">  }) </a>
<a class="sourceLine" id="cb25-9" data-line-number="9">}</a></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># A toy dataset of points around 3 * x + 2</span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2">NUM_EXAMPLES &lt;-<span class="st"> </span><span class="dv">2000</span></a>
<a class="sourceLine" id="cb26-3" data-line-number="3">training_inputs &lt;-<span class="st"> </span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">normal</span>(<span class="dt">shape =</span> <span class="kw"><a href="../../tensorflow/reference/shape.html">shape</a></span>(NUM_EXAMPLES))</a>
<a class="sourceLine" id="cb26-4" data-line-number="4">noise &lt;-<span class="st"> </span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">normal</span>(<span class="dt">shape =</span> <span class="kw"><a href="../../tensorflow/reference/shape.html">shape</a></span>(NUM_EXAMPLES))</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">training_outputs &lt;-<span class="st"> </span>training_inputs <span class="op">*</span><span class="st"> </span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>noise</a>
<a class="sourceLine" id="cb26-6" data-line-number="6"></a>
<a class="sourceLine" id="cb26-7" data-line-number="7"><span class="co"># The loss function to be optimized</span></a>
<a class="sourceLine" id="cb26-8" data-line-number="8">loss &lt;-<span class="st"> </span><span class="cf">function</span>(model, inputs, targets) {</a>
<a class="sourceLine" id="cb26-9" data-line-number="9">  error &lt;-<span class="st"> </span><span class="kw">model</span>(inputs) <span class="op">-</span><span class="st"> </span>targets</a>
<a class="sourceLine" id="cb26-10" data-line-number="10">  tf<span class="op">$</span><span class="kw">reduce_mean</span>(tf<span class="op">$</span><span class="kw">square</span>(error))</a>
<a class="sourceLine" id="cb26-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb26-12" data-line-number="12"></a>
<a class="sourceLine" id="cb26-13" data-line-number="13">grad &lt;-<span class="st"> </span><span class="cf">function</span>(model, inputs, targets) {</a>
<a class="sourceLine" id="cb26-14" data-line-number="14">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb26-15" data-line-number="15">    loss_value &lt;-<span class="st"> </span><span class="kw">loss</span>(model, inputs, targets)</a>
<a class="sourceLine" id="cb26-16" data-line-number="16">  })</a>
<a class="sourceLine" id="cb26-17" data-line-number="17">  tape<span class="op">$</span><span class="kw">gradient</span>(loss_value, <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(model<span class="op">$</span>w, model<span class="op">$</span>b))</a>
<a class="sourceLine" id="cb26-18" data-line-number="18">}</a></code></pre></div>
<p>Next:</p>
<ol style="list-style-type: decimal">
<li>Create the model.</li>
<li>The Derivatives of a loss function with respect to model parameters.</li>
<li>A strategy for updating the variables based on the derivatives.</li>
</ol>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">model &lt;-<span class="st"> </span><span class="kw">Linear</span>()</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">optimizer &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/optimizer_sgd.html">optimizer_sgd</a></span>(<span class="dt">lr =</span> <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb27-3" data-line-number="3"></a>
<a class="sourceLine" id="cb27-4" data-line-number="4"><span class="kw"><a href="https://rdrr.io/r/base/cat.html">cat</a></span>(<span class="st">"Initial loss: "</span>, <span class="kw"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span>(<span class="kw">loss</span>(model, training_inputs, training_outputs), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>))</a></code></pre></div>
<pre><code>## Initial loss:  68.56763</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span>(<span class="dv">300</span>)) {</a>
<a class="sourceLine" id="cb29-2" data-line-number="2">  grads &lt;-<span class="st"> </span><span class="kw">grad</span>(model, training_inputs, training_outputs)</a>
<a class="sourceLine" id="cb29-3" data-line-number="3">  optimizer<span class="op">$</span><span class="kw">apply_gradients</span>(purrr<span class="op">::</span><span class="kw"><a href="https://purrr.tidyverse.org/reference/transpose.html">transpose</a></span>(</a>
<a class="sourceLine" id="cb29-4" data-line-number="4">    <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(grads, <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(model<span class="op">$</span>w, model<span class="op">$</span>b))</a>
<a class="sourceLine" id="cb29-5" data-line-number="5">  ))</a>
<a class="sourceLine" id="cb29-6" data-line-number="6">  <span class="cf">if</span> (i <span class="op">%%</span><span class="st"> </span><span class="dv">20</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb29-7" data-line-number="7">    <span class="kw"><a href="https://rdrr.io/r/base/cat.html">cat</a></span>(<span class="st">"Loss at step "</span>, i, <span class="st">": "</span>, <span class="kw"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span>(<span class="kw">loss</span>(model, training_inputs, training_outputs)), <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</a>
<a class="sourceLine" id="cb29-8" data-line-number="8">}</a></code></pre></div>
<pre><code>## Loss at step  20 :  31.22758 
## Loss at step  40 :  14.53243 
## Loss at step  60 :  7.066751 
## Loss at step  80 :  3.727762 
## Loss at step  100 :  2.234176 
## Loss at step  120 :  1.56596 
## Loss at step  140 :  1.266953 
## Loss at step  160 :  1.133134 
## Loss at step  180 :  1.073232 
## Loss at step  200 :  1.046412 
## Loss at step  220 :  1.034402 
## Loss at step  240 :  1.029022 
## Loss at step  260 :  1.026612 
## Loss at step  280 :  1.025532 
## Loss at step  300 :  1.025048</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1">model<span class="op">$</span>w</a></code></pre></div>
<pre><code>## &lt;tf.Variable 'weight:0' shape=() dtype=float32, numpy=3.0064435&gt;</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1">model<span class="op">$</span>b</a></code></pre></div>
<pre><code>## &lt;tf.Variable 'bias:0' shape=() dtype=float32, numpy=2.029713&gt;</code></pre>
<p>Note: Variables persist until the last reference to the object
is removed, and is the variable is deleted.</p>
</div>
<div id="object-based-saving" class="section level3">
<h3>Object-based saving</h3>
<p>A Keras model includes a convinient <code>save_weights</code> method allowing you to easily create a checkpoint:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw"><a href="../../keras/reference/save_model_weights_tf.html">save_model_weights_tf</a></span>(model, <span class="st">"weights"</span>)</a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw"><a href="../../keras/reference/save_model_weights_tf.html">load_model_weights_tf</a></span>(model, <span class="dt">filepath =</span> <span class="st">"weights"</span>)</a></code></pre></div>
<p>Using <code>tf$train$Checkpoint</code> you can take full control over this process.</p>
<p>This section is an abbreviated version of the <a href="./checkpoint/">guide to training checkpoints</a>.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">checkpoint &lt;-<span class="st"> </span>tf<span class="op">$</span>train<span class="op">$</span><span class="kw">Checkpoint</span>(<span class="dt">x =</span> x)</a></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">x<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/assign.html">assign</a></span>(<span class="dv">2</span>) <span class="co"># Assign a new value to the variables and save.</span></a></code></pre></div>
<pre><code>## &lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0&gt;</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">checkpoint_path &lt;-<span class="st"> "ckpt/"</span></a>
<a class="sourceLine" id="cb39-2" data-line-number="2">checkpoint<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/save.html">save</a></span>(checkpoint_path)</a></code></pre></div>
<pre><code>## [1] "ckpt/-1"</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1">x<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/assign.html">assign</a></span>(<span class="dv">11</span>) <span class="co"># Change the variable after saving.</span></a></code></pre></div>
<pre><code>## &lt;tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=11.0&gt;</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">checkpoint<span class="op">$</span><span class="kw">restore</span>(tf<span class="op">$</span>train<span class="op">$</span><span class="kw">latest_checkpoint</span>(checkpoint_path))</a></code></pre></div>
<pre><code>## &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus&gt;</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">x</a></code></pre></div>
<pre><code>## &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;</code></pre>
<p>To save and load models, <code>tf$train$Checkpoint</code> stores the internal state of objects,
without requiring hidden variables. To record the state of a <code>model</code>,
an <code>optimizer</code>, and a global step, pass them to a <code>tf$train$Checkpoint</code>:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1">model &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/keras_model_sequential.html">keras_model_sequential</a></span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_conv_2d.html">layer_conv_2d</a></span>(<span class="dt">filters =</span> <span class="dv">16</span>, <span class="dt">kernel_size =</span> <span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="dt">activation =</span> <span class="st">"relu"</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb47-3" data-line-number="3"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_global_average_pooling_2d.html">layer_global_average_pooling_2d</a></span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb47-4" data-line-number="4"><span class="st">  </span><span class="kw"><a href="../../keras/reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb47-5" data-line-number="5"></a>
<a class="sourceLine" id="cb47-6" data-line-number="6">optimizer &lt;-<span class="st"> </span><span class="kw"><a href="../../keras/reference/optimizer_adam.html">optimizer_adam</a></span>(<span class="dt">lr =</span> <span class="fl">0.001</span>)</a>
<a class="sourceLine" id="cb47-7" data-line-number="7"></a>
<a class="sourceLine" id="cb47-8" data-line-number="8">checkpoint_dir &lt;-<span class="st"> 'path/to/model_dir'</span></a>
<a class="sourceLine" id="cb47-9" data-line-number="9"><span class="cf">if</span> (<span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/files2.html">dir.exists</a></span>(checkpoint_dir))</a>
<a class="sourceLine" id="cb47-10" data-line-number="10">  <span class="kw"><a href="https://rdrr.io/r/base/files2.html">dir.create</a></span>(checkpoint_dir, <span class="dt">recursive =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb47-11" data-line-number="11"></a>
<a class="sourceLine" id="cb47-12" data-line-number="12">checkpoint_prefix &lt;-<span class="st"> </span><span class="kw"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span>(checkpoint_dir, <span class="st">"ckpt"</span>)</a>
<a class="sourceLine" id="cb47-13" data-line-number="13"></a>
<a class="sourceLine" id="cb47-14" data-line-number="14">root &lt;-<span class="st"> </span>tf<span class="op">$</span>train<span class="op">$</span><span class="kw">Checkpoint</span>(<span class="dt">optimizer =</span> optimizer, <span class="dt">model =</span> model)</a>
<a class="sourceLine" id="cb47-15" data-line-number="15"></a>
<a class="sourceLine" id="cb47-16" data-line-number="16">root<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/save.html">save</a></span>(checkpoint_prefix)</a></code></pre></div>
<pre><code>## [1] "path/to/model_dir/ckpt-1"</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1">root<span class="op">$</span><span class="kw">restore</span>(tf<span class="op">$</span>train<span class="op">$</span><span class="kw">latest_checkpoint</span>(checkpoint_dir))</a></code></pre></div>
<pre><code>## &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus&gt;</code></pre>
<p>Note: In many training loops, variables are created after tf<span class="math inline">\(train\)</span>Checkpoint.restore is called. These variables will be restored as soon as they are created, and assertions are available to ensure that a checkpoint has been fully loaded. See the guide to training checkpoints for details.</p>
</div>
<div id="object-oriented-metrics" class="section level3">
<h3>Object-oriented metrics</h3>
<p><code>tf$keras$metrics</code> are stored as objects. Update a metric by passing the new data to
the callable, and retrieve the result using the <code>tf$keras$metrics$result</code> method,
for example:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1">m &lt;-<span class="st"> </span>tf<span class="op">$</span>keras<span class="op">$</span>metrics<span class="op">$</span><span class="kw">Mean</span>(<span class="st">"loss"</span>)</a>
<a class="sourceLine" id="cb51-2" data-line-number="2"><span class="kw">m</span>(<span class="dv">0</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(0.0, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw">m</span>(<span class="dv">5</span>)</a></code></pre></div>
<pre><code>## tf.Tensor(2.5, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">m<span class="op">$</span><span class="kw">result</span>()</a></code></pre></div>
<pre><code>## tf.Tensor(2.5, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="kw">m</span>(<span class="kw"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="dv">8</span>, <span class="dv">9</span>))</a></code></pre></div>
<pre><code>## tf.Tensor(5.5, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">m<span class="op">$</span><span class="kw">result</span>()</a></code></pre></div>
<pre><code>## tf.Tensor(5.5, shape=(), dtype=float32)</code></pre>
</div>
<div id="summaries-and-tensorboard" class="section level3">
<h3>Summaries and TensorBoard</h3>
<p><a href="https://tensorflow.org/tensorboard">TensorBoard</a> is a visualization tool for
understanding, debugging and optimizing the model training process. It uses
summary events that are written while executing the program.</p>
<p>You can use <code>tf$summary</code> to record summaries of variable in eager execution.
For example, to record summaries of <code>loss</code> once every 100 training steps:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1">logdir &lt;-<span class="st"> "./tb/"</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2">writer =<span class="st"> </span>tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">create_file_writer</span>(logdir)</a>
<a class="sourceLine" id="cb61-3" data-line-number="3"><span class="kw"><a href="../../tensorflow/reference/tensorboard.html">tensorboard</a></span>(<span class="dt">log_dir =</span> logdir) <span class="co"># This will open a browser window pointing to Tensorboard</span></a>
<a class="sourceLine" id="cb61-4" data-line-number="4"></a>
<a class="sourceLine" id="cb61-5" data-line-number="5"><span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(writer<span class="op">$</span><span class="kw">as_default</span>(), {</a>
<a class="sourceLine" id="cb61-6" data-line-number="6">  <span class="cf">for</span> (step <span class="cf">in</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span>(<span class="dv">1000</span>)) {</a>
<a class="sourceLine" id="cb61-7" data-line-number="7">    <span class="co"># Calculate loss with your real train function.</span></a>
<a class="sourceLine" id="cb61-8" data-line-number="8">    loss =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.001</span> <span class="op">*</span><span class="st"> </span>step</a>
<a class="sourceLine" id="cb61-9" data-line-number="9">    <span class="cf">if</span> (step <span class="op">%%</span><span class="st"> </span><span class="dv">100</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb61-10" data-line-number="10">      tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="st">'loss'</span>, loss, <span class="dt">step=</span>step)</a>
<a class="sourceLine" id="cb61-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb61-12" data-line-number="12">})</a></code></pre></div>
</div>
</div>
<div id="advanced-automatic-differentiation-topics" class="section level2">
<h2>Advanced automatic differentiation topics</h2>
<div id="dynamic-models" class="section level3">
<h3>Dynamic models</h3>
<p><code>tf$GradientTape</code> can also be used in dynamic models. This example for a
<a href="https://wikipedia.org/wiki/Backtracking_line_search">backtracking line search</a>
algorithm looks like normal R code, except there are gradients and is
differentiable, despite the complex control flow:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">line_search_step &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">custom_gradient</span>(<span class="kw"><a href="https://rdrr.io/pkg/tfautograph/man/autograph.html">autograph</a></span>(<span class="cf">function</span>(fn, init_x, <span class="dt">rate =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb62-2" data-line-number="2">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb62-3" data-line-number="3">    tape<span class="op">$</span><span class="kw">watch</span>(init_x)</a>
<a class="sourceLine" id="cb62-4" data-line-number="4">    value &lt;-<span class="st"> </span><span class="kw">fn</span>(init_x)</a>
<a class="sourceLine" id="cb62-5" data-line-number="5">  })</a>
<a class="sourceLine" id="cb62-6" data-line-number="6">  grad &lt;-<span class="st"> </span>tape<span class="op">$</span><span class="kw">gradient</span>(value, init_x)</a>
<a class="sourceLine" id="cb62-7" data-line-number="7">  grad_norm &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_sum</span>(grad <span class="op">*</span><span class="st"> </span>grad)</a>
<a class="sourceLine" id="cb62-8" data-line-number="8">  init_value &lt;-<span class="st"> </span>value</a>
<a class="sourceLine" id="cb62-9" data-line-number="9">  <span class="cf">while</span>(value <span class="op">&gt;</span><span class="st"> </span>(init_value <span class="op">-</span><span class="st"> </span>rate <span class="op">*</span><span class="st"> </span>grad_norm)) {</a>
<a class="sourceLine" id="cb62-10" data-line-number="10">    x &lt;-<span class="st"> </span>init_x <span class="op">-</span><span class="st"> </span>rate <span class="op">*</span><span class="st"> </span>grad</a>
<a class="sourceLine" id="cb62-11" data-line-number="11">    value &lt;-<span class="st"> </span><span class="kw">fn</span>(x)</a>
<a class="sourceLine" id="cb62-12" data-line-number="12">    rate =<span class="st"> </span>rate<span class="op">/</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb62-13" data-line-number="13">  }</a>
<a class="sourceLine" id="cb62-14" data-line-number="14">  <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(x, value)</a>
<a class="sourceLine" id="cb62-15" data-line-number="15">}))</a></code></pre></div>
</div>
<div id="custom-gradients" class="section level3">
<h3>Custom gradients</h3>
<p>Custom gradients are an easy way to override gradients. Within the forward function, define the gradient with respect to the
inputs, outputs, or intermediate results. For example, here’s an easy way to clip
the norm of the gradients in the backward pass:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">clip_gradient_by_norm &lt;-<span class="st"> </span><span class="cf">function</span>(x, norm) {</a>
<a class="sourceLine" id="cb63-2" data-line-number="2">  y &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/identity.html">identity</a></span>(x)</a>
<a class="sourceLine" id="cb63-3" data-line-number="3">  grad_fn &lt;-<span class="st"> </span><span class="cf">function</span>(dresult) {</a>
<a class="sourceLine" id="cb63-4" data-line-number="4">    <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(tf<span class="op">$</span><span class="kw">clip_by_norm</span>(dresult, norm), <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb63-5" data-line-number="5">  }</a>
<a class="sourceLine" id="cb63-6" data-line-number="6">  <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(y, grad_fn)</a>
<a class="sourceLine" id="cb63-7" data-line-number="7">}</a></code></pre></div>
<p>Custom gradients are commonly used to provide a numerically stable gradient for a
sequence of operations:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">log1pexp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb64-2" data-line-number="2">  tf<span class="op">$</span>math<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/Log.html">log</a></span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(x))</a>
<a class="sourceLine" id="cb64-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb64-4" data-line-number="4"></a>
<a class="sourceLine" id="cb64-5" data-line-number="5">grad_log1pexp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb64-6" data-line-number="6">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb64-7" data-line-number="7">    tape<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb64-8" data-line-number="8">    value &lt;-<span class="st"> </span><span class="kw">log1pexp</span>(x)</a>
<a class="sourceLine" id="cb64-9" data-line-number="9">  })</a>
<a class="sourceLine" id="cb64-10" data-line-number="10">  tape<span class="op">$</span><span class="kw">gradient</span>(value, x)</a>
<a class="sourceLine" id="cb64-11" data-line-number="11">}</a></code></pre></div>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="co"># The gradient computation works fine at x = 0.</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2"><span class="kw">grad_log1pexp</span>(tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">0</span>))</a></code></pre></div>
<pre><code>## tf.Tensor(0.5, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="co"># However, x = 100 fails because of numerical instability.</span></a>
<a class="sourceLine" id="cb67-2" data-line-number="2"><span class="kw">grad_log1pexp</span>(tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">100</span>))</a></code></pre></div>
<pre><code>## tf.Tensor(nan, shape=(), dtype=float32)</code></pre>
<p>Here, the <code>log1pexp</code> function can be analytically simplified with a custom
gradient. The implementation below reuses the value for <code>tf$exp(x)</code> that is
computed during the forward pass—making it more efficient by eliminating
redundant calculations:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">log1pexp &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">custom_gradient</span>(<span class="dt">f =</span> <span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb69-2" data-line-number="2">  e &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/Log.html">exp</a></span>(x)</a>
<a class="sourceLine" id="cb69-3" data-line-number="3">  grad_fn &lt;-<span class="st"> </span><span class="cf">function</span>(dy) {</a>
<a class="sourceLine" id="cb69-4" data-line-number="4">    dy <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>(e <span class="op">+</span><span class="st"> </span>e))</a>
<a class="sourceLine" id="cb69-5" data-line-number="5">  }</a>
<a class="sourceLine" id="cb69-6" data-line-number="6">  <span class="kw"><a href="https://rdrr.io/r/base/list.html">list</a></span>(tf<span class="op">$</span>math<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/base/Log.html">log</a></span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>e), grad_fn)</a>
<a class="sourceLine" id="cb69-7" data-line-number="7">})</a>
<a class="sourceLine" id="cb69-8" data-line-number="8"></a>
<a class="sourceLine" id="cb69-9" data-line-number="9">grad_log1pexp &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb69-10" data-line-number="10">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw">GradientTape</span>() <span class="op">%as%</span><span class="st"> </span>tape, {</a>
<a class="sourceLine" id="cb69-11" data-line-number="11">    tape<span class="op">$</span><span class="kw">watch</span>(x)</a>
<a class="sourceLine" id="cb69-12" data-line-number="12">    value &lt;-<span class="st"> </span><span class="kw">log1pexp</span>(x)</a>
<a class="sourceLine" id="cb69-13" data-line-number="13">  })</a>
<a class="sourceLine" id="cb69-14" data-line-number="14">  tape<span class="op">$</span><span class="kw">gradient</span>(value, x)</a>
<a class="sourceLine" id="cb69-15" data-line-number="15">}</a></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="co"># As before, the gradient computation works fine at x = 0.</span></a>
<a class="sourceLine" id="cb70-2" data-line-number="2"><span class="kw">grad_log1pexp</span>(tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">0</span>))</a></code></pre></div>
<pre><code>## tf.Tensor(0.5, shape=(), dtype=float32)</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="co"># And the gradient computation also works at x = 100.</span></a>
<a class="sourceLine" id="cb72-2" data-line-number="2"><span class="kw">grad_log1pexp</span>(tf<span class="op">$</span><span class="kw">constant</span>(<span class="dv">100</span>))</a></code></pre></div>
<pre><code>## tf.Tensor(1.0, shape=(), dtype=float32)</code></pre>
</div>
</div>
<div id="performance" class="section level2">
<h2>Performance</h2>
<p>Computation is automatically offloaded to GPUs during eager execution. If you
want control over where a computation runs you can enclose it in a
<code>tf$device('/gpu:0')</code> block (or the CPU equivalent):</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">fun &lt;-<span class="st"> </span><span class="cf">function</span>(device, <span class="dt">steps =</span> <span class="dv">200</span>) {</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">  <span class="kw"><a href="https://rdrr.io/r/base/with.html">with</a></span>(tf<span class="op">$</span><span class="kw"><a href="https://rdrr.io/r/grDevices/Devices.html">device</a></span>(device), {</a>
<a class="sourceLine" id="cb74-3" data-line-number="3">    x &lt;-<span class="st"> </span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">normal</span>(<span class="dt">shape =</span> shape)</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span>(steps)) {</a>
<a class="sourceLine" id="cb74-5" data-line-number="5">      tf<span class="op">$</span><span class="kw">matmul</span>(x, x)  </a>
<a class="sourceLine" id="cb74-6" data-line-number="6">    }  </a>
<a class="sourceLine" id="cb74-7" data-line-number="7">  })</a>
<a class="sourceLine" id="cb74-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb74-9" data-line-number="9">microbenchmark<span class="op">::</span><span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb74-10" data-line-number="10">  <span class="kw">fun</span>(<span class="st">"/cpu:0"</span>),</a>
<a class="sourceLine" id="cb74-11" data-line-number="11">  <span class="kw">fun</span>(<span class="st">"/gpu:0"</span>)</a>
<a class="sourceLine" id="cb74-12" data-line-number="12">)</a></code></pre></div>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="co"># Unit: milliseconds</span></a>
<a class="sourceLine" id="cb75-2" data-line-number="2"><span class="co">#           expr      min        lq      mean    median        uq       max neval</span></a>
<a class="sourceLine" id="cb75-3" data-line-number="3"><span class="co">#  fun("/cpu:0") 1117.596 1135.5450 1165.6269 1157.2208 1195.1529 1300.2236   100</span></a>
<a class="sourceLine" id="cb75-4" data-line-number="4"><span class="co">#  fun("/gpu:0")  112.888  121.7164  127.8525  126.6708  132.0415  228.1009   100</span></a></code></pre></div>
<p>A <code>tf$Tensor</code> object can be copied to a different device to execute its
operations:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1">x &lt;-<span class="st"> </span>tf<span class="op">$</span>random<span class="op">$</span><span class="kw">normal</span>(<span class="dt">shape =</span> <span class="kw"><a href="../../tensorflow/reference/shape.html">shape</a></span>(<span class="dv">10</span>,<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb76-2" data-line-number="2"></a>
<a class="sourceLine" id="cb76-3" data-line-number="3">x_gpu0 &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">gpu</span>()</a>
<a class="sourceLine" id="cb76-4" data-line-number="4">x_cpu &lt;-<span class="st"> </span>x<span class="op">$</span><span class="kw">cpu</span>()</a>
<a class="sourceLine" id="cb76-5" data-line-number="5"></a>
<a class="sourceLine" id="cb76-6" data-line-number="6">tf<span class="op">$</span><span class="kw">matmul</span>(x_cpu, x_cpu)    <span class="co"># Runs on CPU</span></a>
<a class="sourceLine" id="cb76-7" data-line-number="7">tf<span class="op">$</span><span class="kw">matmul</span>(x_gpu0, x_gpu0)  <span class="co"># Runs on GPU:0</span></a></code></pre></div>
<div id="benchmarks" class="section level3">
<h3>Benchmarks</h3>
<p>For compute-heavy models, such as
<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/resnet50">ResNet50</a>
training on a GPU, eager execution performance is comparable to <code>tf_function</code> execution.
But this gap grows larger for models with less computation and there is work to
be done for optimizing hot code paths for models with lots of small operations.</p>
</div>
</div>
<div id="work-with-functions" class="section level2">
<h2>Work with functions</h2>
<p>While eager execution makes development and debugging more interactive,
TensorFlow 1.x style graph execution has advantages for distributed training, performance
optimizations, and production deployment. To bridge this gap, TensorFlow 2.0 introduces <code>function</code>s via the <code>tf_function</code> API. For more information, see the <a href="./function">tf_function</a> guide.</p>
</div>
