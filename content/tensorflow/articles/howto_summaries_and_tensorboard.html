---
title: "TensorBoard: Visualizing Learning"
output: 
  html_document:
    toc: false
    toc_float: false
type: docs
menu:
  main:
    name: "TensorBoard Basics"
    identifier: "tensorflow-tensorboard-visualizing-learning"
    parent: "tensorflow-tensorboard"
    weight: 10
aliases:
  - /howto_summaries_and_tensorboard.html
---



<p>The computations you’ll use TensorFlow for - like training a massive
deep neural network - can be complex and confusing. To make it easier to
understand, debug, and optimize TensorFlow programs, we’ve included a suite of
visualization tools called TensorBoard. You can use TensorBoard to visualize
your TensorFlow graph, plot quantitative metrics about the execution of your
graph, and show additional data like images that pass through it. When
TensorBoard is fully configured, it looks like this:</p>
<p><img src="images/mnist_tensorboard.png" class="tutorial-illustration tutorial-illustration-large"></p>
<p>This tutorial is intended to get you started with simple TensorBoard usage.
There are other resources available as well! The <a href="https://www.tensorflow.org/code/tensorflow/tensorboard/">TensorBoard README</a>
has a lot more information on TensorBoard usage, including tips &amp; tricks, and
debugging information.</p>
<div id="serializing-the-data" class="section level2">
<h2>Serializing the data</h2>
<p>TensorBoard operates by reading TensorFlow events files, which contain summary
data that you can generate when running TensorFlow. Here’s the general
lifecycle for summary data within TensorBoard.</p>
<p>First, create the TensorFlow graph that you’d like to collect summary
data from, and decide which nodes you would like to annotate with
[summary operations]
(<a href="https://www.tensorflow.org/api_docs/python/train.html#summary-operations" class="uri">https://www.tensorflow.org/api_docs/python/train.html#summary-operations</a>).</p>
<p>For example, suppose you are training a convolutional neural network for
recognizing MNIST digits. You’d like to record how the learning rate
varies over time, and how the objective function is changing. Collect these by
attaching <a href="https://www.tensorflow.org/api_docs/python/summary/generation_of_summaries_#scalar"><code>tf$summary$scalar</code></a> ops
to the nodes that output the learning rate and loss respectively. Then, give
each <code>tf$summary$scalar</code> a meaningful <code>tag</code>, like <code>'learning rate'</code> or <code>'loss function'</code>.</p>
<p>Perhaps you’d also like to visualize the distributions of activations coming
off a particular layer, or the distribution of gradients or weights. Collect
this data by attaching
<a href="https://www.tensorflow.org/api_docs/python/summary/generation_of_summaries_#histogram"><code>tf$summary$histogram</code></a> ops to
the gradient outputs and to the variable that holds your weights, respectively.</p>
<p>For details on all of the summary operations available, check out the docs on
[summary operations]
(<a href="https://www.tensorflow.org/api_docs/python/summary/" class="uri">https://www.tensorflow.org/api_docs/python/summary/</a>).</p>
<p>Operations in TensorFlow don’t do anything until you run them, or an op that
depends on their output. And the summary nodes that we’ve just created are
peripheral to your graph: none of the ops you are currently running depend on
them. So, to generate summaries, we need to run all of these summary nodes.
Managing them by hand would be tedious, so use
<a href="https://www.tensorflow.org/api_docs/python/summary/generation_of_summaries_#merge_all"><code>tf$summary$merge_all</code></a>
to combine them into a single op that generates all the summary data.</p>
<p>Then, you can just run the merged summary op, which will generate a serialized
<code>Summary</code> protobuf object with all of your summary data at a given step.
Finally, to write this summary data to disk, pass the summary protobuf to a
<a href="https://www.tensorflow.org/api_docs/python/summary/generation_of_summaries_#FileWriter"><code>tf$summary$FileWriter</code></a>.</p>
<p>The <code>tf$summary$FileWriter</code> takes a logdir in its constructor - this logdir is quite
important, it’s the directory where all of the events will be written out.
Also, the <code>tf$summary$FileWriter</code> can optionally take a <code>Graph</code> in its constructor.
If it receives a <code>Graph</code> object, then TensorBoard will visualize your graph
along with tensor shape information. This will give you a much better sense of
what flows through the graph: see
<a href="howto_graph_viz.html#tensor-shape-information">Tensor shape information</a>.</p>
<p>Now that you’ve modified your graph and have a <code>tf$summary$FileWriter</code>, you’re ready to
start running your network! If you want, you could run the merged summary op
every single step, and record a ton of training data. That’s likely to be more
data than you need, though. Instead, consider running the merged summary op
every <code>n</code> steps.</p>
<p>The code example below is a modification of the <a href="tutorial_mnist_beginners.html">simple MNIST tutorial</a>, in which we have
added some summary ops, and run them every ten steps. If you run this and then
launch <code>tensorboard --logdir=/tmp/mnist_logs</code>, you’ll be able to visualize
statistics, such as how the weights or accuracy varied during training.
The code below is an excerpt; full source is <a href="https://github.com/rstudio/tensorflow/blob/master/inst/examples/mnist/mnist_with_summaries.R">here</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Attach a lot of summaries to a Tensor</span>
variable_summaries &lt;-<span class="st"> </span><span class="cf">function</span>(var, name) {
  <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"summaries"</span>), {
    mean &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_mean</span>(var)
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(<span class="st">"mean/"</span>, name), mean)
    <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"stddev"</span>), {
      stddev &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/MathFun">sqrt</a></span>(tf<span class="op">$</span><span class="kw">reduce_mean</span>(tf<span class="op">$</span><span class="kw">square</span>(var <span class="op">-</span><span class="st"> </span>mean)))
    })
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(<span class="st">"stddev/"</span>, name), stddev)
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(<span class="st">"max/"</span>, name), tf<span class="op">$</span><span class="kw">reduce_max</span>(var))
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(<span class="st">"min/"</span>, name), tf<span class="op">$</span><span class="kw">reduce_min</span>(var))
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">histogram</span>(name, var)
  })
}

<span class="co"># Reusable code for making a simple neural net layer.</span>
<span class="co">#</span>
<span class="co"># It does a matrix multiply, bias add, and then uses relu to nonlinearize.</span>
<span class="co"># It also sets up name scoping so that the resultant graph is easy to read,</span>
<span class="co"># and adds a number of summary ops.</span>
<span class="co">#</span>
nn_layer &lt;-<span class="st"> </span><span class="cf">function</span>(input_tensor, input_dim, output_dim,
                     layer_name, <span class="dt">act=</span>tf<span class="op">$</span>nn<span class="op">$</span>relu) {
  <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(layer_name), {
    <span class="co"># This Variable will hold the state of the weights for the layer</span>
    <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"weights"</span>), {
      weights &lt;-<span class="st"> </span><span class="kw">weight_variable</span>(<span class="kw">shape</span>(input_dim, output_dim))
      <span class="kw">variable_summaries</span>(weights, <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(layer_name, <span class="st">"/weights"</span>))
    })
    <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"biases"</span>), {
      biases &lt;-<span class="st"> </span><span class="kw">bias_variable</span>(<span class="kw">shape</span>(output_dim))
      <span class="kw">variable_summaries</span>(biases, <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(layer_name, <span class="st">"/biases"</span>))
    })
    <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span> (tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"Wx_plus_b"</span>), {
      preactivate &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">matmul</span>(input_tensor, weights) <span class="op">+</span><span class="st"> </span>biases
      tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">histogram</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(layer_name, <span class="st">"/pre_activations"</span>), preactivate)
    })
    activations &lt;-<span class="st"> </span><span class="kw">act</span>(preactivate, <span class="dt">name =</span> <span class="st">"activation"</span>)
    tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">histogram</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/paste">paste0</a></span>(layer_name, <span class="st">"/activations"</span>), activations)
  })
  activations
}

hidden1 &lt;-<span class="st"> </span><span class="kw">nn_layer</span>(x, 784L, 500L, <span class="st">"layer1"</span>)

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"dropout"</span>), {
  keep_prob &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">placeholder</span>(tf<span class="op">$</span>float32)
  tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="st">"dropout_keep_probability"</span>, keep_prob)
  dropped &lt;-<span class="st"> </span>tf<span class="op">$</span>nn<span class="op">$</span><span class="kw">dropout</span>(hidden1, keep_prob)
})

y &lt;-<span class="st"> </span><span class="kw">nn_layer</span>(dropped, 500L, 10L, <span class="st">"layer2"</span>, <span class="dt">act =</span> tf<span class="op">$</span>nn<span class="op">$</span>softmax)

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"cross_entropy"</span>), {
  diff &lt;-<span class="st"> </span>y_ <span class="op">*</span><span class="st"> </span>tf<span class="op">$</span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/Log">log</a></span>(y)
  <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"total"</span>), {
    cross_entropy &lt;-<span class="st"> </span><span class="op">-</span>tf<span class="op">$</span><span class="kw">reduce_mean</span>(diff)
  })
  tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="st">"cross entropy"</span>, cross_entropy)
})

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"train"</span>), {
  optimizer &lt;-<span class="st"> </span>tf<span class="op">$</span>train<span class="op">$</span><span class="kw">AdamOptimizer</span>(FLAGS<span class="op">$</span>learning_rate)
  train_step &lt;-<span class="st"> </span>optimizer<span class="op">$</span><span class="kw">minimize</span>(cross_entropy)
})

<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"accuracy"</span>), {
  <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"correct_prediction"</span>), {
    correct_prediction &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">equal</span>(tf<span class="op">$</span><span class="kw">arg_max</span>(y, 1L), tf<span class="op">$</span><span class="kw">arg_max</span>(y_, 1L))
  })
  <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/with">with</a></span>(tf<span class="op">$</span><span class="kw">name_scope</span>(<span class="st">"accuracy"</span>), {
    accuracy &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_mean</span>(tf<span class="op">$</span><span class="kw">cast</span>(correct_prediction, tf<span class="op">$</span>float32))
  })
  tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">scalar</span>(<span class="st">"accuracy"</span>, accuracy)
})

<span class="co"># Merge all the summaries and write them out to /tmp/mnist_logs (by default)</span>
merged &lt;-<span class="st"> </span>tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">merge_all</span>()
train_writer &lt;-<span class="st"> </span>tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">FileWriter</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/file.path">file.path</a></span>(FLAGS<span class="op">$</span>summaries_dir, <span class="st">"train"</span>),
                                      sess<span class="op">$</span>graph)
test_writer &lt;-<span class="st"> </span>tf<span class="op">$</span>summary<span class="op">$</span><span class="kw">FileWriter</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/file.path">file.path</a></span>(FLAGS<span class="op">$</span>summaries_dir, <span class="st">"test"</span>))
sess<span class="op">$</span><span class="kw">run</span>(tf<span class="op">$</span><span class="kw">global_variables_initializer</span>())</code></pre>
<p>After we’ve initialized the <code>FileWriters</code>, we have to add summaries to the
<code>FileWriters</code> as we train and test the model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the model, and also write summaries.</span>
<span class="co"># Every 10th step, measure test-set accuracy, and write test summaries</span>
<span class="co"># All other steps, run train_step on training data, &amp; add training summaries</span>

<span class="co"># Make a TensorFlow feed_dict: maps data onto Tensor placeholders.</span>
feed_dict &lt;-<span class="st"> </span><span class="cf">function</span>(train) {
  <span class="cf">if</span> (train <span class="op">||</span><span class="st"> </span>FLAGS<span class="op">$</span>fake_data) {
    batch &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span><span class="kw">next_batch</span>(100L, <span class="dt">fake_data =</span> FLAGS<span class="op">$</span>fake_data)
    xs &lt;-<span class="st"> </span>batch[[<span class="dv">1</span>]]
    ys &lt;-<span class="st"> </span>batch[[<span class="dv">2</span>]]
    k &lt;-<span class="st"> </span>FLAGS<span class="op">$</span>dropout
  } <span class="cf">else</span> {
    xs &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>images
    ys &lt;-<span class="st"> </span>mnist<span class="op">$</span>test<span class="op">$</span>labels
    k &lt;-<span class="st"> </span><span class="fl">1.0</span>
  }
  <span class="kw">dict</span>(<span class="dt">x =</span> xs,
       <span class="dt">y_ =</span> ys,
       <span class="dt">keep_prob =</span> k)
}

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>FLAGS<span class="op">$</span>max_steps) {
  <span class="cf">if</span> (i <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) { <span class="co"># Record summaries and test-set accuracy</span>
    result &lt;-<span class="st"> </span>sess<span class="op">$</span><span class="kw">run</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(merged, accuracy), <span class="dt">feed_dict =</span> <span class="kw">feed_dict</span>(<span class="ot">FALSE</span>))
    summary &lt;-<span class="st"> </span>result[[<span class="dv">1</span>]]
    acc &lt;-<span class="st"> </span>result[[<span class="dv">2</span>]]
    <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/cat">cat</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/sprintf">sprintf</a></span>(<span class="st">"Accuracy at step %s: %s"</span>, i, acc))
    test_writer<span class="op">$</span><span class="kw">add_summary</span>(summary, i) 
  } <span class="cf">else</span> { <span class="co"># Record train set summaries, and train</span>
    result &lt;-<span class="st"> </span>sess<span class="op">$</span><span class="kw">run</span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/list">list</a></span>(merged, train_step), <span class="dt">feed_dict =</span> <span class="kw">feed_dict</span>(<span class="ot">TRUE</span>))
    summary &lt;-<span class="st"> </span>result[[<span class="dv">1</span>]]
    train_writer<span class="op">$</span><span class="kw">add_summary</span>(summary, i)
  }
}</code></pre>
<p>You’re now all set to visualize this data using TensorBoard.</p>
</div>
<div id="launching-tensorboard" class="section level2">
<h2>Launching TensorBoard</h2>
<p>To run TensorBoard, use the following:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tensorboard</span>(<span class="dt">log_dir =</span> <span class="st">"path/to/log-directory"</span>)</code></pre>
<p>where <code>log_dir</code> points to the directory where the <code>tf$summary$FileWriter</code> serialized its
data. If this <code>log_dir</code> directory contains subdirectories which contain
serialized data from separate runs, then TensorBoard will visualize the data
from all of those runs.</p>
<p>When looking at TensorBoard, you will see the navigation tabs in the top right
corner. Each tab represents a set of serialized data that can be visualized.</p>
<p>For in depth information on how to use the <em>graph</em> tab to visualize your graph,
see <a href="howto_graph_viz.html">TensorBoard: Graph Visualization</a>.</p>
<p>For more usage information on TensorBoard in general, see the <a href="https://www.tensorflow.org/code/tensorflow/tensorboard/README.md">TensorBoard
README</a>.</p>
</div>
