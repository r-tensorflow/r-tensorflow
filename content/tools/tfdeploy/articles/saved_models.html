---
title: 'Using Saved Models'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using Saved Models from R} 
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteEncoding{UTF-8}
type: docs
repo: https://github.com/rstudio/tfdeploy
menu:
  main:
    name: "Using Saved Models"
    identifier: "tools-tfdeploy-using-saved-models"
    parent: "tfdeploy-top"
    weight: 20
---



<div id="overview" class="section level2">
<h2>Overview</h2>
<p>The main goal of the tfdeploy package is to create models in R and then export, test, and deploy those models to environments without R. However, there may be cases when it makes sense to use a saved model directly from R:</p>
<ul>
<li>If another R user has saved and/or deployed a model that you would like to use for predictions from R.</li>
<li>If you want to use a saved or deployed model in a Shiny application.</li>
<li>If you want to compare predictions between a saved or deployed model and a new model that is under development.</li>
</ul>
<p>One way to use a deployed model from R would be to execute HTTP requests using a package like <code>httr</code>. For non-deployed models, it is possible to use <code><a href="../../../tools/tfdeploy/reference/serve_savedmodel.html">serve_savedmodel()</a></code> - as we did for local testing - along with a tool like <code>httr</code>. However, there is an easier way to make predictions from a saved model using the <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> function.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Using the same MNIST model described previously, we can easily make predictions for new pre-processed images. For example, we can load the MNIST test data set and create predictions for the first 10 images:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(keras)
<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(tfdeploy)

test_images &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/dataset_mnist.html">dataset_mnist</a></span>()<span class="op">$</span>test<span class="op">$</span>x 
test_images &lt;-<span class="st"> </span><span class="kw"><a href="../../../keras/reference/reexports.html">array_reshape</a></span>(test_images, <span class="dt">dim =</span> <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/nrow">nrow</a></span>(test_images), <span class="dv">784</span>)) <span class="op">/</span><span class="st"> </span><span class="dv">255</span>
test_images &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/lapply">lapply</a></span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="cf">function</span>(i) {test_images[i,]})

<span class="kw"><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel</a></span>(processed_test_list, <span class="st">'savedmodel'</span>)</code></pre>
<pre><code>Prediction 1:
$prediction
 [1] 3.002971e-37 8.401216e-29 2.932129e-24 4.048731e-22 0.000000e+00 9.172148e-37
 [7] 0.000000e+00 1.000000e+00 4.337524e-31 1.772979e-17

Prediction 2:
$prediction
 [1] 0.000000e+00 4.548326e-22 1.000000e+00 2.261879e-31 0.000000e+00 0.000000e+00
 [7] 0.000000e+00 0.000000e+00 2.390626e-38 0.000000e+00
 
 ...</code></pre>
<p>A few things to keep in mind:</p>
<ol style="list-style-type: decimal">
<li><p>Just like the HTTP POST requests, <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> expects the new instance data to be pre-processed.</p></li>
<li><p><code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> requires the new data to be in a list, and it always returns a list. This requirement faciliates models with more complex inputs or ouputs.</p></li>
</ol>
<p>In the previous example we used <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> with the directory, ‘savedmodel’, which was created with the <code><a href="../../../keras/reference/reexports.html">export_savedmodel()</a></code> function In addition to providing a path to a saved model directory, <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> can also be used with a deployed model by supplying a REST URL, a CloudML model by supplying a CloudML name and version, or by supplying a graph object loaded with <code><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel()</a></code>.</p>
<p>The last option above references the <code><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel()</a></code> function. <code><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel()</a></code> should be used alongside of <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code> if you’ll be calling the prediction function multiple times. <code><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel()</a></code> effectively caches the model graph in memory and can speed up repeated calls to <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code>. This caching is useful, for example, in a Shiny application where user input would drive calls to <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># if there will only be one batch of predictions </span>
<span class="kw"><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel</a></span>(instances, <span class="st">'savedmodel'</span>)

<span class="co"># if there will be multiple batches of predictions</span>
sess &lt;-<span class="st"> </span>tensorflow<span class="op">::</span>tf<span class="op">$</span><span class="kw">Session</span>()
graph &lt;-<span class="st"> </span><span class="kw"><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel</a></span>(sess, <span class="st">'savedmodel'</span>)
<span class="kw"><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel</a></span>(instances, graph)
<span class="co"># ... more work ... </span>
<span class="kw"><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel</a></span>(instances, graph)</code></pre>
</div>
<div id="model-representations" class="section level2">
<h2>Model Representations</h2>
<p>There are a few distinct ways that a model can be represented in R. The most straightforward representation is the in-memory, R model object. This object is what is created and used while developing and training a model.</p>
<p>A second representation is the on-disk saved model. This representation of the model can be used by the <code>*_savedmodel</code> functions. As a special case, <code><a href="../../../tools/tfdeploy/reference/load_savedmodel.html">load_savedmodel()</a></code> creates a new R object pointing to the model graph. It is important to keep in mind that these saved models are not the full R model object. For example, you can not update or re-train a graph from a saved model.</p>
<p>Finally, for Keras models there are 2 other representations: HDF5 files and serialized R objects. Each of these represenations captures the entire in-memory R object. For example, using <code><a href="../../../keras/reference/save_model_hdf5.html">save_model_hdf5()</a></code> and then <code><a href="../../../keras/reference/save_model_hdf5.html">load_model_hdf5()</a></code> will result in a model that can be updated or retrained. Use the <code><a href="../../../keras/reference/serialize_model.html">serialize_model()</a></code> and <code>unserialized_model()</code> to save models as R objects.</p>
<div id="what-represenation-should-i-use" class="section level3">
<h3>What represenation should I use?</h3>
<p>If you are developing a model and have access to the in-memory R model object, you should use the model object for predictions using R’s <code>predict</code> function.</p>
<p>If you are developing a Keras model and would like to save the model for use in a different session, you should use the HDF5 file or serialize the model and then save it to an R data format like RDS.</p>
<p>If you are going to deploy a model and want to test it’s HTTP interface, you should export the model using <code><a href="../../../keras/reference/reexports.html">export_savedmodel()</a></code> and then test with either <code><a href="../../../tools/tfdeploy/reference/serve_savedmodel.html">serve_savedmodel()</a></code> and your HTTP client or <code><a href="../../../tools/tfdeploy/reference/predict_savedmodel.html">predict_savedmodel()</a></code>.</p>
<p>If you are using R and want to create predictions from a deployed or saved model, and you don’t have access to the in-memory R model object, you should use <code>predict_savedmode()l</code>.</p>
</div>
</div>
