---
title: Creates attention layer
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: Creates attention layer - keras
    parent: keras-reference
aliases:
- /reference/keras/layer_attention.html
- /keras/reference/layer_attention.html
- /guide/keras/reference/layer_attention.html
- /tools/tools/keras/reference/layer_attention.html
- /installation/keras/reference/layer_attention.html
- /tutorials/keras/reference/layer_attention.html
- /guide/tools/keras/reference/layer_attention.html
- /deploy/keras/reference/layer_attention.html
- /tools/keras/reference/layer_attention.html
- /tutorials/tools/keras/reference/layer_attention.html
---
    
    <p>Dot-product attention layer, a.k.a. Luong-style attention.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>layer_attention</span>(
  <span class='no'>inputs</span>,
  <span class='kw'>use_scale</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>causal</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>batch_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>dtype</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>trainable</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>weights</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>inputs</td>
      <td><p>a list of inputs first should be the query tensor, the second the value tensor</p></td>
    </tr>
    <tr>
      <td>use_scale</td>
      <td><p>If True, will create a scalar variable to scale the attention scores.</p></td>
    </tr>
    <tr>
      <td>causal</td>
      <td><p>Boolean. Set to True for decoder self-attention. Adds a mask such that position i cannot attend to positions j &gt; i.
This prevents the flow of information from the future towards the past.</p></td>
    </tr>
    <tr>
      <td>batch_size</td>
      <td><p>Fixed batch size for layer</p></td>
    </tr>
    <tr>
      <td>dtype</td>
      <td><p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p></td>
    </tr>
    <tr>
      <td>trainable</td>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <td>weights</td>
      <td><p>Initial weights for layer.</p></td>
    </tr>
    </table>

    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other core layers: 
<code><a href='../layer_activation.html'>layer_activation</a>()</code>,
<code><a href='../layer_activity_regularization.html'>layer_activity_regularization</a>()</code>,
<code><a href='../layer_dense_features.html'>layer_dense_features</a>()</code>,
<code><a href='../layer_dense.html'>layer_dense</a>()</code>,
<code><a href='../layer_dropout.html'>layer_dropout</a>()</code>,
<code><a href='../layer_flatten.html'>layer_flatten</a>()</code>,
<code><a href='../layer_input.html'>layer_input</a>()</code>,
<code><a href='../layer_lambda.html'>layer_lambda</a>()</code>,
<code><a href='../layer_masking.html'>layer_masking</a>()</code>,
<code><a href='../layer_permute.html'>layer_permute</a>()</code>,
<code><a href='../layer_repeat_vector.html'>layer_repeat_vector</a>()</code>,
<code><a href='../layer_reshape.html'>layer_reshape</a>()</code></p></div>




