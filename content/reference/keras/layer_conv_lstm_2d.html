---
title: "Convolutional LSTM."
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: "Convolutional LSTM. - keras"
    parent: keras-reference
aliases: '/reference/keras/layer_conv_lstm_2d.html'
---
    
    <p>It is similar to an LSTM layer, but the input transformations and recurrent
transformations are both convolutional.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>layer_conv_lstm_2d</span>(
  <span class='no'>object</span>,
  <span class='no'>filters</span>,
  <span class='no'>kernel_size</span>,
  <span class='kw'>strides</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>1L</span>, <span class='fl'>1L</span>),
  <span class='kw'>padding</span> <span class='kw'>=</span> <span class='st'>"valid"</span>,
  <span class='kw'>data_format</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>dilation_rate</span> <span class='kw'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span>(<span class='fl'>1L</span>, <span class='fl'>1L</span>),
  <span class='kw'>activation</span> <span class='kw'>=</span> <span class='st'>"tanh"</span>,
  <span class='kw'>recurrent_activation</span> <span class='kw'>=</span> <span class='st'>"hard_sigmoid"</span>,
  <span class='kw'>use_bias</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>kernel_initializer</span> <span class='kw'>=</span> <span class='st'>"glorot_uniform"</span>,
  <span class='kw'>recurrent_initializer</span> <span class='kw'>=</span> <span class='st'>"orthogonal"</span>,
  <span class='kw'>bias_initializer</span> <span class='kw'>=</span> <span class='st'>"zeros"</span>,
  <span class='kw'>unit_forget_bias</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>kernel_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>recurrent_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>activity_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>kernel_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>recurrent_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>return_sequences</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>go_backwards</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>stateful</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>dropout</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>recurrent_dropout</span> <span class='kw'>=</span> <span class='fl'>0</span>,
  <span class='kw'>batch_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>trainable</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>weights</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>object</td>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <td>filters</td>
      <td><p>Integer, the dimensionality of the output space (i.e. the
number of output filters in the convolution).</p></td>
    </tr>
    <tr>
      <td>kernel_size</td>
      <td><p>An integer or list of n integers, specifying the
dimensions of the convolution window.</p></td>
    </tr>
    <tr>
      <td>strides</td>
      <td><p>An integer or list of n integers, specifying the strides of
the convolution. Specifying any stride value != 1 is incompatible with
specifying any <code>dilation_rate</code> value != 1.</p></td>
    </tr>
    <tr>
      <td>padding</td>
      <td><p>One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</p></td>
    </tr>
    <tr>
      <td>data_format</td>
      <td><p>A string, one of <code>channels_last</code> (default) or
<code>channels_first</code>. The ordering of the dimensions in the inputs.
<code>channels_last</code> corresponds to inputs with shape <code>(batch, time, ..., channels)</code> while <code>channels_first</code> corresponds to inputs with shape <code>(batch, time, channels, ...)</code>. It defaults to the <code>image_data_format</code> value found
in your Keras config file at <code>~/.keras/keras.json</code>. If you never set it,
then it will be "channels_last".</p></td>
    </tr>
    <tr>
      <td>dilation_rate</td>
      <td><p>An integer or list of n integers, specifying the
dilation rate to use for dilated convolution. Currently, specifying any
<code>dilation_rate</code> value != 1 is incompatible with specifying any <code>strides</code>
value != 1.</p></td>
    </tr>
    <tr>
      <td>activation</td>
      <td><p>Activation function to use. If you don't specify anything,
no activation is applied (ie. "linear" activation: <code>a(x) = x</code>).</p></td>
    </tr>
    <tr>
      <td>recurrent_activation</td>
      <td><p>Activation function to use for the recurrent
step.</p></td>
    </tr>
    <tr>
      <td>use_bias</td>
      <td><p>Boolean, whether the layer uses a bias vector.</p></td>
    </tr>
    <tr>
      <td>kernel_initializer</td>
      <td><p>Initializer for the <code>kernel</code> weights matrix, used
for the linear transformation of the inputs..</p></td>
    </tr>
    <tr>
      <td>recurrent_initializer</td>
      <td><p>Initializer for the <code>recurrent_kernel</code> weights
matrix, used for the linear transformation of the recurrent state..</p></td>
    </tr>
    <tr>
      <td>bias_initializer</td>
      <td><p>Initializer for the bias vector.</p></td>
    </tr>
    <tr>
      <td>unit_forget_bias</td>
      <td><p>Boolean. If TRUE, add 1 to the bias of the forget
gate at initialization. Use in combination with <code>bias_initializer="zeros"</code>.
This is recommended in <a href='http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf'>Jozefowicz et al.</a></p></td>
    </tr>
    <tr>
      <td>kernel_regularizer</td>
      <td><p>Regularizer function applied to the <code>kernel</code>
weights matrix.</p></td>
    </tr>
    <tr>
      <td>recurrent_regularizer</td>
      <td><p>Regularizer function applied to the
<code>recurrent_kernel</code> weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_regularizer</td>
      <td><p>Regularizer function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>activity_regularizer</td>
      <td><p>Regularizer function applied to the output of the
layer (its "activation")..</p></td>
    </tr>
    <tr>
      <td>kernel_constraint</td>
      <td><p>Constraint function applied to the <code>kernel</code> weights
matrix.</p></td>
    </tr>
    <tr>
      <td>recurrent_constraint</td>
      <td><p>Constraint function applied to the
<code>recurrent_kernel</code> weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_constraint</td>
      <td><p>Constraint function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>return_sequences</td>
      <td><p>Boolean. Whether to return the last output in the
output sequence, or the full sequence.</p></td>
    </tr>
    <tr>
      <td>go_backwards</td>
      <td><p>Boolean (default FALSE). If TRUE, rocess the input
sequence backwards.</p></td>
    </tr>
    <tr>
      <td>stateful</td>
      <td><p>Boolean (default FALSE). If TRUE, the last state for each
sample at index i in a batch will be used as initial state for the sample
of index i in the following batch.</p></td>
    </tr>
    <tr>
      <td>dropout</td>
      <td><p>Float between 0 and 1. Fraction of the units to drop for the
linear transformation of the inputs.</p></td>
    </tr>
    <tr>
      <td>recurrent_dropout</td>
      <td><p>Float between 0 and 1. Fraction of the units to drop
for the linear transformation of the recurrent state.</p></td>
    </tr>
    <tr>
      <td>batch_size</td>
      <td><p>Fixed batch size for layer</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p></td>
    </tr>
    <tr>
      <td>trainable</td>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <td>weights</td>
      <td><p>Initial weights for layer.</p></td>
    </tr>
    <tr>
      <td>input_shape</td>
      <td><p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p></td>
    </tr>
    </table>

    <h2 id="input-shape">Input shape</h2>

    

<ul>
<li><p>if data_format='channels_first' 5D tensor with shape:
<code>(samples,time, channels, rows, cols)</code></p><ul>
<li><p>if data_format='channels_last' 5D
tensor with shape: <code>(samples,time, rows, cols, channels)</code></p></li>
</ul></li>
</ul>

    <h2 id="references">References</h2>

    

<ul>
<li><p><a href='http://arxiv.org/abs/1506.04214v1'>Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a>
The current implementation does not include the feedback loop on the cells
output</p></li>
</ul>

    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other convolutional layers: 
<code><a href='layer_conv_1d.html'>layer_conv_1d</a>()</code>,
<code><a href='layer_conv_2d_transpose.html'>layer_conv_2d_transpose</a>()</code>,
<code><a href='layer_conv_2d.html'>layer_conv_2d</a>()</code>,
<code><a href='layer_conv_3d_transpose.html'>layer_conv_3d_transpose</a>()</code>,
<code><a href='layer_conv_3d.html'>layer_conv_3d</a>()</code>,
<code><a href='layer_cropping_1d.html'>layer_cropping_1d</a>()</code>,
<code><a href='layer_cropping_2d.html'>layer_cropping_2d</a>()</code>,
<code><a href='layer_cropping_3d.html'>layer_cropping_3d</a>()</code>,
<code><a href='layer_depthwise_conv_2d.html'>layer_depthwise_conv_2d</a>()</code>,
<code><a href='layer_separable_conv_1d.html'>layer_separable_conv_1d</a>()</code>,
<code><a href='layer_separable_conv_2d.html'>layer_separable_conv_2d</a>()</code>,
<code><a href='layer_upsampling_1d.html'>layer_upsampling_1d</a>()</code>,
<code><a href='layer_upsampling_2d.html'>layer_upsampling_2d</a>()</code>,
<code><a href='layer_upsampling_3d.html'>layer_upsampling_3d</a>()</code>,
<code><a href='layer_zero_padding_1d.html'>layer_zero_padding_1d</a>()</code>,
<code><a href='layer_zero_padding_2d.html'>layer_zero_padding_2d</a>()</code>,
<code><a href='layer_zero_padding_3d.html'>layer_zero_padding_3d</a>()</code></p></div>




