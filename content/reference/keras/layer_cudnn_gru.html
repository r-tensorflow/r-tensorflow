---
title: "Fast GRU implementation backed by <a href='https://developer.nvidia.com/cudnn'>CuDNN</a>."
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: "Fast GRU implementation backed by &lt;a href='https://developer.nvidia.com/cudnn'&gt;CuDNN&lt;/a&gt;. - keras"
    parent: keras-reference
aliases: '/reference/keras/layer_cudnn_gru.html'
---
    
    <p>Can only be run on GPU, with the TensorFlow backend.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>layer_cudnn_gru</span>(
  <span class='no'>object</span>,
  <span class='no'>units</span>,
  <span class='kw'>kernel_initializer</span> <span class='kw'>=</span> <span class='st'>"glorot_uniform"</span>,
  <span class='kw'>recurrent_initializer</span> <span class='kw'>=</span> <span class='st'>"orthogonal"</span>,
  <span class='kw'>bias_initializer</span> <span class='kw'>=</span> <span class='st'>"zeros"</span>,
  <span class='kw'>kernel_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>recurrent_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>activity_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>kernel_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>recurrent_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>return_sequences</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>return_state</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>stateful</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>dtype</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>trainable</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>weights</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>object</td>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <td>units</td>
      <td><p>Positive integer, dimensionality of the output space.</p></td>
    </tr>
    <tr>
      <td>kernel_initializer</td>
      <td><p>Initializer for the <code>kernel</code> weights matrix, used
for the linear transformation of the inputs.</p></td>
    </tr>
    <tr>
      <td>recurrent_initializer</td>
      <td><p>Initializer for the <code>recurrent_kernel</code> weights
matrix, used for the linear transformation of the recurrent state.</p></td>
    </tr>
    <tr>
      <td>bias_initializer</td>
      <td><p>Initializer for the bias vector.</p></td>
    </tr>
    <tr>
      <td>kernel_regularizer</td>
      <td><p>Regularizer function applied to the <code>kernel</code>
weights matrix.</p></td>
    </tr>
    <tr>
      <td>recurrent_regularizer</td>
      <td><p>Regularizer function applied to the
<code>recurrent_kernel</code> weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_regularizer</td>
      <td><p>Regularizer function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>activity_regularizer</td>
      <td><p>Regularizer function applied to the output of the
layer (its "activation")..</p></td>
    </tr>
    <tr>
      <td>kernel_constraint</td>
      <td><p>Constraint function applied to the <code>kernel</code> weights
matrix.</p></td>
    </tr>
    <tr>
      <td>recurrent_constraint</td>
      <td><p>Constraint function applied to the
<code>recurrent_kernel</code> weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_constraint</td>
      <td><p>Constraint function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>return_sequences</td>
      <td><p>Boolean. Whether to return the last output in the
output sequence, or the full sequence.</p></td>
    </tr>
    <tr>
      <td>return_state</td>
      <td><p>Boolean (default FALSE). Whether to return the last state
in addition to the output.</p></td>
    </tr>
    <tr>
      <td>stateful</td>
      <td><p>Boolean (default FALSE). If TRUE, the last state for each
sample at index i in a batch will be used as initial state for the sample
of index i in the following batch.</p></td>
    </tr>
    <tr>
      <td>input_shape</td>
      <td><p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p></td>
    </tr>
    <tr>
      <td>batch_input_shape</td>
      <td><p>Shapes, including the batch size. For instance,
<code>batch_input_shape=c(10, 32)</code> indicates that the expected input will be
batches of 10 32-dimensional vectors. <code>batch_input_shape=list(NULL, 32)</code>
indicates batches of an arbitrary number of 32-dimensional vectors.</p></td>
    </tr>
    <tr>
      <td>batch_size</td>
      <td><p>Fixed batch size for layer</p></td>
    </tr>
    <tr>
      <td>dtype</td>
      <td><p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p></td>
    </tr>
    <tr>
      <td>trainable</td>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <td>weights</td>
      <td><p>Initial weights for layer.</p></td>
    </tr>
    </table>

    <h2 id="references">References</h2>

    

<ul>
<li><p><a href='https://arxiv.org/abs/1409.1259'>On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</a></p></li>
<li><p><a href='http://arxiv.org/abs/1412.3555v1'>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></p></li>
<li><p><a href='http://arxiv.org/abs/1512.05287'>A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</a></p></li>
</ul>

    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other recurrent layers: 
<code><a href='layer_cudnn_lstm.html'>layer_cudnn_lstm</a>()</code>,
<code><a href='layer_gru.html'>layer_gru</a>()</code>,
<code><a href='layer_lstm.html'>layer_lstm</a>()</code>,
<code><a href='layer_simple_rnn.html'>layer_simple_rnn</a>()</code></p></div>




