---
title: Batch normalization layer (Ioffe and Szegedy, 2014).
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: Batch normalization layer (Ioffe and Szegedy, 2014). - keras
    parent: keras-reference
aliases:
- /reference/keras/layer_batch_normalization.html
- /keras/reference/layer_batch_normalization.html
- /guide/keras/reference/layer_batch_normalization.html
- /tools/tools/keras/reference/layer_batch_normalization.html
---
    
    <p>Normalize the activations of the previous layer at each batch, i.e. applies a
transformation that maintains the mean activation close to 0 and the
activation standard deviation close to 1.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>layer_batch_normalization</span>(
  <span class='no'>object</span>,
  <span class='kw'>axis</span> <span class='kw'>=</span> -<span class='fl'>1L</span>,
  <span class='kw'>momentum</span> <span class='kw'>=</span> <span class='fl'>0.99</span>,
  <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='fl'>0.001</span>,
  <span class='kw'>center</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>scale</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>beta_initializer</span> <span class='kw'>=</span> <span class='st'>"zeros"</span>,
  <span class='kw'>gamma_initializer</span> <span class='kw'>=</span> <span class='st'>"ones"</span>,
  <span class='kw'>moving_mean_initializer</span> <span class='kw'>=</span> <span class='st'>"zeros"</span>,
  <span class='kw'>moving_variance_initializer</span> <span class='kw'>=</span> <span class='st'>"ones"</span>,
  <span class='kw'>beta_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>gamma_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>beta_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>gamma_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>dtype</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>trainable</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>weights</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>object</td>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <td>axis</td>
      <td><p>Integer, the axis that should be normalized (typically the
features axis). For instance, after a <code>Conv2D</code> layer with
<code>data_format="channels_first"</code>, set <code>axis=1</code> in <code>BatchNormalization</code>.</p></td>
    </tr>
    <tr>
      <td>momentum</td>
      <td><p>Momentum for the moving mean and the moving variance.</p></td>
    </tr>
    <tr>
      <td>epsilon</td>
      <td><p>Small float added to variance to avoid dividing by zero.</p></td>
    </tr>
    <tr>
      <td>center</td>
      <td><p>If TRUE, add offset of <code>beta</code> to normalized tensor. If FALSE,
<code>beta</code> is ignored.</p></td>
    </tr>
    <tr>
      <td>scale</td>
      <td><p>If TRUE, multiply by <code>gamma</code>. If FALSE, <code>gamma</code> is not used.
When the next layer is linear (also e.g. <code>nn.relu</code>), this can be disabled
since the scaling will be done by the next layer.</p></td>
    </tr>
    <tr>
      <td>beta_initializer</td>
      <td><p>Initializer for the beta weight.</p></td>
    </tr>
    <tr>
      <td>gamma_initializer</td>
      <td><p>Initializer for the gamma weight.</p></td>
    </tr>
    <tr>
      <td>moving_mean_initializer</td>
      <td><p>Initializer for the moving mean.</p></td>
    </tr>
    <tr>
      <td>moving_variance_initializer</td>
      <td><p>Initializer for the moving variance.</p></td>
    </tr>
    <tr>
      <td>beta_regularizer</td>
      <td><p>Optional regularizer for the beta weight.</p></td>
    </tr>
    <tr>
      <td>gamma_regularizer</td>
      <td><p>Optional regularizer for the gamma weight.</p></td>
    </tr>
    <tr>
      <td>beta_constraint</td>
      <td><p>Optional constraint for the beta weight.</p></td>
    </tr>
    <tr>
      <td>gamma_constraint</td>
      <td><p>Optional constraint for the gamma weight.</p></td>
    </tr>
    <tr>
      <td>input_shape</td>
      <td><p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p></td>
    </tr>
    <tr>
      <td>batch_input_shape</td>
      <td><p>Shapes, including the batch size. For instance,
<code>batch_input_shape=c(10, 32)</code> indicates that the expected input will be
batches of 10 32-dimensional vectors. <code>batch_input_shape=list(NULL, 32)</code>
indicates batches of an arbitrary number of 32-dimensional vectors.</p></td>
    </tr>
    <tr>
      <td>batch_size</td>
      <td><p>Fixed batch size for layer</p></td>
    </tr>
    <tr>
      <td>dtype</td>
      <td><p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p></td>
    </tr>
    <tr>
      <td>trainable</td>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <td>weights</td>
      <td><p>Initial weights for layer.</p></td>
    </tr>
    </table>

    <h2 id="input-shape">Input shape</h2>

    <p>Arbitrary. Use the keyword argument <code>input_shape</code> (list
of integers, does not include the samples axis) when using this layer as
the first layer in a model.</p>
    <h2 id="output-shape">Output shape</h2>

    <p>Same shape as input.</p>
    <h2 id="references">References</h2>

    

<ul>
<li><p><a href='https://arxiv.org/abs/1502.03167'>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p></li>
</ul>





