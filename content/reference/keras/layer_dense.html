---
title: Add a densely-connected NN layer to an output
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: Add a densely-connected NN layer to an output - keras
    parent: keras-reference
aliases:
- /reference/keras/layer_dense.html
- /keras/reference/layer_dense.html
- /guide/keras/reference/layer_dense.html
- /tools/tools/keras/reference/layer_dense.html
- /installation/keras/reference/layer_dense.html
- /tutorials/keras/reference/layer_dense.html
- /guide/tools/keras/reference/layer_dense.html
- /deploy/keras/reference/layer_dense.html
- /tools/keras/reference/layer_dense.html
- /tutorials/tools/keras/reference/layer_dense.html
---
    
    <p>Implements the operation: <code>output = activation(dot(input, kernel) + bias)</code>
where <code>activation</code> is the element-wise activation function passed as the
<code>activation</code> argument, <code>kernel</code> is a weights matrix created by the layer, and
<code>bias</code> is a bias vector created by the layer (only applicable if <code>use_bias</code>
is <code>TRUE</code>). Note: if the input to the layer has a rank greater than 2, then
it is flattened prior to the initial dot product with <code>kernel</code>.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>layer_dense</span>(
  <span class='no'>object</span>,
  <span class='no'>units</span>,
  <span class='kw'>activation</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>use_bias</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>kernel_initializer</span> <span class='kw'>=</span> <span class='st'>"glorot_uniform"</span>,
  <span class='kw'>bias_initializer</span> <span class='kw'>=</span> <span class='st'>"zeros"</span>,
  <span class='kw'>kernel_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>activity_regularizer</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>kernel_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>bias_constraint</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_input_shape</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>batch_size</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>dtype</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>trainable</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>weights</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>object</td>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <td>units</td>
      <td><p>Positive integer, dimensionality of the output space.</p></td>
    </tr>
    <tr>
      <td>activation</td>
      <td><p>Name of activation function to use. If you don't specify
anything, no activation is applied (ie. "linear" activation: a(x) = x).</p></td>
    </tr>
    <tr>
      <td>use_bias</td>
      <td><p>Whether the layer uses a bias vector.</p></td>
    </tr>
    <tr>
      <td>kernel_initializer</td>
      <td><p>Initializer for the <code>kernel</code> weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_initializer</td>
      <td><p>Initializer for the bias vector.</p></td>
    </tr>
    <tr>
      <td>kernel_regularizer</td>
      <td><p>Regularizer function applied to the <code>kernel</code>
weights matrix.</p></td>
    </tr>
    <tr>
      <td>bias_regularizer</td>
      <td><p>Regularizer function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>activity_regularizer</td>
      <td><p>Regularizer function applied to the output of the
layer (its "activation")..</p></td>
    </tr>
    <tr>
      <td>kernel_constraint</td>
      <td><p>Constraint function applied to the <code>kernel</code> weights
matrix.</p></td>
    </tr>
    <tr>
      <td>bias_constraint</td>
      <td><p>Constraint function applied to the bias vector.</p></td>
    </tr>
    <tr>
      <td>input_shape</td>
      <td><p>Dimensionality of the input (integer) not including the
samples axis. This argument is required when using this layer as the first
layer in a model.</p></td>
    </tr>
    <tr>
      <td>batch_input_shape</td>
      <td><p>Shapes, including the batch size. For instance,
<code>batch_input_shape=c(10, 32)</code> indicates that the expected input will be
batches of 10 32-dimensional vectors. <code>batch_input_shape=list(NULL, 32)</code>
indicates batches of an arbitrary number of 32-dimensional vectors.</p></td>
    </tr>
    <tr>
      <td>batch_size</td>
      <td><p>Fixed batch size for layer</p></td>
    </tr>
    <tr>
      <td>dtype</td>
      <td><p>The data type expected by the input, as a string (<code>float32</code>,
<code>float64</code>, <code>int32</code>...)</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>An optional name string for the layer. Should be unique in a
model (do not reuse the same name twice). It will be autogenerated if it
isn't provided.</p></td>
    </tr>
    <tr>
      <td>trainable</td>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <td>weights</td>
      <td><p>Initial weights for layer.</p></td>
    </tr>
    </table>

    <h2 id="input-and-output-shapes">Input and Output Shapes</h2>

    


<p>Input shape: nD tensor with shape: <code>(batch_size, ..., input_dim)</code>. The most
common situation would be a 2D input with shape <code>(batch_size, input_dim)</code>.</p>
<p>Output shape: nD tensor with shape: <code>(batch_size, ..., units)</code>. For
instance, for a 2D input with shape <code>(batch_size, input_dim)</code>, the output
would have shape <code>(batch_size, unit)</code>.</p>
    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other core layers: 
<code><a href='../layer_activation.html'>layer_activation</a>()</code>,
<code><a href='../layer_activity_regularization.html'>layer_activity_regularization</a>()</code>,
<code><a href='../layer_attention.html'>layer_attention</a>()</code>,
<code><a href='../layer_dense_features.html'>layer_dense_features</a>()</code>,
<code><a href='../layer_dropout.html'>layer_dropout</a>()</code>,
<code><a href='../layer_flatten.html'>layer_flatten</a>()</code>,
<code><a href='../layer_input.html'>layer_input</a>()</code>,
<code><a href='../layer_lambda.html'>layer_lambda</a>()</code>,
<code><a href='../layer_masking.html'>layer_masking</a>()</code>,
<code><a href='../layer_permute.html'>layer_permute</a>()</code>,
<code><a href='../layer_repeat_vector.html'>layer_repeat_vector</a>()</code>,
<code><a href='../layer_reshape.html'>layer_reshape</a>()</code></p></div>




