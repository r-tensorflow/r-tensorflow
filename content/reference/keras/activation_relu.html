---
title: Activation functions
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: Activation functions - keras
    parent: keras-reference
aliases:
- /reference/keras/activation_relu.html
- /keras/reference/activation_relu.html
- /guide/keras/reference/activation_relu.html
- /tools/tools/keras/reference/activation_relu.html
---
    
    <p>Activations functions can either be used through <code><a href='../layer_activation.html'>layer_activation()</a></code>, or
through the activation argument supported by all forward layers.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>activation_relu</span>(<span class='no'>x</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>0</span>, <span class='kw'>max_value</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>threshold</span> <span class='kw'>=</span> <span class='fl'>0</span>)

<span class='fu'>activation_elu</span>(<span class='no'>x</span>, <span class='kw'>alpha</span> <span class='kw'>=</span> <span class='fl'>1</span>)

<span class='fu'>activation_selu</span>(<span class='no'>x</span>)

<span class='fu'>activation_hard_sigmoid</span>(<span class='no'>x</span>)

<span class='fu'>activation_linear</span>(<span class='no'>x</span>)

<span class='fu'>activation_sigmoid</span>(<span class='no'>x</span>)

<span class='fu'>activation_softmax</span>(<span class='no'>x</span>, <span class='kw'>axis</span> <span class='kw'>=</span> -<span class='fl'>1</span>)

<span class='fu'>activation_softplus</span>(<span class='no'>x</span>)

<span class='fu'>activation_softsign</span>(<span class='no'>x</span>)

<span class='fu'>activation_tanh</span>(<span class='no'>x</span>)

<span class='fu'>activation_exponential</span>(<span class='no'>x</span>)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>x</td>
      <td><p>Tensor</p></td>
    </tr>
    <tr>
      <td>alpha</td>
      <td><p>Alpha value</p></td>
    </tr>
    <tr>
      <td>max_value</td>
      <td><p>Max value</p></td>
    </tr>
    <tr>
      <td>threshold</td>
      <td><p>Threshold value for thresholded activation.</p></td>
    </tr>
    <tr>
      <td>axis</td>
      <td><p>Integer, axis along which the softmax normalization is applied</p></td>
    </tr>
    </table>

    <h2 id="value">Value</h2>

    <p>Tensor with the same shape and dtype as <code>x</code>.</p>
    <h2 id="details">Details</h2>

    
<ul>
<li><p><code>activation_selu()</code> to be used together with the initialization "lecun_normal".</p></li>
<li><p><code>activation_selu()</code> to be used together with the dropout variant "AlphaDropout".</p></li>
</ul>

    <h2 id="references">References</h2>

    

<ul>
<li><p><code>activation_selu()</code>: <a href='https://arxiv.org/abs/1706.02515'>Self-Normalizing Neural Networks</a></p></li>
</ul>





