---
title: Nesterov Adam optimizer
type: docs
repo: https://github.com/rstudio/keras
menu:
  main:
    name: Nesterov Adam optimizer - keras
    parent: keras-reference
aliases:
- /reference/keras/optimizer_nadam.html
- /keras/reference/optimizer_nadam.html
---
    
    <p>Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop
with Nesterov momentum.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>optimizer_nadam</span>(
  <span class='kw'>lr</span> <span class='kw'>=</span> <span class='fl'>0.002</span>,
  <span class='kw'>beta_1</span> <span class='kw'>=</span> <span class='fl'>0.9</span>,
  <span class='kw'>beta_2</span> <span class='kw'>=</span> <span class='fl'>0.999</span>,
  <span class='kw'>epsilon</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>schedule_decay</span> <span class='kw'>=</span> <span class='fl'>0.004</span>,
  <span class='kw'>clipnorm</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>clipvalue</span> <span class='kw'>=</span> <span class='kw'>NULL</span>
)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>lr</td>
      <td><p>float &gt;= 0. Learning rate.</p></td>
    </tr>
    <tr>
      <td>beta_1</td>
      <td><p>The exponential decay rate for the 1st moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p></td>
    </tr>
    <tr>
      <td>beta_2</td>
      <td><p>The exponential decay rate for the 2nd moment estimates. float,
0 &lt; beta &lt; 1. Generally close to 1.</p></td>
    </tr>
    <tr>
      <td>epsilon</td>
      <td><p>float &gt;= 0. Fuzz factor. If <code>NULL</code>, defaults to <code><a href='../k_epsilon.html'>k_epsilon()</a></code>.</p></td>
    </tr>
    <tr>
      <td>schedule_decay</td>
      <td><p>Schedule deacy.</p></td>
    </tr>
    <tr>
      <td>clipnorm</td>
      <td><p>Gradients will be clipped when their L2 norm exceeds this
value.</p></td>
    </tr>
    <tr>
      <td>clipvalue</td>
      <td><p>Gradients will be clipped when their absolute value exceeds
this value.</p></td>
    </tr>
    </table>

    <h2 id="details">Details</h2>

    <p>Default parameters follow those provided in the paper. It is
recommended to leave the parameters of this optimizer at their default
values.</p>
    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p><a href='http://www.cs.toronto.edu/~fritz/absps/momentum.pdf'>On the importance of initialization and momentum in deep learning</a>.</p>
<p>Other optimizers: 
<code><a href='../optimizer_adadelta.html'>optimizer_adadelta</a>()</code>,
<code><a href='../optimizer_adagrad.html'>optimizer_adagrad</a>()</code>,
<code><a href='../optimizer_adamax.html'>optimizer_adamax</a>()</code>,
<code><a href='../optimizer_adam.html'>optimizer_adam</a>()</code>,
<code><a href='../optimizer_rmsprop.html'>optimizer_rmsprop</a>()</code>,
<code><a href='../optimizer_sgd.html'>optimizer_sgd</a>()</code></p></div>




