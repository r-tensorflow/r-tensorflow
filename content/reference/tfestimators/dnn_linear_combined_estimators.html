---
title: Linear Combined Deep Neural Networks
type: docs
repo: https://github.com/rstudio/tfestimators
menu:
  main:
    name: Linear Combined Deep Neural Networks - tfestimators
    parent: tfestimators-reference
aliases:
- /reference/tfestimators/dnn_linear_combined_estimators.html
- /tfestimators/reference/dnn_linear_combined_estimators.html
- /guide/tfestimators/reference/dnn_linear_combined_estimators.html
- /tools/tools/tfestimators/reference/dnn_linear_combined_estimators.html
- /installation/tfestimators/reference/dnn_linear_combined_estimators.html
- /tutorials/tfestimators/reference/dnn_linear_combined_estimators.html
- /guide/tools/tfestimators/reference/dnn_linear_combined_estimators.html
- /deploy/tfestimators/reference/dnn_linear_combined_estimators.html
- /tools/tfestimators/reference/dnn_linear_combined_estimators.html
- /tutorials/tools/tfestimators/reference/dnn_linear_combined_estimators.html
---
    
    <p>Also known as <code>wide-n-deep</code> estimators, these are estimators for
TensorFlow Linear and DNN joined models for regression.</p>

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>dnn_linear_combined_regressor</span>(<span class='kw'>model_dir</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>linear_feature_columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>linear_optimizer</span> <span class='kw'>=</span> <span class='st'>"Ftrl"</span>,
  <span class='kw'>dnn_feature_columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>dnn_optimizer</span> <span class='kw'>=</span> <span class='st'>"Adagrad"</span>,
  <span class='kw'>dnn_hidden_units</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>dnn_activation_fn</span> <span class='kw'>=</span> <span class='st'>"relu"</span>,
  <span class='kw'>dnn_dropout</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>label_dimension</span> <span class='kw'>=</span> <span class='fl'>1L</span>, <span class='kw'>weight_column</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>input_layer_partitioner</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>config</span> <span class='kw'>=</span> <span class='kw'>NULL</span>)

<span class='fu'>dnn_linear_combined_classifier</span>(<span class='kw'>model_dir</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>linear_feature_columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>linear_optimizer</span> <span class='kw'>=</span> <span class='st'>"Ftrl"</span>,
  <span class='kw'>dnn_feature_columns</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>dnn_optimizer</span> <span class='kw'>=</span> <span class='st'>"Adagrad"</span>,
  <span class='kw'>dnn_hidden_units</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>dnn_activation_fn</span> <span class='kw'>=</span> <span class='st'>"relu"</span>,
  <span class='kw'>dnn_dropout</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>n_classes</span> <span class='kw'>=</span> <span class='fl'>2L</span>, <span class='kw'>weight_column</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>label_vocabulary</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>input_layer_partitioner</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>config</span> <span class='kw'>=</span> <span class='kw'>NULL</span>)</code></pre></div>

    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>model_dir</td>
      <td><p>Directory to save the model parameters, graph, and so on.
This can also be used to load checkpoints from the directory into a
estimator to continue training a previously saved model.</p></td>
    </tr>
    <tr>
      <td>linear_feature_columns</td>
      <td><p>The feature columns used by linear (wide) part
of the model.</p></td>
    </tr>
    <tr>
      <td>linear_optimizer</td>
      <td><p>Either the name of the optimizer to be used when
training the model, or a TensorFlow optimizer instance. Defaults to the
FTRL optimizer.</p></td>
    </tr>
    <tr>
      <td>dnn_feature_columns</td>
      <td><p>The feature columns used by the neural network
(deep) part in the model.</p></td>
    </tr>
    <tr>
      <td>dnn_optimizer</td>
      <td><p>Either the name of the optimizer to be used when
training the model, or a TensorFlow optimizer instance. Defaults to the
Adagrad optimizer.</p></td>
    </tr>
    <tr>
      <td>dnn_hidden_units</td>
      <td><p>An integer vector, indicating the number of hidden
units in each layer. All layers are fully connected. For example,
<code><a href='https://rdrr.io/r/base/c.html'>c(64, 32)</a></code> means the first layer has 64 nodes, and the second layer
has 32 nodes.</p></td>
    </tr>
    <tr>
      <td>dnn_activation_fn</td>
      <td><p>The activation function to apply to each layer. This can either be an
actual activation function (e.g. <code>tf$nn$relu</code>), or the name of an
activation function (e.g. <code>"relu"</code>). Defaults to the
<code>"relu"</code> activation function. See
<a href='https://www.tensorflow.org/api_guides/python/nn#Activation_Functions'>https://www.tensorflow.org/api_guides/python/nn#Activation_Functions</a>
for documentation related to the set of activation functions available
in TensorFlow.</p></td>
    </tr>
    <tr>
      <td>dnn_dropout</td>
      <td><p>When not <code>NULL</code>, the probability we will drop out a given
coordinate.</p></td>
    </tr>
    <tr>
      <td>label_dimension</td>
      <td><p>Number of regression targets per example. This is the
size of the last dimension of the labels and logits <code>Tensor</code> objects
(typically, these have shape <code>[batch_size, label_dimension]</code>).</p></td>
    </tr>
    <tr>
      <td>weight_column</td>
      <td><p>A string, or a numeric column created by
<code><a href='../column_numeric.html'>column_numeric()</a></code> defining feature column representing weights. It is used
to down weight or boost examples during training. It will be multiplied by
the loss of the example. If it is a string, it is used as a key to fetch
weight tensor from the <code>features</code> argument. If it is a numeric column,
then the raw tensor is fetched by key <code>weight_column$key</code>, then
<code>weight_column$normalizer_fn</code> is applied on it to get weight tensor.</p></td>
    </tr>
    <tr>
      <td>input_layer_partitioner</td>
      <td><p>An optional partitioner for the input layer.
Defaults to <code>min_max_variable_partitioner</code> with <code>min_slice_size</code> 64 &lt;&lt; 20.</p></td>
    </tr>
    <tr>
      <td>config</td>
      <td><p>A run configuration created by <code><a href='../run_config.html'>run_config()</a></code>, used to configure the runtime
settings.</p></td>
    </tr>
    <tr>
      <td>n_classes</td>
      <td><p>The number of label classes.</p></td>
    </tr>
    <tr>
      <td>label_vocabulary</td>
      <td><p>A list of strings represents possible label values.
If given, labels must be string type and have any value in
<code>label_vocabulary</code>. If it is not given, that means labels are already
encoded as integer or float within <code>[0, 1]</code> for <code>n_classes == 2</code> and
encoded as integer values in <code>{0, 1,..., n_classes  -1}</code> for <code>n_classes &gt; 2</code>. Also there will be errors if vocabulary is not provided and labels are
string.</p></td>
    </tr>
    </table>

    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other canned estimators: <code><a href='../boosted_trees_estimators.html'>boosted_trees_estimators</a></code>,
  <code><a href='../dnn_estimators.html'>dnn_estimators</a></code>,
  <code><a href='../linear_estimators.html'>linear_estimators</a></code></p></div>




