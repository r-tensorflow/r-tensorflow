---
title: "Configuration for the eval component of <code>train_and_evaluate</code>"
type: docs
repo: https://github.com/rstudio/tfestimators
menu:
  main:
    name: "Configuration for the eval component of &lt;code&gt;train_and_evaluate&lt;/code&gt; - tfestimators"
    parent: tfestimators-reference
---
    
    
    <p><code>EvalSpec</code> combines details of evaluation of the trained model as well as its
export. Evaluation consists of computing metrics to judge the performance of
the trained model. Export writes out the trained model on to external
storage.</p>
    

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>eval_spec</span>(<span class='no'>input_fn</span>, <span class='kw'>steps</span> <span class='kw'>=</span> <span class='fl'>100</span>, <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>hooks</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>exporters</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>start_delay_secs</span> <span class='kw'>=</span> <span class='fl'>120</span>, <span class='kw'>throttle_secs</span> <span class='kw'>=</span> <span class='fl'>600</span>)</code></pre></div>
    
    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">
    
    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>  
      
    <tr>
      <td>input_fn</td>
      <td><p>Evaluation input function returning a tuple of:</p><ul>
<li><p>features - <code>Tensor</code> or dictionary of string feature name to <code>Tensor</code>.</p></li>
<li><p>labels - <code>Tensor</code> or dictionary of <code>Tensor</code> with labels.</p></li>
</ul></td>
    </tr>
    <tr>
      <td>steps</td>
      <td><p>Positive number of steps for which to evaluate model.
If <code>NULL</code>, evaluates until <code>input_fn</code> raises an end-of-input exception.</p></td>
    </tr>
    <tr>
      <td>name</td>
      <td><p>Name of the evaluation if user needs to run multiple
evaluations on different data sets. Metrics for different evaluations
are saved in separate folders, and appear separately in tensorboard.</p></td>
    </tr>
    <tr>
      <td>hooks</td>
      <td><p>List of session run hooks to run
during evaluation.</p></td>
    </tr>
    <tr>
      <td>exporters</td>
      <td><p>List of <code>Exporter</code>s, or a single one, or <code>NULL</code>.
<code>exporters</code> will be invoked after each evaluation.</p></td>
    </tr>
    <tr>
      <td>start_delay_secs</td>
      <td><p>Start evaluating after waiting for this many
seconds.</p></td>
    </tr>
    <tr>
      <td>throttle_secs</td>
      <td><p>Do not re-evaluate unless the last evaluation was
started at least this many seconds ago. Of course, evaluation does not
occur if no new checkpoints are available, hence, this is the minimum.</p></td>
    </tr>
    </table>
    
    <h2 id="see-also">See also</h2>

    <div class='dont-index'><p>Other training methods: <code><a href='train_and_evaluate.tf_estimator.html'>train_and_evaluate.tf_estimator</a></code>,
  <code><a href='train_spec.html'>train_spec</a></code></p></div>
    



